[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH6195: Fundamentals of Data Science in R",
    "section": "",
    "text": "Course Structure and Logistics",
    "crumbs": [
      "Course Structure and Logistics"
    ]
  },
  {
    "objectID": "index.html#course-structure-and-logistics",
    "href": "index.html#course-structure-and-logistics",
    "title": "MATH6195: Fundamentals of Data Science in R",
    "section": "",
    "text": "This is the first year this course has been taught. There will be bugs! Please give me feedback if you think anything is too fast or too slow, and we can adjust.\n\n\nTeaching\nYou can’t learn data science by just listening to me talk to you. The teaching structure reflects this; each week we will have a 1 hour a lecture and a 2 hour lab. The point of the lab is for you to do data science; to this end, week we will build on what we saw in lectures by applying it to a data science problem.\n\n\nExtra Work\nThe main reference text for this book is the freely available R for Data Science, (Wickham, Çentnkaya-Rundel, and Grolemund 2023). However, we won’t always follow the book that closely. Each week, I will recommend some further work and/or reading that will help you to understand the material better. This will always be optional but I would encourage you to attempt it.\n\n\nAssessment\nThere will be two in-person supervised courseworks. Each will be worth 50% of your grade.\n\n\n\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Course Structure and Logistics"
    ]
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "1  Introduction to Data Science",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#objectives",
    "href": "week01.html#objectives",
    "title": "1  Introduction to Data Science",
    "section": "",
    "text": "Know what data science is and why it is important in the modern world.\nBe able to give an example of a data science workflow to follow when working as a data scientist.\nBe able to give examples of some tools we can use for Data Science, and discuss the importance of reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#what-is-data-science",
    "href": "week01.html#what-is-data-science",
    "title": "1  Introduction to Data Science",
    "section": "1.1 What is Data Science?",
    "text": "1.1 What is Data Science?\nData science is an interdisciplinary field that combines statistics, computer science, and domain expertise to extract meaningful insights and knowledge from data. It involves collecting, cleaning, analysing, and interpreting large and complex datasets to solve real-world problems, make predictions, and inform decision-making.\nAt its core, data science leverages techniques from mathematics and statistics to understand patterns and relationships in data, while using computational tools to automate and scale analyses. Data scientists often work with messy, unstructured data, applying methods such as data wrangling, visualization, machine learning, and statistical modelling.\n\n\n\nThe Data Science Venn Diagram, from here.\n\n\nThe (potentially controversial) image above puts data science in context: we need to have substantive expertise, maths and statistics knowledge and hacking skills to be a Data Scientist. In this course you will learn the last two; substantive expertise is also known as domain knowledge and is more difficult to acquire. The ultimate goal of data science is to turn raw data into actionable information, enabling organizations and individuals to make better, evidence-based decisions, and this is impossible without knowledge of the application domain.\nThat being said, Data Science is not a solo pursuit. In most of my Data Science projects I have been the mathematician / hacker in a team, containing domain experts. According to the Venn diagram above, it would be more proper to think of the team as the “Data Scientist” rather than me as an individual!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#the-data-science-workflow",
    "href": "week01.html#the-data-science-workflow",
    "title": "1  Introduction to Data Science",
    "section": "1.2 The Data Science Workflow",
    "text": "1.2 The Data Science Workflow\n\n\n\nThe Whole Game, taken from here.\n\n\nThe image above is taken from the course textbook (Wickham, Çentnkaya-Rundel, and Grolemund 2023), and includes the following steps:\n\nImport: Get the data from the source (e.g. Excel spreadsheets, databases) into your Data Science environment of choice.\nTidy: Organise your data to make it easier to work with.\nUnderstand: A loop consisting of:\n\nTransform: Transform the data into a standardised form that you can can be passed to algorithms.\nVisualise: Produce plots from your data to inform model choice.\nModel: Fit statistical or machine learning models to make inferences or predictions.\n\nCommunicate: Translate your results into language that in understandable for stakeholders, so you can discuss what actions to take.\n\nAs with much of data science there are other Data Science workflows you could follow, for example (Donoho 2017) or (Wild and Pfannkuch 1999) but most of them are in the same spirit as the above.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#the-role-of-computation",
    "href": "week01.html#the-role-of-computation",
    "title": "1  Introduction to Data Science",
    "section": "1.3 The Role of Computation",
    "text": "1.3 The Role of Computation\nA key advantage of computation in data science is the ability to automate repetitive or complex analyses. By writing code to process data, fit models, and generate visualizations, you can quickly rerun your entire workflow whenever new data arrives or methods change. This automation not only saves time but also reduces the risk of manual errors.\nReproducibility is another major benefit. When your analysis is scripted, others (including your future self) can reproduce your results exactly, verify your methods, and build upon your work. This is essential for scientific integrity and collaboration.\nFinally, computation enables you to scale your insights. With code, you can efficiently analyze large datasets, apply models to new data, and share your methods with others. This scalability is crucial as data volumes grow and projects become more complex.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#languages-and-tools",
    "href": "week01.html#languages-and-tools",
    "title": "1  Introduction to Data Science",
    "section": "1.4 Languages and Tools",
    "text": "1.4 Languages and Tools\nThis course is entitled “Fundamentals of Data Science in R”, but it should be emphasised that R is not the unique, best choice of programming language, and for many projects it may not even be a good language to use. There are many other languages that people use for Data Science. Perhaps the most widely used is python, but there are other, newer languages like Julia that give it a run for its money, particularly for scientific computing. At the end of the day, you can do Data Science in any language you happen to like: you can produce the same outputs in any of them. What’s more important is the software ecosystem - the packages that are available in that language. If a language has a mature software ecosystem it makes life considerably easier for a Data Scientist.\nWe will focus on R for this course so you can, by the end, become fluent in at least one major language for Data Science. Occasionally I will show you how to do things in other languages. I would encourage you to explore other options than R to find something that works best for you, rather than sticking with what I’ve shown you.\n\n1.4.1 Tools\nWhen working in Data Science, a variety of tools are used to support the workflow from data collection to communication of results. Some of the most important categories of tools include:\n\nIntegrated Development Environments (IDEs): Tools like RStudio (for R), JupyterLab (for R, Python, Julia, and many other languages), and Visual Studio Code (for pretty much any language) provide user-friendly interfaces for writing, running, and debugging code.\nVersion Control: Systems like Git (often used with GitHub or GitLab) help track changes in code and collaborate with others.\nData Visualization: Packages such as ggplot2 in R, matplotlib and seaborn in Python, and interactive tools like Tableau or Power BI are essential for exploring and presenting data.\nData Management: Tools for handling data include relational databases (e.g., SQLite, PostgreSQL), spreadsheet software (e.g., Excel, Google Sheets), and data wrangling libraries (dplyr in R, pandas in Python).\nReproducible Research: Tools like Quarto, R Markdown, and Jupyter Notebooks allow you to combine code, results, and narrative text in a single document, supporting transparency and reproducibility.\nCollaboration and Communication: Platforms such as Slack, Microsoft Teams, and project management tools like Trello or Asana facilitate teamwork and project tracking.\nContinuous Integration (CI): CI tools like GitHub Actions, GitLab CI, and Jenkins automatically run tests and checks on your code whenever changes are made. This helps ensure that your codebase remains functional, reproducible, and free of errors as you and your collaborators develop new features or fix bugs.\n\nChoosing the right tools depends on the project requirements, team preferences, and the specific problems being addressed.\n\n\n1.4.2 Generative AI\nGenerative AI tools, such as large language models (e.g., ChatGPT, GitHub Copilot), are increasingly being used in data science to assist with code generation, data exploration, documentation, and even generating synthetic data. They are valuable resources for learning and productivity in real-world data science projects and we will explore how they can be used to help with Data Science in this course.\n\n\n\n\n\n\nGenAI and Assessment\n\n\n\nFor this course’s assessments, the use of generative AI is not permitted. Assessments are designed to evaluate your individual understanding and skills. While you are encouraged to experiment with generative AI during your learning and practice, all submitted coursework must be your own unaided work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#reproducibility-and-scientific-integrity",
    "href": "week01.html#reproducibility-and-scientific-integrity",
    "title": "1  Introduction to Data Science",
    "section": "1.5 Reproducibility and Scientific Integrity",
    "text": "1.5 Reproducibility and Scientific Integrity\nReproducibility is an important concept in data science. When we say research is reproducible, we meant that another researcher can take the same data and methods and obtain the same results. Without reproducibility, data analysis is a “black box”; we will focus in this course on instead transforming data analysis into a transparent, verifiable process that the scientific community can trust.\n\n\n\n\n\n\nConsequences of Non-Reproducible Results\n\n\n\nIn the real world, data science can drive government, medical and business decisions. Policy decisions based on irreproducible analyses could be genuinely dangerous and harmful. This is therefore something to be taken seriously.\n\n\nWe will look at reproducibility in detail in Week 3. For now we will focus on two practical points that will make engaging with the material in Week 1 and 2 easier.\n\n1.5.1 R and Quarto\nAll of the labs will be conducted in RStudio, primarily using Quarto documents. Quarto is a successor to RMarkdown; it is a markdown-based file format that allows for R code to be embedded in the document. The document can then be knitted to produce various outputs.\nMost of the time in the labs you will load partially-written Quarto files that you pull from GitHub. However, it is worth knowing how to create a fresh Quarto document from RStudio. To create one, open the main menu and select New &gt; Quarto Document.\nRStudio has two visual modes for Quarto:\n\nVisual (default) shows a preview of the rendered document with all formatting applied.\nSource shows the plain Markdown text. This is what is actually saved in the file.\n\n\n\n1.5.2 Git and GitHub\nAll of the labs in this course will be hosted on GitHub using GitHub Classroom. We will go through how to use GitHub classroom and Git on the University computers in the first lab. However, for now, it is helpful to know the following minimal points:\n\nYou can download Git from here.\nYou can download (clone) a Git repository with the command git clone &lt;repository-url&gt;\nYou can add commits to the “staging area” with git add &lt;filename&gt; or just git add *.\nYou can commit changes with git commit -m \"Commit message\"\nYou can push changes up to github with git push.\n\nIf you have Git setup correctly on your machine, you will also see a Git pane in RStudio when you open it. We’ll go over this in more detail in Week 3, but the above will be enough for now.\n\n\n\n\n\n\nAssessment\n\n\n\nThe course assessments will all assess reproducibility of results. You will submit your coursework using GitHub Classroom, as in labs. Therefore, make sure you get comfortable with it!\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWild, C. J., and M. Pfannkuch. 1999. “Statistical Thinking in Empirical Enquiry.” International Statistical Review 67 (3): 223–48. https://doi.org/10.1111/j.1751-5823.1999.tb00442.x.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  “Coding in R”",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#objectives",
    "href": "week02.html#objectives",
    "title": "2  “Coding in R”",
    "section": "",
    "text": "Understand the basic syntax and data types in R, including variables, vectors, and control flow statements.\nLearn how to write and use functions in R, including higher-order functions and closures.\nDevelop foundational debugging skills for R code within Quarto documents.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#an-introduction-to-r",
    "href": "week02.html#an-introduction-to-r",
    "title": "2  “Coding in R”",
    "section": "2.1 An Introduction to R",
    "text": "2.1 An Introduction to R\nR is a powerful programming language and environment designed specifically for statistical computing, data analysis, and graphics. R is one of the most popular tools for data science, used by statisticians, researchers and analysts across academia and industry. Its main competitor is python, which is also a very powerful and widely used programming language. In contrast to R, python was not written primarily for science, which has advantages and disadvantages.\n\n2.1.1 Why Choose R for Data Science?\n\nBuilt for Statistics: Unlike general-purpose programming languages, R was created with statistical analysis in mind.\nPackage Ecosystem: R has over 19.000 packages available on its package hosting platform, CRAN (Comprehensive R Archive Network). Compared with other languages, its packages are significantly more mature in some areas.\nData Visualisation: R excels at creating exploratory plots for your own understanding and publication-quality graphics for sharing results. The ggplot2 package, in particular, has revolutionised data visualisation with its “grammar of graphics” framework.\nOpen Source and Free: R is completely free to use, modify and distribute, which is important for reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#basic-r-syntax",
    "href": "week02.html#basic-r-syntax",
    "title": "2  “Coding in R”",
    "section": "2.2 Basic R Syntax",
    "text": "2.2 Basic R Syntax\nWe will now see some basic R syntax.\n\n2.2.1 Assignments and Types\nAs with most programming languages, at its most basic level R allows us to create variables.\n\nx &lt;- 5\ny &lt;- \"Hello\"\nz = TRUE\n\n\n\n\n\n\n\nAssignment in R\n\n\n\nThe idiomatic way to assign values to variables in R is using the “left pointy arrow” &lt;-, but you can also use the more standard = sign that you may be familiar with from other languages. My background is in python, so I will usually default to = rather than &lt;-. I don’t mind which you use in assessments.\n\n\nWhen you assign a variable, it gains a type:\n\ntypeof(x)\n\n[1] \"double\"\n\ntypeof(y)\n\n[1] \"character\"\n\ntypeof(z)\n\n[1] \"logical\"\n\n\nWe see that xhas type double (a decimal number, stored in the computer as double precision), y has type character (a text string) and z has type logical (a logical, i.e. true or false, value). We will see other types later in the course.\nUnlike lower level languages like C# or java, R is a dynamically typed, interpreted language. An immediate consequence of this is that I can reassign a str variable to another type without any consequence:\n\nnumber = 5\ntypeof(number)\n\n[1] \"double\"\n\nnumber = \"hello\"\ntypeof(number)\n\n[1] \"character\"\n\n\n\n\n2.2.2 Printing Things\nIn R, printing is how you display the value of variables or expressions. There are several ways to do this:\n\n2.2.2.1 Automatic Printing (Interactive Mode)\nWhen you enter an expression that is not assigned to a variable, R automatically prints the result:\n\n1 + 1\n\n[1] 2\n\n\n\n\n2.2.2.2 Explicit Printing with print()\nUse print(...) when you want to be sure something shows up, especially inside functions or scripts:\n\nx &lt;- 42\nprint(x)\n\n[1] 42\n\n\n\n\n2.2.2.3 Formatted Printing with cat()\nUse cat(...) for cleaner, formatted output, especially when combining text and values:\n\nname &lt;- \"Flipper\"\nlength &lt;- 210\n\ncat(\"Penguin\", name, \"has a flipper length of\", length, \"mm.\\n\")\n\nPenguin Flipper has a flipper length of 210 mm.\n\n\n\n\n\n\n\n\nTip\n\n\n\ncat() does not add a newline by default — use if you want one.\nIt also avoids [1] and quotes, making it nicer for messages.\n\n\n\n\n2.2.2.4 String Formatting with sprintf()\nFor more precise formatting:\n\nstr = sprintf(\"Flipper length: %.2f mm\", 209.537)\nstr\n\n[1] \"Flipper length: 209.54 mm\"\n\n\nThis works like printf(...) in other languages and returns a string.\n\n\n\n2.2.3 Vectors\nWe can also assemble vectors. The syntax for this is c(...), i.e.\n\nvector = c(1,2,3,4,5)\ntypeof(vector)\n\n[1] \"double\"\n\n\nNote that the type is reported as a double, just like the type of x above. This is because R treats scalars like length-1 vectors:\n\nlength(vector)\n\n[1] 5\n\nlength(x)\n\n[1] 1\n\n\nWe can retrieve elements of vectors by indexing with square brackets. Note that in R, vectors start from 1 rather than 0 as in other languages.\n\nvector[2]\n\n[1] 2\n\nvector[5] # the last element of the vector would be 4 in other languages, e.g. python.\n\n[1] 5\n\n\nWe can also slice vectors, i.e. retrieve a subset of the elements:\n\nvector[2:4]\n\n[1] 2 3 4\n\n\nLastly, it’s sometimes helpful to use logical indexing\n\nelements = c(TRUE, FALSE, TRUE, TRUE, FALSE)\nvector[elements]\n\n[1] 1 3 4\n\nvector[!elements] # the !elements syntax does elementwise negation.\n\n[1] 2 5\n\n\n\n\n2.2.4 Matrices\nMatrices are “just” vectors with two axes, so most of the above transfers:\n\nmat = matrix(1:6, nrow=2, ncol=3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmat[2,3] # get the (2,3) element\n\n[1] 6\n\nmat[,3] # third column\n\n[1] 5 6\n\nmat[2,] # second row\n\n[1] 2 4 6\n\nmat[1:2, 2:3] # slicing\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n\nmat[mat &lt; 4] # logical indexing (note this returns a vector of matching elements!)\n\n[1] 1 2 3\n\n\n\n\n2.2.5 Data Frames\nData frames are one of the most important data structures in R for storing and manipulating tabular data. Think of a data frame as a table, where each column can contain values of different types (numeric, character, logical, etc.), and each row represents an observation.\nYou can create a data frame using the data.frame() function:\n\ndf &lt;- data.frame(\n    name = c(\"Flipper\", \"Tux\", \"Pingu\"),\n    species = c(\"Adelie\", \"Gentoo\", \"Emperor\"),\n    flipper_length_mm = c(210, 217, 230)\n)\ndf\n\n     name species flipper_length_mm\n1 Flipper  Adelie               210\n2     Tux  Gentoo               217\n3   Pingu Emperor               230\n\n\nIn the data.frame function we specify multiple columns of data through &lt;col name&gt; = &lt;vector of col values&gt;. Of course, the vectors need to be the same length or we will get an error. When we print the data frame we can see the column names along the top, the row numbers on the left and the values in the middle. We can access columns from the data frame like so:\n\ndf$name\n\n[1] \"Flipper\" \"Tux\"     \"Pingu\"  \n\ndf$flipper_length_mm\n\n[1] 210 217 230\n\n\nRows can be accessed as:\n\ndf[2,]\n\n  name species flipper_length_mm\n2  Tux  Gentoo               217\n\n\nSpecific entries can be accessed as\n\ndf[2,\"species\"]\n\n[1] \"Gentoo\"\n\ndf[2,2]\n\n[1] \"Gentoo\"\n\n\n\n\n\n\n\n\nTibbles\n\n\n\nData frames have been supplanted by a more modern implementation called a tibble, provided through the tidyverse package of the same name. On a practical level the functionality provided is basically the same. We will not use data.frame much outside of this week, in favour of tibble, which we will see in Week 4.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#control-flow",
    "href": "week02.html#control-flow",
    "title": "2  “Coding in R”",
    "section": "2.3 Control Flow",
    "text": "2.3 Control Flow\n“Control flow” statements allow us to control the order in which instructions are executed in a program.\n\n2.3.1 Conditionals\nThe most commonly used conditional statement is an if statement.\n\nx = 1\nif(x == 1) print(\"hello\") # simple statements can be on one line\n\n[1] \"hello\"\n\nif(x &lt; 1) {\n    print(\"I don't know how you got here\") # more complex statements can be enclosed in braces\n}\n\n# we can also use else if, and else, for more fine grained control\nif(x &lt; 0) {\n    print(\"x was negative.\")\n} else if(x == 0) { # note that the else *must appear on the same line* as the closing brace!\n    print(\"x was 0\")\n} else {\n    print(\"x was positive\")\n}\n\n[1] \"x was positive\"\n\n# Finally, we can include more complex statements\nif(x &gt; 0 && x &lt; 2) print(\"hello\")\n\n[1] \"hello\"\n\n\nThere are also more complex conditionals, like switch statements, but we won’t cover these here.\n\n\n2.3.2 Loops\nLoops allow us to execute a block of code repeatedly. The most commonly used is a for loop:\n\n# this runs the block inside the braces for each value of i between 1 and 5\nfor(i in 1:5) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n# this shows more complex logic inside the loop.\n# break forces the loop to end\nfor(i in 1:10) {\n    if(i &gt; 2) print(i)\n    if(i &gt;= 5) break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\nWe can also use this structure to loop through elements of vectors. This replicates the previous loop but using boolean indexing.\n\nvector = 1:10\nfor(i in vector[(vector &gt; 2) & (vector &lt; 6)]) {\n    print(i)\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above uses more advanced boolean indexing; we will see this in more detail in the lab.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#functions",
    "href": "week02.html#functions",
    "title": "2  “Coding in R”",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nWriting one long script is very bad practice. It’s much better to break R code up across multiple functions and, as we will see in Week 8 multiple script files. We can define callable functions in R as follows\n\nadd = function(x, y) {\n    x + y\n}\nadd(1, 10)\n\n[1] 11\n\n\nNotice that the above assigns a function to the variable add. This is because functions are objects in R. This enables a powerful paradigm called functional programming, which is a very efficient way to build complex programs by writing small building block functions that you compose to yield complexity. The tidyverse, which we will see in Week 4, embraces functional programming paradigms.\nIn other programming languages it is necessary to specify what you want to return from a function. This is not required in R; the default is that the last statement in the function will be the return value. The return keyword still exists, but is used slightly differently to other languages. The above code is equivalent to\n\nadd = function(x, y) {\n    return(x + y)\n}\nadd(1, 10)\n\n[1] 11\n\n\nOf course, there are times when you might need to use return to exit a function early. For example:\n\nsilly_add = function(x,y) {\n    if (x &lt; y) {\n        return(x + y)\n    }\n    x = -x\n    y - x\n}\nadd(1,10)\n\n[1] 11\n\nadd(10,1)\n\n[1] 11\n\n\n(Of course it is possible to think of more applied examples of this…)\n\n2.4.1 Higher Order Functions\nHigher order functions are functions that either take or return functions. Functions that take functions are really useful for array operations. Here we pass the “add” function into the “Reduce” function to sum the elements of the numbers from 1 to 10.\n\nReduce(add, 1:10)\n\n[1] 55\n\n\nWe don’t even need to assign the function to a variable - we can pass it as an anonymous function.\n\nReduce(function(x, y) x * y, 1:10)\n\n[1] 3628800\n\n\nAnother useful example are the apply family of functions, which take a vector and apply a function to each element of the vector.\n\nsapply(1:10, function(x) x*10)\n\n [1]  10  20  30  40  50  60  70  80  90 100\n\n\nFunctions that return functions can also be useful:\n\nmake_multiplier = function(multiplier) {\n    function(x) {\n        multiplier * x\n    }\n}\ntimes_two = make_multiplier(2)\ntimes_ten = make_multiplier(10)\n\ntimes_two(2)\n\n[1] 4\n\ntimes_ten(2)\n\n[1] 20\n\n\n\n\n2.4.2 Pipes\nPipes are a way to make code more readable and expressive by providing a succinct syntax for composition. Writing this mathematically, suppose we wanted to compute \\(f(g(x))\\). If we specify these functions in R, we might have\n\nf = function(y) {\n    y * 10\n}\n\ng = function(x) {\n    x + 5\n}\n\nx = 10\nf(g(x))\n\n[1] 150\n\n\nWe can express this equivalently using the forward pipe syntax |&gt;.\n\ng(x) |&gt; f()\n\n[1] 150\n\n\nor even\n\nx |&gt; g() |&gt; f()\n\n[1] 150\n\n\nThe parentheses () are required. R is implicitly inserting the argument “behind” the pipe into the first argument of the function. This means that we can use functions that have more arguments, for example:\n\nx |&gt; add(5) |&gt; times_two()\n\n[1] 30\n\n\nWe will use pipes much more extensively in Week 4 and 5!\n\n\n\n\n\n\nOther Pipes\n\n\n\nThere are other kinds of pipe available in R. Another one which is commonly used is the %&gt;% pipe, which precedes |&gt; and was introduced in the R package magrittr. In most cases the behaviour of these two pipes is the same, however |&gt; is part of the base R language while %&gt;% is not (though, it is included as part of tidyverse).\nAnother useful pipe is the “tee pipe” %T&gt;%, which can be used to “split” a pipe to call functions that don’t return anything (which would otherwise end the pipe), then continue to use the pipe expression afterwards.\n\n\n\n\n2.4.3 Closures (Advanced)\nLet’s examine the type of the add function we defined earlier.\n\ntypeof(add)\n\n[1] \"closure\"\n\n\nThis says add is a “closure”. “Closure” actually means something more complicated than just “function”. A closure has full access to the environment it was defined in. For example:\n\nz = 10\nclosure = function(x) {\n    x + z\n}\nclosure(1)\n\n[1] 11\n\n\nThe closure can “see” the value of z from the outside scope. This can be dangerous, because it means that functions can have unpredictable behaviour unless we are careful about controlling what variables they access.\n\nz = 50\nclosure(1)\n\n[1] 51\n\nz = 20\nclosure(1)\n\n[1] 21\n\n\nIn other words, the value of closure(1) can depend on things other than just the values input to the function. Fortunately functions do not (typically) modify the outer scope; this is referred to as “lexical scoping”\n\nbad_closure = function() {\n    z = rnorm(1) \n}\nbad_closure()\nclosure(1)\n\n[1] 21\n\nbad_closure()\nclosure(1)\n\n[1] 21\n\n\nIn the above we try to modify the value of z inside very_bad_closure, but we can see that the modification is limited to the “inner” scope of that function and does not propagate to the outer scope. However - we can still modify the outer scope if we really want to using the superassignment operator, &lt;&lt;-:\n\nvery_bad_closure = function() {\n    z &lt;&lt;- rnorm(1) \n}\nvery_bad_closure()\nclosure(1)\n\n[1] 1.22574\n\nvery_bad_closure()\nclosure(1)\n\n[1] -0.8947173\n\n\nThis allows us to do some powerful - but dangerous - things!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week02.html#basic-debugging",
    "href": "week02.html#basic-debugging",
    "title": "2  “Coding in R”",
    "section": "2.5 Basic Debugging",
    "text": "2.5 Basic Debugging\nBugs happen, and a central part of being a data scientist is knowing how to fix them when they do. Some studies have shown that around 50% of development time is spent debugging, and the average software project has 15-50 bugs per 1000 lines of code. When working in quarto as we will mostly in this course, here are a few approaches to debugging that can help:\n\nUse print or cat to inspect values.\nIsolate the problem in a new chunk. If something isn’t working, break the chunk up into multiple sections to check the logic, or isolate portions of a complex function to test them.\nCheck your type assumptions using str or typeof to ensure the data is what you expect it to be.\nSet error: true in the chunk options to see an error message without halting the document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>\"Coding in `R`\"</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  Reproducible Research and Project Management",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#objectives",
    "href": "week03.html#objectives",
    "title": "3  Reproducible Research and Project Management",
    "section": "",
    "text": "Know the principles of reproducibility and literate programming, and how they can be realised in R using Quarto.\nKnow how to collaborate and track your code using git.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#introduction",
    "href": "week03.html#introduction",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nReproducibility is at the heart of credible scientific computing. Yet, it’s increasingly common to find projects where no one, not even the original author, can recreate past results. Code is scattered, data is missing, and assumptions live only in someone’s head. This undermines the foundations of science. In response to these issues, two powerful ideas have emerged: literate programming and version control.\nLiterate programming encourages us to weave narrative and code together — to treat analysis as a story told with data. Quarto, the modern successor to R Markdown, provides a seamless way to do this that supports R and many other languages.\nMeanwhile, Git gives us a robust system for tracking changes, collaborating with others, and safeguarding our work. With Git, we can see the evolution of a project, roll back mistakes, and work in parallel as a team.\nAs said in (Buckheit and Donoho 1995) (regarding research papers about computational results):\n\n… these documents are not the research [rather] these documents are the “advertising.” The research is the “full software environment, code, and data that produced the results”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#reproducibility-and-literate-programming-in-r-with-quarto",
    "href": "week03.html#reproducibility-and-literate-programming-in-r-with-quarto",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.2 Reproducibility and Literate Programming in R with Quarto",
    "text": "3.2 Reproducibility and Literate Programming in R with Quarto\nWe have already seen some Quarto documents, but up until now we have treated it as just a way to run R code. We will now give a brief, more formal introduction to Quarto. More information is available in the Quarto guide.\n\n3.2.1 Reproducibility and Literate Programming\nReproducibility means that someone else (or even your future self) can take your code, data, and documentation, and obtain the same results you did. This is essential for scientific integrity and for building on previous work. In practice, reproducibility requires:\n\nAll code and data are available and clearly organized.\nThe computational environment (software versions, dependencies) is documented.\nThe process to generate results is automated and documented.\n\nLiterate programming is a methodology introduced by Donald Knuth (Knuth 1992), where code and narrative are woven together in a single document. The idea is to write programs that are understandable by humans first, and computers second. In practice, this means:\n\nExplanations, assumptions, and reasoning are written alongside code.\nThe document can be “knitted” or “rendered” to produce a readable report, with code, output, and narrative together.\nErrors or changes in data/code are automatically reflected in the output, reducing manual mistakes.\n\nQuarto (and previously R Markdown) are tools that enable literate programming in R and other languages. They allow you to combine markdown text, code chunks, and output in a single .qmd file. This approach ensures that your analysis is transparent, reproducible, and easier to share or update.\n\n\n3.2.2 Anatomy of a Quarto Document\nA Quarto document is a plain text document with the extension .qmd. It can start with a YAML Header followed by narrative text interspersed with code chunks.\n\n3.2.2.1 Knitting\nFundamental to understanding Quarto is knowing what you can use it for. Quarto is intended to allow a user to write documents in the literate programming paradigm, in a way that is agnostic to the final output file format of the document. For example, these lecture notes are all written in Quarto, and it allows them to be compiled into HTML or PDF as you can find on the course Blackboard. There are a lot of other formats that Quarto can be compiled to, though! The process of taking the raw .qmd file and turning it into HTML or PDF is called “knitting”. In RStudio, knitting is accomplished by clicking the Render button.\n\n\n3.2.2.2 YAML Header\nYAML is an acronym for “Yet Another Markup Language” or “YAML Ain’t Markup Language”. It is intended to be a way of serialising data that is easily readable both by humans and machines. Giving a full specification of YAML is beyond the scope of this course, but those interested can find more information here.\nA Quarto document does not need to include a YAML header, but it can be helpful to include metadata as part of the document. For example, a minimal YAML header might look like:\n---\ntitle: An Example Quarto Document\n---\nThe two --- lines are important; these tell Quarto that this is the YAML header, and everything between the --- is to be treated as YAML. In this case, the contents simply tells Quarto that the title of the document is “An Example Quarto Document”. When the document is knitted, the title can be used in various ways. For example, it can be placed into the &lt;title&gt; tags in a HTML document, or it can be used to generate a PDF table of contents.\nThe YAML header is also used to inform the knitting process. For example, this header tells Quarto that we would like to knit to HTML:\n---\ntitle: An Example Quarto Document\nformat: html\n---\nThe next one specifies that either HTML or PDF are options. For HTML, it says the theme and injects some custom styles. For PDF it specifies the documentclass and injects a custom preamble.\n---\ntitle: An Example Quarto Document\nformat: \n    html:\n        theme: cosmo\n        css: styles.css\n    pdf:\n        documentclass: scrreport\n        include-in-header: preamble.tex\n---\nThere are a great many other keys that can be included to customise the document. For this course, the following simple header will usually suffice (or, even, the default one generated by RStudio):\n---\ntitle: &lt;Your Title Here&gt;\nformat: html\n---\n\n\n3.2.2.3 Narrative Text\nAfter the YAML header, the rest of the text is markdown interspersed with code chunks. Markdown is a simple language that describes how text should be formatted when knitted / rendered. For example:\n## An Amazing Title\n\nThis *text* is **even more** _amazing_ than the title. It can even include math! $\\pi = 4$.\nKnits to:\n\n\n\n\n\n\nOutput\n\n\n\n1.1 An Amazing Title\nThis text is even more amazing than the title. It can even include math! \\(\\pi = 4\\).\n\n\nA more complete description of Markdown can be found here.\n\n\n3.2.2.4 Code Chunks\nCode chunks in Quarto are indicated by using the ``` syntax, i.e.:\n\n```{r}\nprint(\"Hello world!\")\n```\n\nIf we embed this in a Quarto document, we get the following output when knitted:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nThe ``` are processed by the knitter and a nicely formatted version of the code block is substituted. But, crucially, the code is actually run and the output is placed after it! As we have already seen, this can be used to embed figures in the knitted output. For example, by adding this:\n\n```{r}\nlibrary(tidyverse)\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n```\n\nWe get this:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\npenguins &lt;- read_csv(\"data/penguins.csv\")\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rowid, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNotice, however, that this included a lot of “ugly”, unnecessary output. We can fix this by using code block options which we add at the top of code blocks.\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n```\n\nIn the above:\n\nmessage: false removes messages printed when loading the data or tidyverse,\nwarning: false removes warning messages\necho: false stops the code from being rendered, so we just get the figure.\n\nSo, if we run this, we just get the figure:\n\n\n\n\n\n\n\n\n\nImportantly, all the code to produce the figure is still in the .qmd, so reproducibility of the document is preserved.\n\n\n\n\n\n\nOther Execution Options\n\n\n\nFurther execution options are listed here. A particularly useful (but advanced) one is output: asis, which allows your code to generate raw markdown that Quarto will then postprocess. This allows you to do advanced things like autogenerate custom tables to display in the document.\n\n\nOne final note is that we can even run code inline. For example, suppose I wanted to include the text:\n\nThe Palmer Penguins dataset contains 344 records.\n\nIt’s not very convenient or robust to count the rows in the Palmer Penguins dataset and write it in the document as a string! Instead we can use an “inline” code chunk, e.g.\n\nThe Palmer Penguins dataset contains `r nrow(penguins)` records.\n\nThe text:\n\n`r nrow(penguins)`\n\ntells the knitting process to run nrow(penguins) and substitute the result into the document, so that this is not necessary. This is much better for reproducibility than including the number in the source, since now the document remains correct even if the raw data changes.\n\n\n\n3.2.3 Advantages of Quarto\nA major advantage of Quarto is that it is easy to integrate with version control because the files are “just” plain text files. We will examine this in more detail in the next section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#an-amazing-title",
    "href": "week03.html#an-amazing-title",
    "title": "3  Reproducible Research and Project Management",
    "section": "1.1 An Amazing Title",
    "text": "1.1 An Amazing Title\nThis text is even more amazing than the title. It can even include math! \\(\\pi = 4\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#introduction-to-git",
    "href": "week03.html#introduction-to-git",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.3 Introduction to Git",
    "text": "3.3 Introduction to Git\nGit is a version control system which you can use to manage changes to your codebase. It can be used to record what changed, when and why across all the files you are tracking. At any point, you can “rewind” to a previous state, compare versions, or explore how your code evolved. You can even branch off to try something new without breaking the main version.\nGit is interacted with through git verbs. When we use git from the command line, the verbs are commands added after git that specify what we want git to do. However, understanding what the verbs mean also helps when interacting with GUI tools, such as the git extensions in VS Code and RStudio.\n\n3.3.1 Initialising Git Repositories\nThere are two major ways to create a git repository.\n\nUse the git init verb. This initialises an empty local git repository in a folder.\nUse the git clone verb. This allows you to clone a repository from GitHub to your local machine.\n\nUsing git init does not create a remote repository on GitHub automatically. You still need to go into GitHub and create the remote, then add it. When you create an empty remote on GitHub, the repository contains instructions that describe how to add it as a remote in a local repository, or how to clone it to your local machine.\n\n\n\n\n\n\nTip\n\n\n\nIn this course, because we will be using GitHub classroom, you will rarely have to create repositories for yourself. But it’s still useful to know how to do this from scratch.\n\n\n\n\n3.3.2 Commits\nThe basic atomic element of a git repository is a “commit”. Each commit is like a snapshot of the repository with a (user-supplied) message describing what changed between that snapshot and the previous one. The verbs which control commits are:\n\ngit add: stage local changes to the staging area.\ngit status: check what is currently staged in the staging area.\ngit commit: create a commit containing all of the changes in the staging area.\n\n\n\n3.3.3 Branches\nA branch is like a parallel version of your project in which you can work on new features, experiments or fixes without affecting the main code. The default branch is usually called main, but you can create a new branch and switch between them at any time.\nBranches are also very useful for collaboration. If you are working in parallel with other team members, best practice is to each work on your own branch of the code, then when you are done, merge your changes together. This ensures that all your changes are grouped together, and there is always a version of your code stored in your branch that works as expected.\nFinally, they are useful for clarity. If you are developing a major new feature in a codebase, grouping all of the commits for that feature together in a branch with the feature name makes the logical connection between them clearer.\nThe verbs which control branching are:\ngit branch # see all branches\n\ngit branch my-amazing-feature # create a new branch called &lt;branch-name&gt;\n# note that you are still on the original branch!\n\ngit checkout my-amazing-feature # switch to the branch &lt;branch-name&gt;\n# this command will not work if you have changes that are not committed!\nWhen you are done making changes on the branch my-amazing-feature, you will want to merge the changes back onto main. This can be done as follows:\ngit checkout main # switch back to the main branch\ngit merge my-amazing-feature # merge in changes\nGit will do its best to merge the changes onto main. However, if main already contains commits that are not present in my-amazing-feature, it may not be obvious how to combine the commits from my-amazing-feature and those on main. This creates a conflict, which we will see in a moment.\n\n\n3.3.4 Collaboration\nInside each git repository there is a folder named .git which stores the repository. The git commands you type at the command line examine and modify the contents of this .git folder according to the command you issue. This means that the entire repository is stored on your local machine; there is no need for git to talk to GitHub apart from to perform specific operations related to collaboration.\n\n\n\n\n\n\nWarning\n\n\n\nYou should never modify the contents of the .git folder manually unless you really know what you are doing. Modifying it can catastrophically break your local git repository. Only interact with it through the git commands, or GUIs.\n\n\nTo enable collaboration we need a “central” git repository which all members of a team can see and talk to. This is the role of GitHub (or other Git platforms, like GitLab), which provides these central repositories. GitHub essentially stores a separate copy of your .git folder, which you can push to.\ngit checkout my-amazing-feature\n# you can get the URL for origin from GitHub\ngit remote add origin git@github.com:jcockanye/my_amazing_app\n\ngit push # this command might fail if you have never pushed my-amazing-feature\n# if so you can run this command to tell GitHub to set the \"upstream\" branch on origin\ngit push -u origin my-amazing-feature\nPushing only works if the remote branch HEAD is contained in your local commit tree. If there is a mismatch between commits, then the difference first needs to be pulled down to your local machine.\ngit pull # pull down changes from the remote\nIf there are changes on origin that you don’t have locally, git will try to merge them, as when you use git merge. This has the same caveat, that it can lead to conflicts.\n\n\n3.3.5 Merge Conflicts\nMerge conflicts usually occur when git can’t automatically combine changes from two branches. The most common reason for this is if two people edited the same line of a file, or if you changed the same file in different branches. To fix the conflict you need to intervene manually to tell git how the changes should be combined. Merge conflicts are perfectly normal in collaborative work, and resolving them carefully ensures you don’t lose anyone’s changes.\nWhen a conflict occurs, Git will mark conflicting lines in the file like so:\n# Load data\ndata &lt;- read.csv(\"data.csv\")\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Calculate mean of column A\nmean_A &lt;- mean(data$A)\nprint(mean_A)\n=======\n# Calculate sum of column A\nsum_A &lt;- sum(data$A)\nprint(sum_A)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/sum-column\nThe &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD part up to ======= shows the code from your current branch. The text between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/sum-column show the end of the incoming changes. In this case, we need to examine what the two blocks do and work out how to resolve the conflict. In this case, a sensible way to do this would be as follows:\n# Load data\ndata &lt;- read.csv(\"data.csv\")\n\n# Calculate mean and sum of column A\nmean_A &lt;- mean(data$A)\nsum_A &lt;- sum(data$A)\n\nprint(mean_A)\nprint(sum_A)\nOnce all the conflicts have been resolved you can then create a new merge commit containing the merge results, and push up to GitHub.\n\n\n\n\n\n\nAvoiding Conflicts\n\n\n\n\nKeep changes small and focused to avoid conflicts. (Don’t create monolithic, 1000 line commits!)\nCommunicate with collaborators before making major changes, to avoid large conflicting rewrites.\nWork on separate branches, to simplify merges and reduce their frequency.\n\n\n\n\n\n3.3.6 Ignoring Files\nWe don’t always want to commit all files to version control. A common example is output files, e.g. images or PDFs. These are often called build artifacts - they are things your code is written to produce. Committing build artifacts is bad practice:\n\nThey can be very large, bloating the repository.\nThey can be recreated from source files\nThey often cause unnecessary merge conflicts.\n\nOf course, there are exceptions to this, for example if you want to store small, critical outputs for collaborators who don’t build locally.\nHowever - git doesn’t know what files are source files and what are build artifacts. We can use a .gitignore file, stored in the root directory of your Git repository, to tell Git what to exclude from commits, for example:\n*.html\n*.pdf\n*.png\n*.jpg\n*.RData\nAs an aside, generating .gitignore files is a fantastic use of GenAI. Github contains billions of them, so GenAI knows how to make them very well!\n\n\n\n\n\n\nTip\n\n\n\nFor this course the best practice is to only commit input and source files, not outputs. Your code should be able to regenerate all results from scratch.\n\n\n\n\n3.3.7 Using Git in RStudio\nAs with many IDEs, RStudio has built in Git integration which can make the git workflow easier than interacting purely through the terminal. However, knowing how to do things in the terminal is still helpful - not all features are exposed through IDEs.\nWhen you open a project that contains a Git repository in RStudio you will see a Git pane, typically on the right-hand-side in the top panel. We will explore how to interact with this briefly in the labs.\n\n\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “WaveLab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer New York. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nKnuth, Donald E. 1992. Literate Programming. Center for the Study of Language and Information Publication Lecture Notes. Stanford, CA: Centre for the Study of Language & Information.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#objectives",
    "href": "week04.html#objectives",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "",
    "text": "Know the “tidy data” philosophy.\nKnow how to realise this in R using dplyr, tidyr and the tidyverse.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#motivation",
    "href": "week04.html#motivation",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.1 Motivation",
    "text": "4.1 Motivation\nAs a data scientist, data is central. The problem is that data can be highly heterogeneous; the best way to organise it is different when it is stored and used for analysis. Moreover, the best way to organise it depends on what kind of analysis you intend to do.\n\n\n\n\n\n\nExample: Aircraft Data\n\n\n\nAn aircraft fleet manager has many aircraft. Each aircraft can have 2 or 4 engines. Extensive in-flight data about the aircraft is collected, including information about the engines (e.g. temperatures, pressures, thrust produced, fuel consumption rates) and the aircraft themselves (e.g. total number of passengers, source and destination airports, GPS coordinates, heights). The best way to store all of this information might be in a series of tables in a relational database (see Week 9).\nWhen analysing the data, it is helpful to have all the explanatory variables in a single matrix, and the response variables in a vector, one row per observation. Depending on what we are modelling, the shape of this data may change. For example: 1. If we are interested in flight time as a function of aircraft weight, we would have one row per journey. 2. If we are interested in engine performance as a function of aircraft height, we would have one row per engine monitoring record. The two datasets would have (significantly) different sizes, and moreover, would be completely different to the structure of the database in which the data is stored.\n\n\nThe goal of tidying data is to obtain a standardized data representation to allow the analysis itself to be as standardized as possible. Of course, the analysis will still be highly domain dependent, but if the functions we write and algorithms we use depend on a standardized data representation, it can significantly reduce the amount of application-specific coding that’s required whenever we encounter a new problem.\n\n\n\n\n\n\nImportant\n\n\n\nIn your assessments you will be allowed to re-use code you have written before the assessment! It is therefore helpful to understand the standardization process so that you can maximise the reusability of your code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#the-tidy-data-philosophy",
    "href": "week04.html#the-tidy-data-philosophy",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.2 The Tidy Data Philosophy",
    "text": "4.2 The Tidy Data Philosophy\nThe Tidy Data Philosophy is outlined in (Wickham, Çentnkaya-Rundel, and Grolemund 2023, sec. 5). In brief, this specifies that we should organise our data for analysis into a data frame such that:\n\nEach variable is a column.\nEach observation is a row.\nEach cell contains a single value.\n\nHere is an example borrowed from (Wickham, Çentnkaya-Rundel, and Grolemund 2023, sec. 5.2). Below we have some data organised in three different ways. Each shows the same values of four variables: country, year and population, and number of documented cases of Tuberculosis, but organises the data in different ways.\ntable1\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\ntable2\n#&gt; # A tibble: 12 × 4\n#&gt;   country      year type           count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999 cases            745\n#&gt; 2 Afghanistan  1999 population  19987071\n#&gt; 3 Afghanistan  2000 cases           2666\n#&gt; 4 Afghanistan  2000 population  20595360\n#&gt; 5 Brazil       1999 cases          37737\n#&gt; 6 Brazil       1999 population 172006362\n#&gt; # ℹ 6 more rows\n\ntable3\n#&gt; # A tibble: 6 × 3\n#&gt;   country      year rate             \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1 Afghanistan  1999 745/19987071     \n#&gt; 2 Afghanistan  2000 2666/20595360    \n#&gt; 3 Brazil       1999 37737/172006362  \n#&gt; 4 Brazil       2000 80488/174504898  \n#&gt; 5 China        1999 212258/1272915272\n#&gt; 6 China        2000 213766/1280428583\nOne of these, table1, will be much easier to work with because it is tidy.\nTidying data has several advantages:\n\nIt plays nicely with Data Frame representations\nWe can create standard algorithms that understand how to work with tidy data.\nWe can plug it into visualisation routines.\n\nMoreover, because of vectorised computation in R, it can be considerably faster to store your data is this way.\n\n\n\n\n\n\nNote\n\n\n\nAgain, a tidy format for analysis is not always the same as the storage format. Some datasets can be too large and complex to be stored as a square data frame like this, even though they can easily be stored and queried in more complex formats. Likewise, when you have “tidied” a dataset for your analysis you may not want always to save it to disk. This can cause problems if the underlying data changes and, depending on how you store the data, it could be considerably larger on disk than when stored in memory. On the other hand it can have speed advantages to cache the tidied data if it is static.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#the-tidyverse-ecosystem",
    "href": "week04.html#the-tidyverse-ecosystem",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.3 The tidyverse Ecosystem",
    "text": "4.3 The tidyverse Ecosystem\nThe tidyverse is a collection of R packages with a shared design philosophy. The core packages of the tidyverse are:\n\nggplot2 for visualisation, using the Grammar of Graphics (Week 6),\ndplyr which provides data manipulation tools,\ntidyr for transforming datasets to a tidy format,\nreadr for reading rectangular data in a variety of formats,\npurrr, a functional programming toolkit that reduces the need for loops,\ntibble for a more modern data frames,\nstringr, providing string manipulation tools,\nforcats for manipulating factor variables in R, and\nlubridate for date manipulation.\n\nHowever, the tidyverse can be installed and loaded as a single bundle of packages using\ninstall.packages(\"tidyverse\")\nand\nlibrary(tidyverse)\n\n4.3.1 An Opinionated Library\nThe tidyverse describes itself as opinionated, meaning that it is designed to do data analysis in a particular way, i.e. it forces users to conform to a particular style. This has some advantages; if you are happy with the style of the tidyverse then you can accomplish some very powerful things with minimal code. On the other hand, if you want to do things that are outside the focus of the tidyverse it can be much more complicated. This can be particularly challenging when it comes to data visualisation tools like ggplot2, where some visualisation types may not be supported.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#loading-data",
    "href": "week04.html#loading-data",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.4 Loading Data",
    "text": "4.4 Loading Data\nData loading is a fundamental skill in data science. Real world data comes in various formats and from different sources. R provides many built-in functions and packages to efficiently import data from various sources. This section covers the most common data loading techniques you’ll encounter in practice.\n\n4.4.1 Built-in Data Loading Functions\n\n4.4.1.1 CSV Files\nCSV stands for “Comma Separated Values”, meaning that the data is stored in plain text with commas separating the columns and new lines separating the rows. The Palmer Penguins dataset was stored in CSV; here are some example rows from the raw file:\n\"rowid\",\"species\",\"island\",\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\",\"sex\",\"year\"\n1,\"Adelie\",\"Torgersen\",39.1,18.7,181,3750,\"male\",2007\n2,\"Adelie\",\"Torgersen\",39.5,17.4,186,3800,\"female\",2007\n3,\"Adelie\",\"Torgersen\",40.3,18,195,3250,\"female\",2007\nIn this case the file contains a header row with column names and then multiple following rows with the data.\nCSV files are among the most common data formats. tidyverse provides several functions for loading them through the readr component, which is automatically loaded with library(tidyverse).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- read_csv(\"data/penguins.csv\")\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rowid, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ntidyverse load operations return data in tibble format, a modern reimagining of R’s core data.frame. Surprisingly, typeof(data) returns list; this is because the core data type of a tibble is actually a list. We can check whether an object is a tibble using several functions:\n\nis_tibble(data) \n\n[1] TRUE\n\nis.data.frame(data)\n\n[1] TRUE\n\nclass(data) \n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\n\n4.4.1.2 Other Delimited Files\nIn a csv file the comma is called a delimeter; it is used to delimit columns. It’s possible to store data in the same textual format, but with other delimeters. For example, this is what Palmer Penguins might look like if it was delimited with pipes (|) instead of commas:\n\"rowid\"|\"species\"|\"island\"|\"bill_length_mm\"|\"bill_depth_mm\"|\"flipper_length_mm\"|\"body_mass_g\"|\"sex\"|\"year\"\n1|\"Adelie\"|\"Torgersen\"|39.1|18.7|181|3750|\"male\"|2007\n2|\"Adelie\"|\"Torgersen\"|39.5|17.4|186|3800|\"female\"|2007\n3|\"Adelie\"|\"Torgersen\"|40.3|18|195|3250|\"female\"|2007\nOf course, we can’t use read_csv to load a file like this! Instead we would need to use the function read_delim, which takes a delimeter as an optional argument. If we had a version of Palmer Penguins with pipe delimeters stored in data/penguins_pipe.txt, we could load it with:\ndata = read_delim(\"data/penguins_pipe.txt\", delim=\"|\")\nKnowing which loading function to use for plain text input will typically involve looking at the file and experimenting until the data loads successfully. We will explore data loading issues in more detail in the lab.\n\n\n4.4.1.3 Other Loading Functions\nIn this course we will focus on delimited text files and, later, loading from relational databases. However, there are hundreds of other file types that you might encounter in the real world. Here are some examples:\n\nExcel Files: Using the read_excel function from tidyerse.\nparquet files, increasingly popular for big data. Uses the arrow package.\nJSON or XML files - common web formats. These can be read with jsonlite or xml2.\nStata, SPSS or SAS files, using the haven package (which is a part of tidyverse).\n\n\n\n\n\n\n\nTip\n\n\n\nYou will only be assessed on your ability to read delimited files and load data from databases. We might see some of the other formats in labs, but they will not appear in assessments.\n\n\n\n\n\n4.4.2 Core tidyverse Data Operations\nTo create tidy data the main operations are\n\nData Import: (readr::read_csv() or, later, dbplyr)\nTransformations (dplyr)\nReshaping (tidyr)\n\nWe will look at these, as well as some other common tidyverse libraries and functions, in the labs this week and next.\nIt is standard to use pipes (|&gt; and %&gt;%) to chain together operations in a readable, step-by-step way. Moreover tidyverse is tightly integrated with plotting libraries like ggplot2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#dplyr",
    "href": "week04.html#dplyr",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.5 dplyr",
    "text": "4.5 dplyr\nThe purpose of dplyr is to transform and summarise data frames in a readable, consistent way. As with git it consists of core verbs that are intended to be human-readable, e.g.\n\nmutate() - add or transform columns\nselect() - subset columns\nfilter() - subset rows\nsummarise() - reduces multiple values down to a single summary\narrange() - reorder rows\n\nAs an example, consider the gapminder dataset. This provides country-level data on life expectancy, GDP per capita and population, over time.\n\nlibrary(tidyverse)\nlibrary(gapminder)\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nAs we can see, the gapminder is an r tibble, with 1704 rows and 6 columns. Suppose we want to compute the average GDP and life expectancy of countries in Asia in 2007. Here’s an example using dplyr to accomplish this.\n\ngapminder |&gt; \n    filter(year == 2007, continent == \"Asia\") |&gt;\n    mutate(gdp_billions = gdpPercap * pop / 1e9) |&gt;\n    arrange(desc(lifeExp)) |&gt;\n    select(country, lifeExp, gdp_billions) |&gt;\n    summarise(mean_life_expectancy=mean(lifeExp), mean_gdp=mean(gdp_billions))\n\n# A tibble: 1 × 2\n  mean_life_expectancy mean_gdp\n                 &lt;dbl&gt;    &lt;dbl&gt;\n1                 70.7     628.\n\n\nThe code above is human readable - each row describes an operation applied to the dataset, chained together with pipes |&gt;. In other words, we can look at the code and see that we\n\nTook the gapminder dataset,\nApplied a filter to the year and continent,\nAdded a computed column (gdp_billions),\nOrdered by lifeExp,\nSelected the relevant columns.\nUsed summarise to compute the mean of both life expectancy and GDP.\n\n\n\n\n\n\n\nOther Summarise Functions\n\n\n\nsummarise works with most of the functions you would expect, including things like mean, median, sum, min, max. We can also count the number of rows using n() and the number of unique values with n_distinct().\n\n\nIf we choose any pipe |&gt; and delete the pipe and everything after it, we still get a tibble output which just applies the subset of operations before the pipe. We can thus interpret the pipes as “chaining together” the operations we want to apply.\nNote that this can have an impact on cost. For example, consider this example:\n\ngapminder |&gt; \n    mutate(gdp_billions = gdpPercap * pop / 1e9) |&gt;\n    arrange(desc(lifeExp)) |&gt;\n    filter(year == 2007, continent == \"Asia\") |&gt;\n    select(country, lifeExp, gdp_billions)\n\n# A tibble: 33 × 3\n   country          lifeExp gdp_billions\n   &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 Japan               82.6       4035. \n 2 Hong Kong, China    82.2        277. \n 3 Israel              80.7        164. \n 4 Singapore           80.0        215. \n 5 Korea, Rep.         78.6       1145. \n 6 Taiwan              78.4        666. \n 7 Kuwait              77.6        119. \n 8 Oman                75.6         71.5\n 9 Bahrain             75.6         21.1\n10 Vietnam             74.2        208. \n# ℹ 23 more rows\n\n\nThe output is the same, but because we apply the mutate operation before the filter, we will compute gdp_billions for all 1704 rows before discarding all but 23 rows. This is wasteful, and can have a significant performance implication on larger datasets!\n\n4.5.1 Grouping\nGrouping data is a powerful feature in dplyr that allows you to perform operations within subsets of your data. The group_by() function is used to specify one or more variables to group by, and then you can apply summary functions or transformations to each group.\nFor example, suppose we want to compute the average life expectancy for each continent in the gapminder dataset. We can first group by the continent:\n\ngrouped = gapminder |&gt;\n    group_by(continent)\ngrouped\n\n# A tibble: 1,704 × 6\n# Groups:   continent [5]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe result tells us that we have a tibble that has been separated into subgroups, and shows the first of the groups. We can access other groups as follows:\n\ngroup_keys(grouped) # view what groups are available\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Africa   \n2 Americas \n3 Asia     \n4 Europe   \n5 Oceania  \n\ngroup_split(grouped)[[5]] # access the Oceania group\n\n# A tibble: 24 × 6\n   country   continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Australia Oceania    1952    69.1  8691212    10040.\n 2 Australia Oceania    1957    70.3  9712569    10950.\n 3 Australia Oceania    1962    70.9 10794968    12217.\n 4 Australia Oceania    1967    71.1 11872264    14526.\n 5 Australia Oceania    1972    71.9 13177000    16789.\n 6 Australia Oceania    1977    73.5 14074100    18334.\n 7 Australia Oceania    1982    74.7 15184200    19477.\n 8 Australia Oceania    1987    76.3 16257249    21889.\n 9 Australia Oceania    1992    77.6 17481977    23425.\n10 Australia Oceania    1997    78.8 18565243    26998.\n# ℹ 14 more rows\n\n\nFar more useful, though, is to use the summarise command to aggregate the groups directly, producing a new tibble with the results. The below command does this:\n\ngapminder |&gt;\n    group_by(continent) |&gt;\n    summarise(avg_lifeExp = mean(lifeExp, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  continent avg_lifeExp\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Africa           48.9\n2 Americas         64.7\n3 Asia             60.1\n4 Europe           71.9\n5 Oceania          74.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#tidyr",
    "href": "week04.html#tidyr",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.6 tidyr",
    "text": "4.6 tidyr\nThe gapminder dataset is a tall dataset; it contains multiple rows for each country and continent. tidyr contains pivot operations to allow us to convert tall datasets into wide datasets, for example:\n\ngapminder_wide = gapminder |&gt; \n    select(country, year, lifeExp) |&gt;\n    pivot_wider(names_from=year, values_from=lifeExp)\ngapminder_wide\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nNow we end up with a 142 \\(\\times\\) 13 tibble whose entries are lifeExp. We can use the “wide” dataset to easily calculate the change in life expecancy:\n\ngapminder_wide |&gt;\n    mutate(change = `2007` - `1952`) |&gt;\n    select(country, change)\n\n# A tibble: 142 × 2\n   country     change\n   &lt;fct&gt;        &lt;dbl&gt;\n 1 Afghanistan   15.0\n 2 Albania       21.2\n 3 Algeria       29.2\n 4 Angola        12.7\n 5 Argentina     12.8\n 6 Australia     12.1\n 7 Austria       13.0\n 8 Bahrain       24.7\n 9 Bangladesh    26.6\n10 Belgium       11.4\n# ℹ 132 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe backquotes, above, (around 2007 and 1952) are needed to tell R that these are not to be treated as numbers but as literal column names. They would also be needed if for column names containing spaces or symbols like + or $.\n\n\nThe “inverse” of pivot_wider is pivot_longer:\n\ngapminder_wide |&gt;\n    pivot_longer(\n        cols=colnames(gapminder_wide)[2:ncol(gapminder_wide)], \n        names_to=\"year\", \n        values_to=\"lifeExp\"\n    )\n\n# A tibble: 1,704 × 3\n   country     year  lifeExp\n   &lt;fct&gt;       &lt;chr&gt;   &lt;dbl&gt;\n 1 Afghanistan 1952     28.8\n 2 Afghanistan 1957     30.3\n 3 Afghanistan 1962     32.0\n 4 Afghanistan 1967     34.0\n 5 Afghanistan 1972     36.1\n 6 Afghanistan 1977     38.4\n 7 Afghanistan 1982     39.9\n 8 Afghanistan 1987     40.8\n 9 Afghanistan 1992     41.7\n10 Afghanistan 1997     41.8\n# ℹ 1,694 more rows\n\n\n\n\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#objectives",
    "href": "week05.html#objectives",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "",
    "text": "Describe “messier” data types structures that might be encountered in the real world.\nBe able to join tables together using join functions.\nBe able to deal with date and string types.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#joins-connecting-multiple-tables",
    "href": "week05.html#joins-connecting-multiple-tables",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.1 Joins: Connecting Multiple Tables",
    "text": "5.1 Joins: Connecting Multiple Tables\nA join combines rows from two or mor tables based on a related column between them. There are various types of joins depending on what kinds of data you want to join; we’ll go through each of them. To illustrate we will use the (synthetic) student data from student_data.csv and student_grades.csv.\n\n#|\nlibrary(tidyverse)\nstudent_data = read_csv(\"data/student_data.csv\")\nstudent_grades = read_csv(\"data/student_grades.csv\")\nhead(student_data)\n\n# A tibble: 6 × 4\n     ID `Given Name` `Family Name` `Date of Birth`\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt;\n1     1 Adaora       Nkomo                   37205\n2     2 Hiroshi      Tanaka                  37845\n3     3 Aaliyah      Williams                37505\n4     4 Dimitri      Volkov                  37717\n5     5 Amara        Baptiste                37387\n6     6 Rajesh       Krishnamurthy           38218\n\nhead(student_grades)\n\n# A tibble: 6 × 3\n  `Student ID` Course   Grade\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1            1 MATH6195    68\n2            2 MATH6195    67\n3            3 MATH6195    68\n4            4 MATH6195    74\n5            5 MATH6195    52\n6            6 MATH6195    85\n\n\nWe have two tables, and they are linked by a common ID column (called Student ID in student_grades). Names and dates of birth are available in student_data but the student grades are in student_grades. Each student takes multiple courses, so the relationship between the student and grade tables is one-to-many.\n\n\n\n\n\n\nWhy Store Data Like This?\n\n\n\nStoring in multiple tables is good practice for multiple reasons:\n\nData size: If we stored it in a single flat table, the student name would be repeated for every course they take. This can make the data files significantly larger!\nConsistency and Integrity: If a student changes their name, it only needs to be updated in one place.\nFlexibilty: Adding new attributes is much easier. For example, if we wanted to start recording student reference requests, we could add a new student_references table, with a one-to-many relationship with student_data but no link at all to student_grades.\nClarity: It’s easier to think about the data structure if it is logically separated like this (e.g. “each student takes multiple courses”).\n\n\n\n\n5.1.1 Inner Join\nAn inner join takes two tables and joins them together into a single table on based on the matching column, returning only rows that exist in both tables.\n\nresult = inner_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nhead(result)\n\n# A tibble: 6 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course   Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 Adaora       Nkomo                   37205 MATH6195    68\n2     1 Adaora       Nkomo                   37205 MATH2010    59\n3     1 Adaora       Nkomo                   37205 MATH2011    58\n4     2 Hiroshi      Tanaka                  37845 MATH6195    67\n5     2 Hiroshi      Tanaka                  37845 MATH2010    72\n6     2 Hiroshi      Tanaka                  37845 MATH2011    72\n\n\nNotice how we have the union of columns from both tables, apart from the ID column which we joined on. Let’s look at the rows.\n\nnrow(result)\n\n[1] 53\n\nnrow(student_grades)\n\n[1] 59\n\n\nSee result has fewer columns than student grades. This must mean there are some student IDs in the student_grades table that are not in the student_data table. Let’s check:\n\nsetdiff(student_grades$`Student ID`, student_data$ID)\n\n[1] 21 22\n\n\nThere are two missing students from student_data. Because this is an inner join these were excluded from the final table. Let’s now check if there are any students in student_data that are not present in the result.\n\nsetdiff(student_data$ID, result$ID)\n\n[1] 7\n\n\nIt seems the student with ID 7 is missing:\n\nresult |&gt;\n    filter(ID == 7)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: ID &lt;dbl&gt;, Given Name &lt;chr&gt;, Family Name &lt;chr&gt;,\n#   Date of Birth &lt;dbl&gt;, Course &lt;chr&gt;, Grade &lt;dbl&gt;\n\n\nTo check why this might be, we can look for that student in the student_grades table:\n\nstudent_grades |&gt;\n    filter(`Student ID` == 7)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Student ID &lt;dbl&gt;, Course &lt;chr&gt;, Grade &lt;dbl&gt;\n\n\nSo the student is missing because they have no grade records, so the inner join excludes them from the final table. Hopefully they have applied for special considerations!\n\n\n5.1.2 Left / Right Join\nLeft and right join keep all the records in the left table or the right table, respectively, in the join statement. Let’s see what the result looks like with a left join instead:\n\nresult = left_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nresult |&gt; filter(ID == 7)\n\n# A tibble: 1 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     7 Lin          Yifan                   38146 &lt;NA&gt;      NA\n\n\nNow the student who has no student_grades records is still present in the final table. However, because there are no grade records, the table is filled with &lt;NA&gt; under Course and Grade. Right join is essentially the same, but keeps records in the right table.\n\n\n5.1.3 Outer Join\nOuter join keeps records in both tables. For some reason R calls this a full_join, which is not standard terminology.\n\nresult = full_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nresult |&gt; filter(ID == 7)\n\n# A tibble: 1 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     7 Lin          Yifan                   38146 &lt;NA&gt;      NA\n\n\nWe see that the result contains the student with missing grades, but it will also contain the two students who were missing in the student_data table:\n\nresult |&gt; filter(ID %in% c(21,22))\n\n# A tibble: 6 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course   Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH6195    70\n2    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH6195    89\n3    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2010    71\n4    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2010    56\n5    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2011    62\n6    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2011    84\n\n\nNow the Given Name, Family Name and Date of Birth fields are all &lt;NA&gt;.\n\n\n5.1.4 Other Types of Join\nThere are some other types of join which are much less common to encounter. Here are some examples:\n\nsemi_join(x, y, by=\"id\"): keeps rows in x that have a match in y but does not add columns from y.\nanti_join(x, y, by=\"id\"): keeps rows in x that do not have a match in y, but again does not add columns from y.\ncross_join(x,y): gives the cartesian product of x and y, that is, the tibbles are not matched on any common column, but rather all possible combinations of rows in x and y appear.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#strings",
    "href": "week05.html#strings",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.2 Strings",
    "text": "5.2 Strings\nWe often need to deal with purely textual data in data science. While it would be nice for all the data we receive to be highly structured, we frequently get “free text” input from users, where the input may need to be cleaned or otherwise processed before it can be used. In tidyverse this is the role of the stringr library, which provides multiple functions (usually prefixed with str_) that are designed to work nicely with the tibble format. We will look at some string manipulations in the lab, including:\n\nstr_length: for getting the length of a string.\nstr_sub: for taking a substring (the subset of the string between two indices)\nstr_to_lower, str_to_upper, str_to_title: for standardising cases.\nstr_trim, str_squish: for removing leading/trailing whitespace or too many internal spaces.\n\nAll of these can be applied to either a single string or an array of strings, i.e.:\n\nstr_length(\"Hello World\")\n\n[1] 11\n\nstr_length(c(\"Alice\", \"Bob\", \"Dave\"))\n\n[1] 5 3 4\n\n\n\n\n\n\n\n\nRegular Expressions\n\n\n\nThere is a more advanced text processing tool called a regular expression this is essentially a “mini language” that allows a user to describe patterns they want to look for in text. This could be used, for example, to detect phone numbers or email addresses that have been entered in “free text” boxes in surveys. We won’t cover regular expressions in this course, but they are worth knowing about. A good resource for learning is regex101.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#dates",
    "href": "week05.html#dates",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.3 Dates",
    "text": "5.3 Dates\nWhen we imported the student data above, there was a column called “Date of Birth” - but it did not appear to contain dates:\n\nstudent_data$`Date of Birth`\n\n [1] 37205 37845 37505 37717 37387 38218 38146 37596 38167 37590 37472 37803\n[13] 37283 37852 37900 37865 38162 37870 37282 37481\n\ntypeof(student_data$`Date of Birth`[1])\n\n[1] \"double\"\n\n\nThe numbers above are Excel format dates. They are (roughly) the number of days that have elapsed since December 31, 1989. In theory we could process these dates using this knowledge, but it is much more convenient to convert the number to a proper date type in R. This (and more) functionality is provided by lubridate, a package in tidyverse.\n\n5.3.1 Loading and Converting Dates\nLoading dates is surprisingly complex because there is no standard format. For example, here are a variety of perfectly valid ways to represent the date “11th August 2025”:\n\ndates = c(\n    '2025-08-11',\n    '11/08/2025',\n    '08/11/2025',\n    '11 August, 2025',\n    '11-Aug-2025'\n)\n\nThe list above is non-exhaustive, so some domain knowledge will always be required to load dates. R may try to infer a date format, but it might be wrong. It is always safer to specify the date format, or at least, to carefully check that dates have been loaded correctly before doing any data analysis.\nlubridate provides several helper functions to convert from some standard formats, that are fairly robust to different sub conventions like delimeters or padding.\n\ntoday1 = ymd(\"2025-August-11\")\ntoday2 = dmy(\"11/08/25\")\ntoday3 = mdy(\"8.11.2025\")\nc(today1, today2, today3)\n\n[1] \"2025-08-11\" \"2025-08-11\" \"2025-08-11\"\n\n\nR has correctly inferred the date in all three cases. We can also use the as.Date function to customise parsing further, by providing a format string to describe how the date string is expected to be formatted. For more information on formatting see the as.Date documentation.\nNote that while the dates are printed as strings, they do in fact have a date type:\n\nclass(today1)\n\n[1] \"Date\"\n\n\nFor the Excel date example, if we read Excel files with readxl there won’t be a problem. But, sometimes Excel dates make their way into other file formats, and we can use the janitor package to load them.\n\nlibrary(janitor)\nstudent_data$`Date of Birth` = excel_numeric_to_date(student_data$`Date of Birth`)\nstudent_data$`Date of Birth`\n\n [1] \"2001-11-10\" \"2003-08-12\" \"2002-09-06\" \"2003-04-06\" \"2002-05-11\"\n [6] \"2004-08-19\" \"2004-06-08\" \"2002-12-06\" \"2004-06-29\" \"2002-11-30\"\n[11] \"2002-08-04\" \"2003-07-01\" \"2002-01-27\" \"2003-08-19\" \"2003-10-06\"\n[16] \"2003-09-01\" \"2004-06-24\" \"2003-09-06\" \"2002-01-26\" \"2002-08-13\"\n\n\n\n\n5.3.2 Date Processing\nOnce dates are in the proper format, there exist various functions to process them. For example, we can extract the year, month or day:\n\nc(year(today()), month(today()), day(today()))\n\n[1] 2025   11    4\n\n\nFor example, if we want to compute a student’s age we can use lubridate’s interval function followed by time_length\n\nstudent_data$Age = interval(student_data$`Date of Birth`, today()) |&gt;\n    time_length(\"years\")\nstudent_data$Age\n\n [1] 23.98356 22.23014 23.16164 22.58082 23.48493 21.21096 21.40822 22.91233\n [9] 21.35068 22.92877 23.25205 22.34521 23.76986 22.21096 22.07945 22.17534\n[17] 21.36438 22.16164 23.77260 23.22740\n\nstudent_data$Age |&gt; floor()\n\n [1] 23 22 23 22 23 21 21 22 21 22 23 22 23 22 22 22 21 22 23 23\n\n\nOr, we could use as.period to get a precise calendar difference:\n\ninterval(student_data$`Date of Birth`, today()) |&gt;\n    as.period(unit=\"years\")\n\n [1] \"23y 11m 25d 0H 0M 0S\" \"22y 2m 23d 0H 0M 0S\"  \"23y 1m 29d 0H 0M 0S\" \n [4] \"22y 6m 29d 0H 0M 0S\"  \"23y 5m 24d 0H 0M 0S\"  \"21y 2m 16d 0H 0M 0S\" \n [7] \"21y 4m 27d 0H 0M 0S\"  \"22y 10m 29d 0H 0M 0S\" \"21y 4m 6d 0H 0M 0S\"  \n[10] \"22y 11m 5d 0H 0M 0S\"  \"23y 3m 0d 0H 0M 0S\"   \"22y 4m 3d 0H 0M 0S\"  \n[13] \"23y 9m 8d 0H 0M 0S\"   \"22y 2m 16d 0H 0M 0S\"  \"22y 0m 29d 0H 0M 0S\" \n[16] \"22y 2m 3d 0H 0M 0S\"   \"21y 4m 11d 0H 0M 0S\"  \"22y 1m 29d 0H 0M 0S\" \n[19] \"23y 9m 9d 0H 0M 0S\"   \"23y 2m 22d 0H 0M 0S\" \n\n\nWe could also just subtract the dates, but this gives a difference in terms of days which would be difficult to convert to years (due to leap years).\n\ntoday() - student_data$`Date of Birth`\n\nTime differences in days\n [1] 8760 8120 8460 8248 8578 7747 7819 8369 7798 8375 8493 8162 8682 8113 8065\n[16] 8100 7803 8095 8683 8484\n\n\n\n\n5.3.3 Times and Time Zones\nTimes can be handled similarly to dates. There are routines for loading times that are similar to the date-specific routines described above:\n\ntime1 = ymd_hms(\"2025-08-11 11:33:01\")\ntime1\n\n[1] \"2025-08-11 11:33:01 UTC\"\n\ntime2 = now()\ntime2\n\n[1] \"2025-11-04 13:52:18 GMT\"\n\n\nWe can then extract hours, minutes and seconds:\n\nc(hour(time1), minute(time1), second(time1))\n\n[1] 11 33  1\n\n\nOne extra consideration with times is the time zone. For example, we can see that the time zone of time1 and time2 differ, above:\n\nc(tz(time1), tz(time2))\n\n[1] \"UTC\" \"\"   \n\n\nNote that tz(time2) returns an empty string, which is standard in R for the current time zone. If we want to be explicit we can force now to be in a particular time zone, e.g.\n\ntime3 = now(\"Asia/Tokyo\")\ntz(time3)\n\n[1] \"Asia/Tokyo\"\n\n\nWe can also change the time zone on times using the with_tz function:\n\nwith_tz(time1, \"Asia/Tokyo\")\n\n[1] \"2025-08-11 20:33:01 JST\"\n\ntime1\n\n[1] \"2025-08-11 11:33:01 UTC\"\n\n\nNotice that this adjusts the time from the previous time zone (UTC) to match the new time zone (JST). If we want to add a time zone without changing the time, we use the force_tz function:\n\nforce_tz(time1, \"Asia/Tokyo\")\n\n[1] \"2025-08-11 11:33:01 JST\"\n\n\nNow the time is the same as time1, but the time zone has been modified.\n\n\n5.3.4 Formatting Dates and Times\nBy default invoking today1 in R uses the specific format 2025-08-11. We may wish to use another format, for which we can invoke the format function. This takes a date and a format string (as per as.Date, above), and outputs a string in that format. For example:\n\nformat(today1, \"%Y-%m-%d\")\n\n[1] \"2025-08-11\"\n\nformat(today1, \"%d %b %y\")\n\n[1] \"11 Aug 25\"\n\nformat(time1, \"%d %b %y %H:%M\")\n\n[1] \"11 Aug 25 11:33\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "6  Data Visualisation Principles",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#objectives",
    "href": "week06.html#objectives",
    "title": "6  Data Visualisation Principles",
    "section": "",
    "text": "Understand the grammar of graphics framework.\nBe able to critique visualisations for clarity and effectiveness.\nKnow how visual perception affects the interpretation of plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#the-grammar-of-graphics-framework",
    "href": "week06.html#the-grammar-of-graphics-framework",
    "title": "6  Data Visualisation Principles",
    "section": "6.1 The Grammar of Graphics Framework",
    "text": "6.1 The Grammar of Graphics Framework\nThe Grammar of Graphics (Wilkinson 2005) is implemented in R through the library ggplot2, a core library in tidyverse. It provides a systematic approach to constructing data visualisations. Rather than thinking about chart types (bar chart, scatter plot, etc.), the grammar breaks down visualisations into fundamental components that can be combined in flexible ways.\n\n6.1.1 Core Components of the Grammar\nThe grammar consists of several key layers:\nData: The dataset being visualised. In ggplot2 this is assumed to be in tidy format. Aesthetics: referred to as aes in ggplot2. These are the visual properties that represent data variables, including:\n\nPosition (\\((x,y)\\) coordinates)\nColour (hue, saturation)\nShape (point shapes, line types)\nSize (point size, line width)\nTransparency (alpha values)\n\nGeometries: the geometric objects that represent data points. How these are referred to in ggplot2 is included in parentheses below.\n\nPoints (geom_point())\nLines (geom_line())\nBars (geom_bar(), geom_col())\nAreas (geom_area())\nText (geom_text())\n\nStatistics (stat): (optional) statistical transformations applied to data before plotting, e.g.:\n\nCount (frequency tables)\nSmoothing (trend lines)\nBinning (histograms)\n\nScales: How aesthetics translate data values to visual properties:\n\nContinuous scales (for numeric data)\nDiscrete scales (for categorical data)\nColour scales (viridis colour maps, brewer colour palettes)\nPosition scales (log or square-root transformations)\n\nCoordinate Systems: How data coordinates map to the plane of the plot:\n\nCartesian coordinates (most common)\nPolar coordinates\nMap projections\n\nFaceting: Break data into subsets for producing multiple smaller charts:\n\nfacet_wrap(): arranges panels in rows and columns\nfacet_grid(): arranging panels in a grid based on variables.\n\n\n\n6.1.2 Constructing Plots by Composing Components\nThe grammar allows components to be composed with one another by “adding” them to a plot to build visualisations incrementally. Here are multiple examples using the Palmer penguins dataset; we will go into building these in much more detail in the lab.\n\nlibrary(tidyverse)\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  facet_wrap(~island) +\n  labs(\n    title = \"Penguin measurements by species, faceted by island\",\n    x = \"Flipper length (mm)\",\n    y = \"Body Mass (g)\"\n  ) + \n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(colour = species), alpha = 0.6, size = 2) + # scatter points\n  geom_smooth(aes(colour = species), method = \"lm\", se = TRUE) + # trend-line per species\n  geom_smooth(method = \"lm\", colour = \"black\", linetype = \"dashed\", se = FALSE) + # overall trend line\n  scale_colour_viridis_d(name = \"Species\") + # Species colour scale\n  labs(\n    title = \"Multiple Geometries: Points + Trend Lines\",\n    subtitle = \"Species-specific trends (colored) vs overall trend (dashed)\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 2 rows containing non-finite outside the scale range (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(aes(y = after_stat(density), fill = species), \n                 alpha = 0.7, bins = 30, position = \"identity\") + # Histogram of densities (rather than counts)\n  geom_density(aes(colour = species), size = 1.2) + # Add density lines\n  scale_fill_viridis_d(name = \"Species\", alpha = 0.7) + \n  scale_colour_viridis_d(name = \"Species\") + # colour lines and fill blocks with Viridis D\n  labs(\n    title = \"Statistical Transformations: Histogram + Density\",\n    x = \"Body Mass (g)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = body_mass_g, size = flipper_length_mm), alpha = 0.7) +\n  scale_colour_viridis_c(name = \"Body Mass (g)\", trans = \"log10\") +\n  scale_size_continuous(name = \"Flipper Length (mm)\", range = c(1, 8)) +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\") +\n  labs(\n    title = \"Scale Transformations: Log Scales\",\n    subtitle = \"Both axes and colour scale transformed\",\n    x = \"Bill Length (mm) - Log Scale\",\n    y = \"Bill Depth (mm) - Log Scale\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(ggforce)\npenguins %&gt;%\n  ggplot(aes(x = body_mass_g, y = species)) +\n  ggforce::geom_sina(aes(colour = island), size = 1.5, alpha = 0.6) +\n  geom_boxplot(alpha = 0.3, outlier.shape = NA, colour = \"black\") +\n  scale_colour_manual(name = \"Island\", values = c(\"#E31A1C\", \"#FF7F00\", \"#1F78B4\")) +\n  labs(\n    title = \"Advanced Composition: Sina Plot + Boxplot\",\n    subtitle = \"Combining ggforce geometries with standard ggplot2\",\n    x = \"Body Mass (g)\",\n    y = \"Species\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_sina()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#visual-perception-and-plot-interpretation",
    "href": "week06.html#visual-perception-and-plot-interpretation",
    "title": "6  Data Visualisation Principles",
    "section": "6.2 Visual Perception and Plot Interpretation",
    "text": "6.2 Visual Perception and Plot Interpretation\nYou may already have noticed that not all of the above plots are straightforward to interpret. ggplot gives us a lot of freedom to design our figures, but it does not tell us what are “good” and “bad” designs. For example, this plot gives the impression that welfare recipients in the US are spiralling out of control - but the \\(y\\)-axis starts from 94,000,000:\n\n\n\nPlot of Federal Welfare Recipients in the USA, from here\n\n\nThe next example was presented by General H.R. McMaster to US officials in 2009, intended to explain the complexity of US military strategy in Afghanistan. It led the New York Times to publish an article called We Have Met the Enemy, and He Is Powerpoint.\n\n\n\nWe Have Met the Enemy, and He Is Powerpoint, from here.\n\n\nCareful thought is needed to ensure that plots effectively convey the information that we want them to convey.\n\n6.2.1 Perceptual Accuracy of Visual Elements\n(Cleveland and McGill 1984) developed a theory of graphical perception, the process by which people decode quantitative information from visual representations. They argue that the effectiveness of a graph depends on how accurately viewers can visually judge the data or information that the graph encodes. They rank elementary perceptual tasks, like judging position along a common scale, length, area, angle, colour etc. by how precisely humans can perform them, based on controlled experiments.\nThey found that:\n\nPosition along a common scale is judged most accurately,\nLength, angle and slope are less accurate, with angle and slope worse than length,\nArea, volume or colour saturation are even less reliable.\n\nThese provide guidance for how we should encode data into graphs. For example, bar charts (measuring length), aligned on a common scale (height) might be preferred to pie charts (comparing angle and area).\nSince (Cleveland and McGill 1984), visualisation theory has continued to develop. The full theory is well beyond the scope of this course, but the main points are:\n\nNot all visual encodings are equal. As (Cleveland and McGill 1984) shows, position can be read most accurately, then length/angle, then area/colour.\nDesign with perception in mind. Prioritise encodings that make the main comparison easier, using the most informative visual encoding.\nModern tools help. ggplot2 is based on the (Wilkinson 2005), which builds on these ideas. The mapping of aes to geoms describe these perceptual choices.\n\n\n\n6.2.2 Colour Blindness Considerations\nAnother important concern is accessibility. Around 8% of men and 0.5% of women are affected by colour blindness, which can make some plots impossible to perceive for some users. It is also worthwhile to be mindful that your plots may be printed out in black-and-white or viewed on greyscale screens, so relying solely on colour to make points is better avoided.\nThe most common type of colour blindness is red-green deficiencies, so using red-green contrast should be avoided. Fortunately, ggplot makes it straightforward to adhere to this by providing perceptually uniform, colorblind-safe palettes such as viridis or the ColorBrewer safe sets. Where possible it is still best to make sure there is visual redundancy, rather than using colour alone, using label, shape or line type.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(size=3) +\n  scale_colour_manual(values=c(\"Adelie\" = \"red\", \"Chinstrap\" = \"green\", \"Gentoo\" = \"blue\")) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIt would be difficult for someone with red-green colour blindness to distinguish Adelie and Chinstrap penguins.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species, shape=species)) +\n  geom_point(size=3) +\n  scale_colour_brewer(type=\"qual\") + \n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing scale_colour_brewer with type=\"qual\" tells R that this is qualitative data (as opposed to sequential, where the factor ordering is meaningful); an appropriate colour palette is then selected automatically. The addition of shape=species also ensures that a viewer who finds colour perception challenging will still be able to interpret the plot.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week07.html",
    "href": "week07.html",
    "title": "7  Introduction to Shiny",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#objectives",
    "href": "week07.html#objectives",
    "title": "7  Introduction to Shiny",
    "section": "",
    "text": "Know what shiny is and what problems it solves.\nSee how a basic shiny app is structured and executed.\nBe able to describe reactive programming at a high level.\nRecognise common use cases for shiny, and its limitations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#what-is-shiny",
    "href": "week07.html#what-is-shiny",
    "title": "7  Introduction to Shiny",
    "section": "7.1 What is Shiny?",
    "text": "7.1 What is Shiny?\nShiny is an R package that makes it easy to build interactive web applications directly from R. Developed by RStudio (now Posit), Shiny allows users to turn analyses into interactive dashboards and tools without requiring knowledge of HTML, CSS, or JavaScript.\nShiny apps are especially useful for sharing data analyses with others, enabling users to explore data and results dynamically through a web browser. This interactivity helps bridge the gap between static reports and fully custom web applications.\nKey features of Shiny include: - Seamless integration with R code and packages. - Automatic UI updates in response to user input. - Support for a wide range of input and output widgets. - Ability to deploy apps locally or on the web (e.g., via shinyapps.io or Shiny Server).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#basic-structure-of-a-shiny-app",
    "href": "week07.html#basic-structure-of-a-shiny-app",
    "title": "7  Introduction to Shiny",
    "section": "7.2 Basic Structure of a Shiny App",
    "text": "7.2 Basic Structure of a Shiny App\nA basic Shiny app consists of two main components:\n\nUI (User Interface): Defines the layout and appearance of the app.\nServer: Contains the instructions for how the app responds to user input.\n\nHere is a minimal example of a Shiny app:\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Choose a number:\", 1, 100, 50)\n    ),\n    mainPanel(\n      textOutput(\"result\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  output$result &lt;- renderText({\n    paste(\"You selected:\", input$num)\n  })\n}\n\nshinyApp(ui, server)\nThe variable ui describes a simple user interface. The code entered into fluidPage is translated into an HTML page dynamically (using the bootstrap CSS library). The variable server describes the “back-end” server that waits for input from the UI. When the user changes something on the UI, the server function is called with the input. We can modify the output, and it is automatically passed back to the UI. This triggers the mainPanel variable to render the output stored in the result variable to the UI.\nThe example above produces the following output:\n\n\n\nRendered Shiny app: GUI",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#how-shiny-apps-are-executed",
    "href": "week07.html#how-shiny-apps-are-executed",
    "title": "7  Introduction to Shiny",
    "section": "7.3 How Shiny Apps are Executed",
    "text": "7.3 How Shiny Apps are Executed\nRunning Shiny apps within Quarto is not possible; they need to be written in R scripts. There are several ways to run Shiny apps in development:\n\nIn an interactive session, type the code shinyApp(ui, server).\nPass the app file name to the Shiny function runApp.\nBy clicking the “Run App” button in RStudio, that appears when you open a shiny app.\n\nIn all of the above, Shiny will start a web server on your computer. You should see an output that looks like:\nListening on http://127.0.0.1:3689\nBrowsing http://127.0.0.1:3689\nThis gives you a URL that you can enter into your browser to interact with the app. In RStudio the app will be opened automatically in a new window.\nThe execution flow is as follows:\n\nInitialization: The UI is generated and displayed in the browser.\nUser Interaction: When a user interacts with an input (e.g., moves a slider), the browser sends the new input value to the server.\nReactivity: The server detects which outputs depend on the changed input and recalculates them.\nOutput Update: The updated outputs are sent back to the browser and displayed instantly.\n\nYou can run Shiny apps:\n\nLocally: Directly from your R session, accessible only on your machine.\nRemotely: By deploying to a server (e.g., shinyapps.io), making the app accessible to others via a web link.\n\nThis architecture allows for real-time, interactive data exploration without requiring users to install R or any packages.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#reactive-programming",
    "href": "week07.html#reactive-programming",
    "title": "7  Introduction to Shiny",
    "section": "7.4 Reactive Programming",
    "text": "7.4 Reactive Programming\nReactive programming is a programming paradigm centred around the propagation of changes. In Shiny, this means that outputs automatically update in response to changes in inputs, without the need for explicit instructions to recalculate or redraw.\nShiny achieves this through a system of “reactive expressions” and “reactive values.” When a user interacts with an input widget (like a slider or text box), Shiny tracks which outputs depend on that input. Only the affected outputs are recalculated and updated, making the app efficient and responsive.\nKey concepts in Shiny’s reactive programming:\n\nInputs: User interface elements that collect information from the user (e.g., sliders, text boxes).\nOutputs: UI elements that display results or visualizations, updated automatically when their dependencies change.\nReactive expressions: Special functions (created with reactive()) that cache their results and only re-execute when their inputs change.\nObservers: Code blocks (created with observe() or observeEvent()) that perform actions in response to changes, such as updating UI elements or triggering side effects.\n\nWe have already seen inputs and outputs; now we will talk about reactive expressions and observers.\n\n7.4.1 Reactive Expressions\nReactivity in Shiny is the mechanism by which code automatically updates when inputs, or other reactive values, change. In the previous code block we already used a reactive function: renderText. This ensures that the server is called - and therefore the UI is updated - whenever input$num changes. In the background, R is building a graph of dependencies on values from the UI, and updates only the appropriate parts of the server logic when the dependencies change.\nThere are multiple other reactive functions\n\nreactive({...}): define a reactive expression that can be re-used in multiple places.\nreactiveVal: define new reactive values that you can then get or set.\nrenderText, renderPlot, renderTable, renderUI: create outputs for the UI.\n\nHere is an example in which we use reactive and renderPlot to make more complex logic.\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  fluidRow(\n    column(\n      2,\n      numericInput(\"mu1\", label = \"$\\\\mu_1$\", value = -1),\n      numericInput(\"mu2\", label = \"$\\\\mu_2$\", value = 1)\n    ),\n    column(9, plotOutput(\"plot\"))\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  x1 &lt;- data.frame(x = rnorm(500, input$mu1, 1))\n  x2 &lt;- data.frame(x = rnorm(500, input$mu2, 1))\n  output$plot &lt;- renderPlot(\n    {\n      ggplot() +\n        geom_freqpoly(data = x1, aes(x), color = \"red\", binwidth = 0.5, size = 1) +\n        geom_freqpoly(data = x2, aes(x), color = \"blue\", binwidth = 0.5, size = 1) +\n        coord_cartesian(xlim = c(-5, 5))\n    },\n    res = 96\n  )\n}\n\nshinyApp(ui, server)\n\n\n7.4.2 Observers\nObservers are another way to introduce reactivity. Whereas reactive statements typically return values and are designed to give reusable expressions, observe statements do not return values, and are used when one might want to run code with side effects (e.g. for logging) or to trigger UI updates manually.\nThe two main observer functions are:\n\nobserve({...}): triggers if any reactive value inside the block changes\nobserveEvent(event, {...}): explicitly triggered only by the event specified.\n\nHere’s an example showing how observeEvent can be used to run code on button clicks.\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  actionButton(\"inc\", \"Increase counter\"),\n  actionButton(\"reset\", \"Reset counter\"),\n  textOutput(\"value\")\n)\n\nserver &lt;- function(input, output, session) {\n  # Create a reactiveVal starting at 0\n  counter &lt;- reactiveVal(0)\n\n  # Increase counter when 'inc' button is clicked\n  observeEvent(input$inc, {\n    counter(counter() + 1)   # update value\n  })\n\n  # Reset counter when 'reset' button is clicked\n  observeEvent(input$reset, {\n    counter(0)\n  })\n\n  # Display the current value\n  output$value &lt;- renderText({\n    paste(\"Counter is:\", counter())   # read value\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#common-use-cases",
    "href": "week07.html#common-use-cases",
    "title": "7  Introduction to Shiny",
    "section": "7.5 Common Use Cases",
    "text": "7.5 Common Use Cases\nShiny is widely used in a variety of contexts where interactive data exploration, visualization, or reporting is valuable. Common use cases include:\n\nInteractive dashboards: Creating dynamic dashboards for monitoring key metrics, business intelligence, or research results.\nData exploration tools: Allowing users to filter, subset, and visualize data interactively without writing code.\nStatistical analysis apps: Providing interfaces for running statistical models, simulations, or hypothesis tests with user-specified parameters.\nEducational tools: Building interactive tutorials, quizzes, or demonstrations for teaching statistics, data science, or other quantitative subjects.\nReporting and decision support: Enabling stakeholders to explore scenarios, adjust assumptions, and view results in real time.\nPrototyping and sharing analyses: Rapidly developing and sharing prototypes of data products or analyses with collaborators or clients.\n\nShiny is especially useful when you want to empower non-programmers to interact with data or models, or when you need to communicate results in a more engaging and flexible way than static reports allow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week07.html#limitations",
    "href": "week07.html#limitations",
    "title": "7  Introduction to Shiny",
    "section": "7.6 Limitations",
    "text": "7.6 Limitations\nWhile Shiny is a powerful tool, it has several limitations:\n\nPerformance: Shiny apps can become slow with large datasets or complex computations, as all processing is handled on the server side and R is single-threaded by default.\nScalability: Each user session runs a separate R process, which can limit scalability for many simultaneous users unless additional infrastructure (e.g., Shiny Server Pro, load balancing) is used.\nSecurity: Shiny apps can expose your R environment to users, so care must be taken to sanitize inputs and avoid code injection vulnerabilities.\nDeployment: Deploying Shiny apps for public access often requires additional setup, such as using shinyapps.io or configuring a Shiny Server, which usually cost money.\nLimited UI Customization: While Shiny provides many UI components, highly customized or modern web interfaces may require additional HTML, CSS, or JavaScript knowledge.\nDependency on R: Shiny apps require an R environment to run, which may not be suitable for all deployment scenarios.\n\nUnderstanding these limitations helps in deciding when Shiny is the right tool and when alternative solutions (such as static reports, other dashboard frameworks, or web technologies) may be more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Shiny</span>"
    ]
  },
  {
    "objectID": "week08.html",
    "href": "week08.html",
    "title": "8  Research Software Engineering and Architecture in R",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Software Engineering and Architecture in R</span>"
    ]
  },
  {
    "objectID": "week08.html#objectives",
    "href": "week08.html#objectives",
    "title": "8  Research Software Engineering and Architecture in R",
    "section": "",
    "text": "Understand what distinguishes research software from research scripts.\nRecognise the importance of software engineering principles in research.\nUnderstand functional programming approaches in R.\nRecognise common architectural patterns for research projects.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Software Engineering and Architecture in R</span>"
    ]
  },
  {
    "objectID": "week08.html#introduction-to-research-software-engineering",
    "href": "week08.html#introduction-to-research-software-engineering",
    "title": "8  Research Software Engineering and Architecture in R",
    "section": "8.1 Introduction to Research Software Engineering",
    "text": "8.1 Introduction to Research Software Engineering\nResearch software engineering (RSE) is the discipline of applying software engineering principles and practices to the development of software that supports research. Unlike simple research scripts, which are often written quickly to answer a specific question or analyze a dataset, research software is intended to be robust, reusable, and maintainable over time. This distinction is important because research increasingly relies on complex computational methods, and poorly engineered code can lead to irreproducible results, wasted effort, and difficulties in collaboration.\nKey aspects of research software engineering include version control, testing, documentation, and modular design. By adopting these practices, researchers can ensure that their code is understandable, reliable, and easier to share or extend. RSE also emphasizes the importance of collaboration between domain experts and software engineers, recognizing that high-quality research software is essential for reproducible and impactful science.\nWe have already seen several RSE principles in Chapter 3, such as using Quarto to document analyses and adopting version control. In this lecture we will focus on designing testable and reusable code using functional principles, by interacting with the purrr library in tidyverse. We will also look at some further, more general principles.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Software Engineering and Architecture in R</span>"
    ]
  },
  {
    "objectID": "week08.html#functional-programming-in-r",
    "href": "week08.html#functional-programming-in-r",
    "title": "8  Research Software Engineering and Architecture in R",
    "section": "8.2 Functional Programming in R",
    "text": "8.2 Functional Programming in R\nThere exist a variety of programming paradigms that can be used to (broadly) separate languages into different categories, for example:\n\nProcedural Programming: Organizes code into procedures or routines (functions), focusing on a sequence of computational steps to be carried out.\nObject-Oriented Programming (OOP): Structures code around objects that encapsulate data and behavior, promoting modularity and reuse.\nFunctional Programming: Emphasizes the use of pure functions and immutable data, avoiding side effects and enabling easier reasoning about code.\n\nIt is rare for a language to fall exclusively into one of these paradigms; R can support all three of the above. However, knowing about the principles of different programming paradigms can influence the code you write and result in more reproducible, reusable and maintainable code.\nR is an excellent language in which to adopt a functional programming paradigm. In Week 2 we saw some of the core features of R that enable this, such as:\n\nFirst-class functions: Functions in R can be assigned to variables, passed as arguments, and returned from other functions.\nAnonymous (lambda) functions: R supports the creation of functions without names, useful for short, throwaway operations.\nHigher-order functions: It is easy to create functions that return functions. Functions like lapply(), sapply(), purrr::map(), and others take functions as arguments and apply them over data structures.\n\nThe tidyverse library purrr gives access to even more functional features.\n\n8.2.1 Pure Functions\nThis is the principle that functions should have no side effects; the same input should always produce the same output. This makes code much easier to test, debug and reason about.\n# Pure = good\nadd = function(x, y) x + y\n\n# Impure = bad\ncounter = 0\nincrement = function() {\n    counter &lt;&lt;- counter + 1 \n    counter\n}\nRecall from Section 2.4.3 that &lt;&lt;- is the super-assignment operator which can modify the global state. This should be avoided wherever possible.\n\n\n8.2.2 Higher-Order Functions\nFrom Section 2.4.1, recall that a higher order function is a function that either accepts or returns another function. These are the foundation of functional data manipulation. To give a concrete example, a common use case is to map a list through a function, i.e., to take each element of the list and apply a function to it. In R these are materialised through the apply family of functions. Consider the following code:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nx = 1:10\n\n# Apply = good\nsquare = function(x) x^2\nsquared_map = sapply(x, square)\n# or even using an anonymous function\nsquared_map = sapply(x, function(x) x^2)\n# using purrr\nsquared_map = map_dbl(x, function(x) x^2)\n\n# For loop = bad\nsquared_loop = numeric(length(x))\nfor (i in seq_along(x)) {\n    squared_loop[i] = x[i]^2\n}\n\nUsing the for loop it is easier to make mistakes because, inside the loop, we can modify the global state. Adopting functional principles simplifies and clarifies the code and makes it more testable (we can test the square function works as intended separately).\nThe map_dbl function from purrr is one of a family of map_ functions that specify the return type of the vector, e.g.:\n\nmap_int: returns integers.\nmap_chr: returns strings.\n\npurr offers more advanced mapping, for example to allow iterating two vectors in parallel:\n\nx = 1:10\ny = rep(c(2,3), 5)\nmap2_dbl(x, y, function(x,y) x^y)\n\n [1]    1    8    9   64   25  216   49  512   81 1000\n\n\nIt also offers functions for reducing a vector by applying an operation recursively to each pair of elements, e.g.\n\nx = 1:10\nsum_x = sum(x)\nsum_x_purrr = reduce(x, function(x,y) x + y) \nsum_x_purrr = reduce(x, `+`) # equivalently\nsum_x_purr = x |&gt; reduce(`+`) # with pipes\nc(sum_x, sum_x_purrr)\n\n[1] 55 55\n\n\nLastly it allows us to filter elements from vectors using a predicate; this is similar to the filter functions from dplyr (see Section 4.4.2), but works on vectors, not just tibbles. We can also perform other operations, like checking whether all elements match a predicate or none do.\n\nx = 1:10\nx |&gt; keep(function(x) x &gt; 5) # the second argument is a function that returns a boolean\n\n[1]  6  7  8  9 10\n\n# this is called a predicate.\n\nx |&gt; every(function(x) x &gt; 5) # do all elements match?\n\n[1] FALSE\n\nx |&gt; none(function(x) x &gt; 10)\n\n[1] TRUE\n\n\n\n\n8.2.3 Function Composition\nWe have already seen how this is implemented in R using the pipe operator; see Section 2.4.2\n\n\n8.2.4 Currying\nCurrying is the process of taking a function and fixing some arguments of it, for example to allow it to be used in map functions. This is implemented in purrr using partial:\n\nadd_10 = partial(`+`, 10)\nx = 1:10\nx |&gt; map_dbl(add_10)\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\nnames = c(\"Ada Lovelace\", \"Alan Turing\", \"Grace Hopper\")\n\nadd_hello = partial(paste, \"Hello\")\nnames |&gt; map_vec(add_hello)\n\n[1] \"Hello Ada Lovelace\" \"Hello Alan Turing\"  \"Hello Grace Hopper\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Software Engineering and Architecture in R</span>"
    ]
  },
  {
    "objectID": "week08.html#key-practices-for-robust-research-software",
    "href": "week08.html#key-practices-for-robust-research-software",
    "title": "8  Research Software Engineering and Architecture in R",
    "section": "8.3 Key Practices for Robust Research Software",
    "text": "8.3 Key Practices for Robust Research Software\nThe principles above help to write good, reusable code, but there are some further principles that make the whole software lifecycle more sustainable.\n\n8.3.1 Defensive Programming\nDefensive programming is the principle that functions should check that the inputs are expected before performing any operations. We saw an example of this in Lab 2, where the - operator implicitly wraps vectors if they are not of the same length:\n\nx = 1:4\ny = 1:2\n\nx - y # we might expect that this throws an error - but it does not!\n\n[1] 0 0 2 2\n\n\nThis had an impact in a distance function we wrote:\n\ndistance = function(x, y) sqrt(sum((x-y)^2))\ndistance(x, y)\n\n[1] 2.828427\n\n\nTo implement this defensively, we can use the stop function, which causes R to throw an error with an error message.\n\ndistance_defensive = function(x, y) {\n    if (!is.numeric(x) || !is.numeric(y)) {\n        stop(\"Both x and y must be numeric vectors.\")\n    }\n    if (length(x) != length(y)) {\n        stop(\"x and y must have the same length.\")\n    }\n    sqrt(sum((x - y)^2))\n}\n\ndistance_defensive(x, y)\n\nError in distance_defensive(x, y): x and y must have the same length.\n\n\nNow the function complains if we pass in vectors that are invalid.\n\n\n8.3.2 Error Handling with try-catch\nIn addition to defensive programming, robust research software should gracefully handle unexpected errors. In R, this can be achieved using try and tryCatch.\n\ntry(expr) evaluates an expression and continues execution even if an error occurs, returning the error as an object.\ntryCatch(expr, error = function(e) ...) allows you to specify custom behavior when errors (or warnings) occur.\n\nFor example:\n\nresult = try(distance_defensive(x,y))\n\nError in distance_defensive(x, y) : x and y must have the same length.\n\ncat(paste0(\"The error was 'caught', so I can print it here:\\n\", result))\n\nThe error was 'caught', so I can print it here:\nError in distance_defensive(x, y) : x and y must have the same length.\n\n\n\n\n8.3.3 Documentation Strategies\nWe have seen how to provide comments in R code. Quarto documents are also themselves a kind of documentation, allowing us to write explanatory text around our analysis. However, if implementing pure R functions it is important to provide further documentation to explain what the function does, what arguments it takes, and what it returns. If implemented properly, these will appear on hovering the function in RStudio, or when using ?function.\nOne of the most convenient ways to accomplish this is using the roxygen2 library. This allows us to provide documentation in comments, in our R code. If we were then to compile our code into an R package (beyond the scope of this course), we could use roxygen2 to autogenerate documentation for users.\nroxygen2 requires comments in a specific format. Here’s an example:\n#' The Euclidean distance between two vectors.\n#'\n#' Returns the Euclidean distance between the vectors x and y, implemented as \\sqrt(\\sum (x_i - y_i)^2)\n#'\n#' @param x The first vector\n#' @param y The second vector\n#' @return The distance between x and y\n#' @examples\n#' x = c(1,2)\n#' y = c(3,4)\n#' distance(x,y)\n#' @export\n#' @author Jon Cockayne\ndistance = function(x, y) {\n    if (!is.numeric(x) || !is.numeric(y)) {\n        stop(\"Both x and y must be numeric vectors.\")\n    }\n    if (length(x) != length(y)) {\n        stop(\"x and y must have the same length.\")\n    }\n    sqrt(sum((x - y)^2))\n}\n\n\n8.3.4 Other Considerations\nThere are two final points that are beyond the scope of this course, but nevertheless important topics. These are included mainly to give you references to how to implement them in R.\n\nUnit Testing is the process whereby we write code to test that functions we have implemented return the expected result. In large codebases this can help to catch errors that get introduced when making modifications. In R unit tests can be implemented using the testthat library.\nDependency Management was mentioned in Chapter 2; it is bad practice to use install.packages to install packages in your Quarto scripts, but we still need to communicate what packages are required by our software to users. In R this can be implemented using the renv package, which allows us to document dependencies and freeze particular versions required.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Software Engineering and Architecture in R</span>"
    ]
  },
  {
    "objectID": "week09.html",
    "href": "week09.html",
    "title": "9  SQL for Data Science",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week09.html#objectives",
    "href": "week09.html#objectives",
    "title": "9  SQL for Data Science",
    "section": "",
    "text": "Understand SQL as a declarative language for transforming data.\nKnow how to use SQL joins, GROUP BY aggregations and subqueries.\nAppreciate the tradeoffs of doing computation in-database rather than in R.\nWrite queries that are reusable and support reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week09.html#what-is-a-relational-database",
    "href": "week09.html#what-is-a-relational-database",
    "title": "9  SQL for Data Science",
    "section": "9.1 What is a Relational Database?",
    "text": "9.1 What is a Relational Database?\nA relational database is a structured collection of data organized into tables (also called relations). Each table consists of rows (records) and columns (fields), where each column has a specific data type and each row represents a single entry.\nTo draw analogies with data frames, each table is essentially a data frame. However, the data frames are linked through a key mechanism. Each table has a column that is referred to as a primary key which is like a unique identifier for the row. Often this is an integer, but it can be other things (e.g. a string, or a UUID). The important thing is that each value of a primary key can exist only once in the table, to ensure the row can be identified uniquely.\nA table can also have columns foreign keys, which are values that refer to primary keys in other tables, thus allowing for relations between the tables. If a column is labelled as a foreign key, consistency checks will be performed to ensure that the foreign key exists in the table it refers to.\n\n9.1.1 Why use Relational Databases?\nRelational databases offer several advantages:\n\nData Integrity: Enforce rules (such as primary and foreign keys) to maintain accurate and consistent data.\nScalability: Efficiently handle large volumes of data and complex queries.\nFlexibility: Support powerful querying and data manipulation using SQL.\nSecurity: Provide robust access controls and user permissions.\nRedundancy Reduction: Minimize data duplication through normalization (i.e. breaking down the data into smaller, related tables rather than a single large table).\nConcurrent Access: Allow multiple users to access and modify data safely at the same time.\nBackup and Recovery: Offer built-in mechanisms for data backup and restoration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week09.html#what-is-sql",
    "href": "week09.html#what-is-sql",
    "title": "9  SQL for Data Science",
    "section": "9.2 What is SQL?",
    "text": "9.2 What is SQL?\nSQL (Structured Query Language) is a standardized language used to manage and manipulate relational databases. It allows users to define, query, update, and control access to data stored in tables. SQL is declarative, meaning you specify what data you want, not how to retrieve it. Common SQL operations include selecting data (SELECT), filtering (WHERE), joining tables (JOIN), aggregating results (GROUP BY), and modifying data (INSERT, UPDATE, DELETE). SQL is widely supported by database systems such as MySQL, PostgreSQL, SQLite, and Microsoft SQL Server, though there are some features that differ between implementations. In this course we will focus on sqlite, since it is widely available and does not require access to a server on which to host the database.\nMuch like R, SQL is typically written in plain text into text files, with the extension .sql. SQL is also stricter on formatting; all SQL statements must be terminated with a semicolon (;).\n\n9.2.1 Creating a SQLite DB\nIn this course you will mostly be supplied with pre-build SQLite databases to work with. There are various ways to create a database from scratch. One user-friendly way is using the DB Browser for SQLite. This gives the facility to create new databases and execute arbitrary SQL scripts on them, as well as interactive ways to create tables and insert data.\n\n\n9.2.2 Creating Tables\nThe command for creating a table in a database is\n    CREATE TABLE &lt;table_name&gt; (\n        column_1 &lt;column_1_type&gt;,\n        column_2 &lt;column_2_type&gt;, \n        ...,\n        &lt;constraints&gt;\n    )\nIt is idiomatic for keywords (e.g. CREATE TABLE) to be in all caps. Unlike R the column types must be specified at table creation and the database will be very strict in checking inserted values match the specified type.\nTypes that can be used vary according to database implementation. In SQLite the following types are available:\n\nINTEGER\nREAL: a decimal number\nTEXT: a text string\nBLOB: binary data (we won’t use these.)\n\nIn addition to the type, one column in a table can be designated as PRIMARY KEY, indicating that it is should contain the unique identifier for each row. We can further add the text AUTOINCREMENT after PRIMARY KEY to have SQLite manage inserting new IDs automatically; this avoids having to check what the largest ID in a table is before inserting.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week09.html#other-sqlite-types",
    "href": "week09.html#other-sqlite-types",
    "title": "9  SQL for Data Science",
    "section": "9.3 Other SQLite Types",
    "text": "9.3 Other SQLite Types\nThe list above is all types available in SQLite, which is quite limited. Particular omissions (and workarounds) are:\n\nBoolean types. These should be stored as an integer, with 0 for false and 1 for true. SQLite recognises the keywords TRUE and FALSE, but these are just macros that bind to the integers given above.\nDates. We can encode these into strings, real numbers or integers. We won’t use these in the course.\n\nA richer set of datatypes are available in other database implementations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week09.html#why-use-sql-vs.-r",
    "href": "week09.html#why-use-sql-vs.-r",
    "title": "9  SQL for Data Science",
    "section": "9.4 Why Use SQL vs. R",
    "text": "9.4 Why Use SQL vs. R\nYou will by now have noticed that the SQL operations described above replicate a lot of operations we learned how to do in R in Weeks 4-5. We have already discussed the advantages of using relational databases as a storage mechanism, but the question remains of the order of operations. In other words, what are the advantages of:\n\nRunning a query in SQL and importing the result to R, rather than\nImporting the data into R and then running the query.\n\nThere are several important considerations when deciding whether to perform data manipulation in SQL before importing to R, or to import raw data into R and then process it there:\n\nEfficiency: SQL databases are optimized for handling large datasets and complex queries efficiently. Filtering, joining, and aggregating data in SQL can be much faster and use less memory than performing the same operations in R, especially when working with datasets that are too large to fit comfortably in memory.\nMinimizing Data Transfer: By using SQL to select only the data you need, you reduce the amount of data transferred from the database to R. This is particularly important when working with remote databases or very large tables, as it avoids unnecessary data movement and speeds up analysis.\nReproducibility: SQL queries can be saved, version-controlled, and reused, making your data extraction process transparent and reproducible. This is especially valuable in collaborative projects or when analyses need to be repeated or audited.\nSecurity and Access Control: Databases often have built-in mechanisms for managing user permissions and restricting access to sensitive data. By performing data manipulation in SQL, you can leverage these controls and ensure that only authorized data is accessed or exported.\nData Integrity: SQL databases enforce constraints (such as primary and foreign keys) that help maintain data consistency. Manipulating data within the database ensures these constraints are respected, reducing the risk of introducing errors.\n\nHowever, R provides a more flexible and expressive environment for statistical analysis, visualization, and modelling. In practice, a common workflow is to use SQL to extract and preprocess the relevant subset of data, and then use R for further analysis and visualization.\n\n\n9.4.1 The Bridge: dbplyr\nThe R library dbplyr allows you to connect to a database and interact with it as though it were dplyr, while still getting SQL level performance. Supposing we have the database generated above stored in data/customers.db, here’s an example of how we can interact with it using dbplyr.\n\n# additional packages required\n# install.packages(\"DBI\")\n# install.packages(\"dbplyr\")\n# install.packages(\"RSQLite\")\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Connect to the SQLite database\ncon &lt;- dbConnect(RSQLite::SQLite(), \"data/customers.db\")\n\n# Reference tables as lazy tibbles\ncustomers &lt;- tbl(con, \"customers\")\nproducts &lt;- tbl(con, \"products\")\norders &lt;- tbl(con, \"orders\")\n\n# note that it is reported as ?? x 4 - tidyverse has not loaded the entire table\ncustomers\n\n# Source:   table&lt;`customers`&gt; [?? x 4]\n# Database: sqlite 3.50.4 [/Users/jon/Teaching/math6195/notes/data/customers.db]\n      id first_name last_name email                    \n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;                    \n 1     1 David      Brown     david.brown@example.com  \n 2     2 Eve        Davis     eve.davis@example.com    \n 3     3 Frank      Miller    frank.miller@example.com \n 4     4 Grace      Wilson    grace.wilson@example.com \n 5     5 Henry      Moore     henry.moore@example.com  \n 6     6 Ivy        Taylor    ivy.taylor@example.com   \n 7     7 Jack       Anderson  jack.anderson@example.com\n 8     8 Karen      Thomas    karen.thomas@example.com \n 9     9 Leo        Jackson   leo.jackson@example.com  \n10    10 Mona       White     mona.white@example.com   \n# ℹ more rows\n\n# Example: Find total orders per customer\norders |&gt;\n    group_by(customer_id) |&gt;\n    summarise(order_count = n()) |&gt; # n() - shortcut for counting rows\n    collect()\n\n# A tibble: 23 × 2\n   customer_id order_count\n         &lt;int&gt;       &lt;int&gt;\n 1           1           6\n 2           2           5\n 3           3           5\n 4           4           5\n 5           5           5\n 6           6           5\n 7           7           5\n 8           8           5\n 9           9           5\n10          10           5\n# ℹ 13 more rows\n\n# Example: Join tables to get customer names and their orders\norders |&gt;\n    inner_join(customers, by = c(\"customer_id\" = \"id\")) |&gt;\n    inner_join(products, by = c(\"product_id\" = \"id\")) |&gt;\n    select(first_name, last_name, product_name, quantity) |&gt;\n    collect()\n\n# A tibble: 107 × 4\n   first_name last_name product_name quantity\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;           &lt;int&gt;\n 1 David      Brown     Widget              2\n 2 Eve        Davis     Gadget              1\n 3 Frank      Miller    Thingamajig         5\n 4 David      Brown     Thingamajig         1\n 5 Grace      Wilson    Contraption         2\n 6 Henry      Moore     Apparatus           3\n 7 Ivy        Taylor    Tool                1\n 8 Jack       Anderson  Gadget              4\n 9 Karen      Thomas    Gizmo               2\n10 Leo        Jackson   Implement           1\n# ℹ 97 more rows\n\n# Disconnect when done\ndbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>SQL for Data Science</span>"
    ]
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "10  Working with Big Data",
    "section": "",
    "text": "Lecture",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "week10.html#lecture",
    "href": "week10.html#lecture",
    "title": "10  Working with Big Data",
    "section": "",
    "text": "Understand what constitutes “big data” in practice and why in-memory tools like R or tidyverse might fail.\nUnderstand tradeoffs between memory, compute, storage and speed.\nKnow about other tools for working with big data, outside of R.\nBe familiar with tools that scale R workflows to big data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "week10.html#what-is-big-data",
    "href": "week10.html#what-is-big-data",
    "title": "10  Working with Big Data",
    "section": "10.1 What is Big Data",
    "text": "10.1 What is Big Data\n“Big data” refers to datasets that are too large or complex to be processed and analyzed using traditional data processing tools and techniques. The defining characteristics of big data are often summarized by the “three Vs”:\n\nVolume: The sheer amount of data generated and stored, often measured in terabytes (1,000,000 MB) or petabytes (1,000,000,000 MB).\nVelocity: The speed at which new data is generated and needs to be processed, such as real-time streaming data (e.g. the “firehose” from the website formerly known as Twitter).\nVariety: The diversity of data types and sources, including structured, semi-structured, and unstructured data (e.g., text, images, sensor data).\n\nIn practice, “big data” is relative; it depends on the limitations of your hardware and software. Data that is “big” for one organization or tool may be manageable for another. As data grows in size and complexity, traditional in-memory tools like base R or the tidyverse may struggle due to memory constraints, leading to the need for specialized tools and techniques that can handle distributed storage and computation.\n\n10.1.1 Why Big Data is a Problem\nWith all the tools we have seen so far, the entire dataset is loaded into your computer’s memory. When we create a tibble using read_csv, the text in the CSV file is read and interpreted as a collection of floating point numbers, strings etc., that are stored internally as vectors. That means that in order to load a tibble, your computer needs to have enough memory to store all the data at the same time.\nMost modern computers have between 8GB and 32GB of memory. This means that “big” data by volume might be too large to fit into memory. Nevertheless, we might still be able to process the data without being able to store it all in memory. For example, to compute the mean of a vector we don’t need to have the whole vector in memory; we just need to read each element once and then discard it (as well as to know the total number of elements). There exist various tools in R to enable this kind of behaviour, which we will look at in this week’s notes.\n\n\n10.1.2 Tradeoffs: Memory, Compute, Storage, and Speed\nWhen working with big data, it is important to understand the tradeoffs between memory, compute, storage, and speed:\n\nMemory (RAM): Fast but limited in size. Operations performed in memory are typically much faster than those involving disk storage, but you are constrained by the amount of RAM available. If your data exceeds available memory, you must use alternative strategies such as chunking, streaming, or out-of-core processing.\nCompute (CPU/GPU): The processing power available determines how quickly computations can be performed. More powerful CPUs or GPUs can process data faster, but may also require more memory bandwidth and generate more heat. Parallel and distributed computing can help scale up processing, but often introduces additional complexity.\nStorage (Disk/SSD/HDD): Storage devices can hold much larger datasets than memory, but accessing data from disk is much slower than from RAM. SSDs are faster than traditional hard drives, but still much slower than memory. Efficient data formats and indexing can help mitigate some of the speed limitations.\nSpeed (Throughput/Latency): The overall speed of data processing depends on how quickly data can be moved between storage, memory, and compute resources. Bottlenecks often occur when data must be repeatedly read from or written to disk, or when computations are not parallelized effectively.\n\nIn practice, optimizing for big data workflows often involves balancing these resources: minimizing memory usage, maximizing compute efficiency, and reducing slow disk I/O. The right approach depends on the size and structure of your data, the hardware available, and the specific analysis tasks you need to perform.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "week10.html#working-with-big-data",
    "href": "week10.html#working-with-big-data",
    "title": "10  Working with Big Data",
    "section": "10.2 Working with Big Data",
    "text": "10.2 Working with Big Data\nThere are multiple options for working with big data. We will suppose, for illustration, that the data is supplied as a large CSV file. Other formats are available and potentially more suitable, including:\n\nRelational Databases: These can run on specialised machine with much larger RAM, and support fast, smart on-disk storage when the entire dataset is too large for RAM.\nColumnar Storage Formats: Formats like Parquet and Feather are designed for efficient storage and retrieval of large, tabular datasets. They support fast reading of only the columns you need, whereas when working with CSV files the whole file must be read to extract a column.\nHDF5: A hierarchical data format suitable for storing large arrays and complex data structures. It is commonly used in scientific computing and supports efficient random access to subsets of data.\n\nIn addition, there are other software architectures specifically designed for big data workflows that lie outside the R ecosystem, such as:\n\nHadoop Ecosystem: An open-source framework for distributed storage (HDFS) and processing (MapReduce, Hive, Spark) of large datasets across clusters of computers. See link.\nApache Spark: A fast, general-purpose cluster computing system for big data analytics, supporting in-memory processing and APIs for R, Python, Scala, and Java. See link.\n\nWithin R there are still several solutions that we will briefly cover.\n\ndplyr with database backends (as we saw in Week 9).\nsparklyr: Provides an interface to Apache Spark, allowing distributed data processing and machine learning from within R. This requires access to a Spark cluster, which is beyond the scope of this course.\nduckdb: An in-process SQL OLAP database that allows fast analytical queries on large datasets, including direct querying of CSV and Parquet files without loading them fully into memory.\nreadr: Provides the function read_csv_chunked which can be used for simple map-reduce workflows.\n\nMany of these tools are specifically implemented in R as data access layers that can be integrated with dplyr, allowing big data to be worked with as we saw in Week 4 and 5.\n\n10.2.1 Example 1: DuckDB\nduckdb is an in-process SQL OLAP (Online Analytical Processing) database management system. OLAP systems are optimized for fast, complex analytical queries on large datasets, such as aggregations and summaries. DuckDB can be used directly from R without the need for a separate database server. In particular, it allows you to run queries on datasets stored on disk without needing to load the entire dataset into memory.\n\n\n\n\n\n\nTip\n\n\n\nThe name “duckdb” is inspired by the “duck test”: “If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.” The creators chose this name because DuckDB is designed to look and behave like a traditional database system (such as SQLite or PostgreSQL), but is optimized for analytical workloads and can be embedded directly into applications. The playful name reflects its lightweight, easy-to-use nature and its ability to “just work” for data analysis tasks.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis demo code is run on the flight prices dataset from Kaggle. You can download this data to try and run it yourself, but be warned - the file size is 29GB!\n\n\n\n# install.packages(\"duckplyr\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(duckplyr)\n\nThe duckplyr package is configured to fall back to dplyr when it encounters an\nincompatibility. Fallback events can be collected and uploaded for analysis to\nguide future development. By default, data will be collected but no data will\nbe uploaded.\nℹ Automatic fallback uploading is not controlled and therefore disabled, see\n  `?duckplyr::fallback()`.\n✔ Number of reports ready for upload: 23.\n→ Review with `duckplyr::fallback_review()`, upload with\n  `duckplyr::fallback_upload()`.\nℹ Configure automatic uploading with `duckplyr::fallback_config()`.\n✔ Overwriting dplyr methods with duckplyr methods.\nℹ Turn off with `duckplyr::methods_restore()`.\n\n# Reference the CSV file as a DuckDB table (no need to load into R memory)\nflights = read_csv_duckdb(\"data/itineraries.csv\")\n\nFirst we run some basic summary statistics.\n\nflights |&gt; names()\n\n [1] \"legId\"                             \"searchDate\"                       \n [3] \"flightDate\"                        \"startingAirport\"                  \n [5] \"destinationAirport\"                \"fareBasisCode\"                    \n [7] \"travelDuration\"                    \"elapsedDays\"                      \n [9] \"isBasicEconomy\"                    \"isRefundable\"                     \n[11] \"isNonStop\"                         \"baseFare\"                         \n[13] \"totalFare\"                         \"seatsRemaining\"                   \n[15] \"totalTravelDistance\"               \"segmentsDepartureTimeEpochSeconds\"\n[17] \"segmentsDepartureTimeRaw\"          \"segmentsArrivalTimeEpochSeconds\"  \n[19] \"segmentsArrivalTimeRaw\"            \"segmentsArrivalAirportCode\"       \n[21] \"segmentsDepartureAirportCode\"      \"segmentsAirlineName\"              \n[23] \"segmentsAirlineCode\"               \"segmentsEquipmentDescription\"     \n[25] \"segmentsDurationInSeconds\"         \"segmentsDistance\"                 \n[27] \"segmentsCabinCode\"                \n\nflights |&gt; summarize(nrow=n())\n\n# A duckplyr data frame: 1 variable\n      nrow\n     &lt;int&gt;\n1 82138753\n\n\nNext: what is the average base fare? This only requires a single pass through the dataset to compute, so duckplyr can handle it.\n\n# Compute the average base fare\nflights |&gt;\n    summarise(mean_base_fare = mean(baseFare, na.rm = TRUE))\n\n# A duckplyr data frame: 1 variable\n  mean_base_fare\n           &lt;dbl&gt;\n1           293.\n\n\nHowever, more complex queries throw errors; this is because duckplyr cannot work out how to run the query without materialising the dataset in memory.\n\n# Compute average base fare by origin city\nflights |&gt;\n    group_by(startingAirport) |&gt;\n    summarise(mean_base_fare = mean(baseFare, na.rm = TRUE)) |&gt;\n    arrange(desc(mean_base_fare))\n\nError in `group_by()`:\n! This operation cannot be carried out by DuckDB, and the input is a\n  stingy duckplyr frame.\n• Try `summarise(.by = ...)` or `mutate(.by = ...)` instead of `group_by()` and\n  `ungroup()`.\nℹ Use `compute(prudence = \"lavish\")` to materialize to temporary storage and\n  continue with duckplyr.\nℹ See `vignette(\"prudence\")` for other options.\n\n\nNote that we could write logic to run this query without materialising the dataset! This emphasises that duckplyr is an abstraction that is sometimes, but not always, convenient.\n\n\n10.2.2 Example 2: MapReduce over CSV Chunks in R\nYou can implement a simple MapReduce workflow in R by processing a large CSV file in chunks, applying a “map” function to each chunk, and then combining (“reducing”) the results. The readr::read_csv_chunked() function is useful for this purpose.\nHere’s an example that computes the mean of a column (baseFare) by reading the file in chunks:\n\n\n\n\n\n\nThis Takes a Very Long Time\n\n\n\nThis is not an ideal command to run in a Quarto notebook. It takes a very long time to run! If you run it in an R terminal you will see a nice progress bar.\n\n\nlibrary(readr)\nlibrary(dplyr)\n\n# Define a callback function to process each chunk\norigin_fare_chunk &lt;- function(chunk, pos) {\n    #print(pos)\n    chunk |&gt;\n        group_by(startingAirport) |&gt;\n        summarise(\n            sum_base_fare = sum(baseFare, na.rm = TRUE),\n            n = sum(!is.na(baseFare)),\n            .groups = \"drop\"\n        )\n}\n\n# Read the CSV in chunks and apply the callback\nresults &lt;- read_csv_chunked(\n    \"data/itineraries.csv\",\n    callback = DataFrameCallback$new(origin_fare_chunk),\n    chunk_size = 50000\n)\n\n# Combine the results from all chunks\nfinal_result &lt;- bind_rows(results) |&gt;\n    group_by(startingAirport) |&gt;\n    summarise(\n        total_base_fare = sum(sum_base_fare),\n        total_n = sum(n),\n        mean_base_fare = total_base_fare / total_n,\n        .groups = \"drop\"\n    ) |&gt;\n    arrange(desc(mean_base_fare))\n\nfinal_result\nThis produces the following output:\n# A tibble: 16 × 4\n   startingAirport total_base_fare total_n mean_base_fare\n   &lt;chr&gt;                     &lt;dbl&gt;   &lt;int&gt;          &lt;dbl&gt;\n 1 OAK                 1775978269. 3809884           466.\n 2 SFO                 2184560507. 5706482           383.\n 3 JFK                 1454873170. 4425164           329.\n 4 LAX                 2646536062. 8073281           328.\n 5 IAD                 1117200036. 3464378           322.\n 6 PHL                 1393464667. 4726187           295.\n 7 DEN                 1358640662. 4697143           289.\n 8 DTW                 1279715598. 4547052           281.\n 9 CLT                 1524909832. 5494510           278.\n10 EWR                 1017848977. 3970797           256.\n11 ATL                 1361400305. 5312028           256.\n12 LGA                 1513830212. 5919323           256.\n13 MIA                 1253173185. 4930213           254.\n14 DFW                 1406917690. 5674959           248.\n15 BOS                 1438004962. 5883876           244.\n16 ORD                 1311907639. 5503476           238.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Data Ethics and Biases",
    "section": "",
    "text": "Objectives\nBy the end of this lecture you should be able to: - Identify ethical and regulatory considerations in data science projects. - Recognise different types of biases and their impacts. - Evaluate real-world case studies through an ethical and regulatory lens.",
    "crumbs": [
      "Data Ethics and Biases"
    ]
  },
  {
    "objectID": "ethics.html#ethics-in-data-science",
    "href": "ethics.html#ethics-in-data-science",
    "title": "Data Ethics and Biases",
    "section": "Ethics in Data Science",
    "text": "Ethics in Data Science\nData science decisions can have profound effects on individuals and society. When data is used carelessly or unethically, it can result in discrimination, breaches of privacy, and a loss of public trust. Practising responsible data science helps prevent harm and supports fairness. Ethical considerations should inform every stage of a project, from data collection to analysis and sharing. Being transparent and accountable is crucial for building trust and ensuring that data science serves the public good.\n\n\n\n\n\n\nCase Study: Amazon’s AI Recruiting Tool\n\n\n\nAmazon developed an AI recruiting tool to automate the process of reviewing job applicants. However, the system showed bias against women, downgrading resumes that included terms like “women’s” or were from all-women colleges. This bias stemmed from training data that reflected historical male dominance in tech roles. Despite attempts to fix the issue, Amazon ultimately scrapped the tool, highlighting the risks of relying on biased data and the importance of careful oversight in AI systems.\nFor more information see this news article.\n\n\n\nEthics by Design",
    "crumbs": [
      "Data Ethics and Biases"
    ]
  },
  {
    "objectID": "ethics.html#understanding-bias-in-data-science",
    "href": "ethics.html#understanding-bias-in-data-science",
    "title": "Data Ethics and Biases",
    "section": "Understanding Bias in Data Science",
    "text": "Understanding Bias in Data Science\nThe main cause of the previous case study was a bias in the data set that was unaccounted for. Understanding the kinds of bias that can affect our analyses can help with mitigating the biases. Some examples:\n\nSampling Bias: Data collected is not representative of the population intended to be analyzed.\nSelection Bias: Certain groups are systematically excluded or included, skewing results.\nMeasurement Bias: Errors in how data is collected or measured, leading to inaccurate results.\nConfirmation Bias: Tendency to interpret or select data that confirms pre-existing beliefs.\nAlgorithmic Bias: Models produce systematically prejudiced results due to biased training data or design.\n\nIn the Amazon example above there was a selection bias: because the training set consisted largely of male candidates, the trained algorithm exhibited the same bias towards male candidates.\n\n\n\n\n\n\nAssessing Biases\n\n\n\nIn general, when we train machine learning algorithms, we are looking for algorithms that identify a bias in a training set (e.g. “candidates with higher scores in their data science course at University tend to do better in their jobs”). There is a difference between this kind of bias, where the algorithm identifies a genuine relationship relevant to job performance, and harmful biases, such as discrimination.\nThe key distinction is whether the bias reflects a legitimate, job-related factor or perpetuates unfair treatment based on protected characteristics. Responsible data science requires us to critically assess which biases are acceptable (e.g., those that improve prediction based on relevant skills) and which are unethical or illegal (e.g., those that disadvantage groups based on gender, race, or other protected attributes).\n\n\n\nRegulatory Considerations\n\n\n\n\n\n\nDisclaimer\n\n\n\nWe cannot discuss the regulatory environment of every country, so we will focus on UK regulations. Moreover this is too complex of a topic to cover thoroughly in a single lecture, and regulations can change. Therefore, when applying in your own work, check the latest regulations and seek expert advice if needed. Always ensure compliance with relevant laws and guidelines, as these may differ depending on your location and the nature of your data.\n\n\nFrom the UK Government’s summary of UK GDPR; data must be:\n\n\nused fairly, lawfully and transparently\nused for specified, explicit purposes\nused in a way that is adequate, relevant and limited to only what is necessary\naccurate and, where necessary, kept up to date\nkept for no longer than is necessary\nhandled in a way that ensures appropriate security, including protection against unlawful or unauthorised processing, access, loss, destruction or damage\n\n\nAdditional legal protection is available for analysis that involves protected characteristics, such as:\n\nRace and ethnicity\nPolitical opinions and religious beliefs\nSex life or orientation\n\n\n\nConsequences of Violating GDPR\nTo ensure that GDPR is adopted by companies, hefty penalties have been attached to non-compliance. Organizations can face fines of up to £17.5 million or 4% of annual global turnover, whichever is higher. Penalties are similar in the EU.\nIn addition to this there are non-financial repercussions, including:\n\nReputational Damage: Loss of public trust and damage to brand reputation.\nLegal Action: Individuals affected by data breaches may seek compensation.\nOperational Disruption: Regulatory investigations can disrupt business operations.",
    "crumbs": [
      "Data Ethics and Biases"
    ]
  },
  {
    "objectID": "ethics.html#managing-ethics-as-a-data-scientist",
    "href": "ethics.html#managing-ethics-as-a-data-scientist",
    "title": "Data Ethics and Biases",
    "section": "Managing Ethics as a Data Scientist",
    "text": "Managing Ethics as a Data Scientist\nIn the interests of ethics by design, various frameworks have been introduced to support a data scientist, both in terms of documenting potential ethical concerns in datasets and highlighting the relevant features of downstream processors (e.g. AI models). In this class we will focus on data cards and model cards.\n\nData Cards\nThe research paper by (Pushkarna, Zaldivar, and Kjartansson 2022) proposed data cards, a framework for documenting datasets that includes thorough documentation of potential ethical considerations. A more digestible summary can be found here. The documentation required by the framework is quite extensive and includes far more information than one would typically have access to even as a data owner; completing a full data card could be challenging, but still represents the current “gold standard” for data documentation. Moreover, not every field needs to be completed in a data card to provide a useful data summary. (For example, Kaggle has its users complete partial data cards, and scores datasets based on the quality of the documentation).\nParticularly relevant to ethics is the Sensitivity of Data section (see relevant section here).\n\nPractical Implementation of Data Cards\nFor this course we will use a minimal version of a data card containing the following fields.\n\n\nDataset name\nVersion / date\nShort description (1–2 sentences)\nSource / provenance (where data comes from; link)\nPurpose / intended use (what analyses are allowed; forbidden uses)\nComposition summary (rows, columns, key variables, sample size)\nProtected attributes present? (yes/no; list)\nMissingness summary (major variables with missingness)\nCollection method / measurement details (brief)\nKnown limitations / biases (brief bullets)\nPrivacy/sensitivity (contains personal data? special categories?)\nLicence / sharing restrictions\nContact / owner (name, email)\n\n\n\n\n\n\n\n\nExample\n\n\n\nThis is an example model card for the Titanic dataset from Kaggle(https://www.kaggle.com/c/titanic):\n\nDataset name: Titanic (Kaggle / public)\nVersion / date: 2020-11-01\nShort description: Passenger manifest for Titanic voyage with survival label.\nSource / provenance: Kaggle Titanic dataset — https://www.kaggle.com/c/titanic\nPurpose / intended use: Teaching classification; NOT for profiling or real-world commercial decisions.\nComposition summary: 891 rows, columns include PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked.\nProtected attributes present? Yes — sex (gender). Age may be sensitive for some contexts.\nMissingness summary: Age ~20% missing, Cabin ~77% missing. (For illustration, not accurate)\nCollection method / measurement details: Historical passenger manifest; some fields manually digitized; possible transcription errors.\nKnown limitations / biases:\n\nNot representative of modern populations.\nSurvival correlated with class and gender due to historical boarding policies — not causal for many modern uses.\n\nPrivacy/sensitivity: Public historical data, low privacy risk, but contains personal names.\nLicence / sharing restrictions: Unknown\nContact / owner: Kaggle\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn assessments you will not be required to complete entire data cards, but you may need to complete small parts of them.\n\n\n\n\n\nModel Cards\nModel cards are standardized documentation frameworks designed to provide transparent information about machine learning models. They summarize key details such as intended use, performance metrics, ethical considerations, limitations, and potential risks. Model cards help stakeholders understand how a model works, its strengths and weaknesses, and any fairness or safety concerns.\nWhile we will not use machine learning models in this course, it is increasingly important for a data scientist to be able to understand the models as a consumer. We will therefore focus on reading and understanding model cards from the fairness and safety perspective.\n\nExample Model Card\nBelow is a simplified example of a model card for a hypothetical loan approval classifier:\n\n\n\n\n\n\nExample Model Card\n\n\n\n\nModel name: Loan Approval Classifier v1.0\nVersion / date: 2024-06-01\nShort description: Predicts loan approval based on applicant financial and demographic data.\nIntended use: Assisting bank staff in evaluating loan applications; NOT for fully automated decision-making.\nPerformance metrics: Accuracy: 85%, Precision: 80%, Recall: 78% (on validation set)\nProtected attributes considered: Gender, ethnicity, age (used for fairness analysis, not as predictors)\nFairness assessment: Tested for disparate impact; no significant bias detected on gender, but slight bias observed for age group 60+.\nLimitations: May not generalize to applicants outside the training data region; sensitive to missing income data.\nEthical considerations: Should not be used as the sole basis for loan decisions; human review required.\nRisks: Potential for indirect bias if input features correlate with protected attributes; risk of over-reliance on model outputs.\nContact / owner: Data Science Team, Example Bank (contact@examplebank.com)\n\n\n\nAs a data scientist feeding data into a model, we need to assess whether the model card raises any causes for concern and ensure that our analysis pipeline and stakeholders are aware. In this case we might consider:\n\nAttaching warnings to our analysis highlighting the need for human review.\nTesting for systematic bias, e.g. by classifying records with protected characteristics and assessing empirically whether there is a bias present.\nEnsuring the model is not extrapolating, by checking how closely input test records correlate with the training set.\n\n\n\nWhy Model Cards Matter\nModel cards promote transparency and accountability by documenting how a model was built, tested, and intended to be used. They help users and stakeholders:\n\nUnderstand the model’s strengths and weaknesses.\nIdentify potential fairness or safety concerns.\nMake informed decisions about deploying or trusting the model.\nEnsure compliance with ethical and regulatory standards.\n\nFor more details, see Model Cards for Model Reporting and the Google Model Card Toolkit.",
    "crumbs": [
      "Data Ethics and Biases"
    ]
  },
  {
    "objectID": "ethics.html#takeaways",
    "href": "ethics.html#takeaways",
    "title": "Data Ethics and Biases",
    "section": "Takeaways",
    "text": "Takeaways\nDo’s - Do document your data and models using frameworks like data cards and model cards—transparency builds trust and helps identify risks early. - Do actively look for and mitigate biases at every stage, from data collection through to deployment and monitoring.\nDon’ts\n\nDon’t ignore GDPR and regulatory requirements. Penalties can reach £17.5 million or 4% of global turnover.\nDon’t assume your data is representative or your model is fair. Always critically assess for limitations and potential harms.\n\n\n\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. “Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI.” In 2022 ACM Conference on Fairness Accountability and Transparency, 1776–826. FAccT ’22. ACM. https://doi.org/10.1145/3531146.3533231.",
    "crumbs": [
      "Data Ethics and Biases"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Buckheit, Jonathan B., and David L. Donoho. 1995. “WaveLab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer New York. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nKnuth, Donald E. 1992. Literate Programming. Center for the\nStudy of Language and Information Publication Lecture Notes. Stanford,\nCA: Centre for the Study of Language & Information.\n\n\nPushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022.\n“Data Cards: Purposeful and Transparent Dataset Documentation for\nResponsible AI.” In 2022 ACM Conference on Fairness\nAccountability and Transparency, 1776–826. FAccT ’22. ACM. https://doi.org/10.1145/3531146.3533231.\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWild, C. J., and M. Pfannkuch. 1999. “Statistical Thinking in\nEmpirical Enquiry.” International Statistical Review 67\n(3): 223–48. https://doi.org/10.1111/j.1751-5823.1999.tb00442.x.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics.\nSpringer-Verlag. https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "Bibliography"
    ]
  }
]