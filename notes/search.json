[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH6195: Fundamentals of Data Science in R",
    "section": "",
    "text": "Course Structure and Logistics",
    "crumbs": [
      "Course Structure and Logistics"
    ]
  },
  {
    "objectID": "index.html#course-structure-and-logistics",
    "href": "index.html#course-structure-and-logistics",
    "title": "MATH6195: Fundamentals of Data Science in R",
    "section": "",
    "text": "This is the first year this course has been taught. There will be bugs! Please give me feedback if you think anything is too fast or too slow, and we can adjust.\n\n\nTeaching\nYou can’t learn data science by just listening to me talk to you. The teaching structure reflects this; each week we will have a 1 hour a lecture and a 2 hour lab. The point of the lab is for you to do data science; to this end, week we will build on what we saw in lectures by applying it to a data science problem.\n\n\nExtra Work\nThe main reference text for this book is the freely available R for Data Science, (Wickham, Çentnkaya-Rundel, and Grolemund 2023). However, we won’t always follow the book that closely. Each week, I will recommend some further work and/or reading that will help you to understand the material better. This will always be optional but I would encourage you to attempt it.\n\n\nAssessment\nThere will be two in-person supervised courseworks. Each will be worth 50% of your grade.\n\n\n\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Course Structure and Logistics"
    ]
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "1  Introduction to Data Science",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#objectives",
    "href": "week01.html#objectives",
    "title": "1  Introduction to Data Science",
    "section": "",
    "text": "Know what data science is and why it is important in the modern world.\nBe able to give an example of a data science workflow to follow when working as a data scientist.\nBe able to give examples of some tools we can use for Data Science, and discuss the importance of reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#what-is-data-science",
    "href": "week01.html#what-is-data-science",
    "title": "1  Introduction to Data Science",
    "section": "1.1 What is Data Science?",
    "text": "1.1 What is Data Science?\nData science is an interdisciplinary field that combines statistics, computer science, and domain expertise to extract meaningful insights and knowledge from data. It involves collecting, cleaning, analysing, and interpreting large and complex datasets to solve real-world problems, make predictions, and inform decision-making.\nAt its core, data science leverages techniques from mathematics and statistics to understand patterns and relationships in data, while using computational tools to automate and scale analyses. Data scientists often work with messy, unstructured data, applying methods such as data wrangling, visualization, machine learning, and statistical modelling.\n\n\n\nThe Data Science Venn Diagram, from here.\n\n\nThe (potentially controversial) image above puts data science in context: we need to have substantive expertise, maths and statistics knowledge and hacking skills to be a Data Scientist. In this course you will learn the last two; substantive expertise is also known as domain knowledge and is more difficult to acquire. The ultimate goal of data science is to turn raw data into actionable information, enabling organizations and individuals to make better, evidence-based decisions, and this is impossible without knowledge of the application domain.\nThat being said, Data Science is not a solo pursuit. In most of my Data Science projects I have been the mathematician / hacker in a team, containing domain experts. According to the Venn diagram above, it would be more proper to think of the team as the “Data Scientist” rather than me as an individual!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#the-data-science-workflow",
    "href": "week01.html#the-data-science-workflow",
    "title": "1  Introduction to Data Science",
    "section": "1.2 The Data Science Workflow",
    "text": "1.2 The Data Science Workflow\n\n\n\nThe Whole Game, taken from here.\n\n\nThe image above is taken from the course textbook (Wickham, Çentnkaya-Rundel, and Grolemund 2023), and includes the following steps:\n\nImport: Get the data from the source (e.g. Excel spreadsheets, databases) into your Data Science environment of choice.\nTidy: Organise your data to make it easier to work with.\nUnderstand: A loop consisting of:\n\nTransform: Transform the data into a standardised form that you can can be passed to algorithms.\nVisualise: Produce plots from your data to inform model choice (Week 7).\nModel: Fit statistical or machine learning models to make inferences or predictions (Week 2).\n\nCommunicate: Translate your results into language that in understandable for stakeholders, so you can discuss what actions to take.\n\nWe will go through each of these in the weeks listed above\nAs with much of data science there are other Data Science workflows you could follow, for example (Donoho 2017) or (Wild and Pfannkuch 1999) but most of them are in the same spirit as the above.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#the-role-of-computation",
    "href": "week01.html#the-role-of-computation",
    "title": "1  Introduction to Data Science",
    "section": "1.3 The Role of Computation",
    "text": "1.3 The Role of Computation\nA key advantage of computation in data science is the ability to automate repetitive or complex analyses. By writing code to process data, fit models, and generate visualizations, you can quickly rerun your entire workflow whenever new data arrives or methods change. This automation not only saves time but also reduces the risk of manual errors.\nReproducibility is another major benefit. When your analysis is scripted, others (including your future self) can reproduce your results exactly, verify your methods, and build upon your work. This is essential for scientific integrity and collaboration.\nFinally, computation enables you to scale your insights. With code, you can efficiently analyze large datasets, apply models to new data, and share your methods with others. This scalability is crucial as data volumes grow and projects become more complex.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#languages-and-tools",
    "href": "week01.html#languages-and-tools",
    "title": "1  Introduction to Data Science",
    "section": "1.4 Languages and Tools",
    "text": "1.4 Languages and Tools\nThis course is entitled “Fundamentals of Data Science in R”, but it should be emphasised that R is not the unique, best choice of programming language, and for many projects it may not even be a good language to use. There are many other languages that people use for Data Science. Perhaps the most widely used is python, but there are other, newer languages like Julia that give it a run for its money, particularly for scientific computing. At the end of the day, you can do Data Science in any language you happen to like: you can produce the same outputs in any of them. What’s more important is the software ecosystem - the packages that are available in that language. If a language has a mature software ecosystem it makes life considerably easier for a Data Scientist.\nWe will focus on R for this course so you can, by the end, become fluent in at least one major language for Data Science. Occasionally I will show you how to do things in other languages. I would encourage you to explore other options than R to find something that works best for you, rather than sticking with what I’ve shown you.\n\n1.4.1 Tools\nWhen working in Data Science, a variety of tools are used to support the workflow from data collection to communication of results. Some of the most important categories of tools include:\n\nIntegrated Development Environments (IDEs): Tools like RStudio (for R), JupyterLab (for R, Python, Julia, and many other languages), and Visual Studio Code (for pretty much any language) provide user-friendly interfaces for writing, running, and debugging code.\nVersion Control: Systems like Git (often used with GitHub or GitLab) help track changes in code and collaborate with others.\nData Visualization: Packages such as ggplot2 in R, matplotlib and seaborn in Python, and interactive tools like Tableau or Power BI are essential for exploring and presenting data.\nData Management: Tools for handling data include relational databases (e.g., SQLite, PostgreSQL), spreadsheet software (e.g., Excel, Google Sheets), and data wrangling libraries (dplyr in R, pandas in Python).\nReproducible Research: Tools like Quarto, R Markdown, and Jupyter Notebooks allow you to combine code, results, and narrative text in a single document, supporting transparency and reproducibility.\nCollaboration and Communication: Platforms such as Slack, Microsoft Teams, and project management tools like Trello or Asana facilitate teamwork and project tracking.\nContinuous Integration (CI): CI tools like GitHub Actions, GitLab CI, and Jenkins automatically run tests and checks on your code whenever changes are made. This helps ensure that your codebase remains functional, reproducible, and free of errors as you and your collaborators develop new features or fix bugs.\n\nChoosing the right tools depends on the project requirements, team preferences, and the specific problems being addressed.\n\n\n1.4.2 Generative AI\nGenerative AI tools, such as large language models (e.g., ChatGPT, GitHub Copilot), are increasingly being used in data science to assist with code generation, data exploration, documentation, and even generating synthetic data. They are valuable resources for learning and productivity in real-world data science projects and we will explore how they can be used to help with Data Science in this course.\n\n\n\n\n\n\nGenAI and Assessment\n\n\n\nFor this course’s assessments, the use of generative AI is not permitted. Assessments are designed to evaluate your individual understanding and skills. While you are encouraged to experiment with generative AI during your learning and practice, all submitted coursework must be your own unaided work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week01.html#reproducibility-and-scientific-integrity",
    "href": "week01.html#reproducibility-and-scientific-integrity",
    "title": "1  Introduction to Data Science",
    "section": "1.5 Reproducibility and Scientific Integrity",
    "text": "1.5 Reproducibility and Scientific Integrity\nReproducibility is an important concept in data science. When we say research is reproducible, we meant that another researcher can take the same data and methods and obtain the same results. Without reproducibility, data analysis is a “black box”; we will focus in this course on instead transforming data analysis into a transparent, verifiable process that the scientific community can trust.\n\n\n\n\n\n\nConsequences of Non-Reproducible Results\n\n\n\nIn the real world, data science can drive government, medical and business decisions. Policy decisions based on irreproducible analyses could be genuinely dangerous and harmful. This is therefore something to be taken seriously.\n\n\nWe will look at reproducibility in detail in Week 3. For now we will focus on two practical points that will make engaging with the material in Week 1 and 2 easier.\n\n1.5.1 R and Quarto\nAll of the labs will be conducted in RStudio, primarily using Quarto documents. Quarto is a successor to RMarkdown; it is a markdown-based file format that allows for R code to be embedded in the document. The document can then be knitted to produce various outputs.\nMost of the time in the labs you will load partially-written Quarto files that you pull from GitHub. However, it is worth knowing how to create a fresh Quarto document from RStudio. To create one, open the main menu and select New &gt; Quarto Document.\nRStudio has two visual modes for Quarto:\n\nVisual (default) shows a preview of the rendered document with all formatting applied.\nSource shows the plain Markdown text. This is what is actually saved in the file.\n\n\n\n1.5.2 Git and GitHub\nAll of the labs in this course will be hosted on GitHub using GitHub Classroom. We will go through how to use GitHub classroom and Git on the University computers in the first lab. However, for now, it is helpful to know the following minimal points:\n\nYou can download Git from here.\nYou can download (clone) a Git repository with the command git clone &lt;repository-url&gt;\nYou can add commits to the “staging area” with git add &lt;filename&gt; or just git add *.\nYou can commit changes with git commit -m \"Commit message\"\nYou can push changes up to github with git push.\n\nIf you have Git setup correctly on your machine, you will also see a Git pane in RStudio when you open it. We’ll go over this in more detail in Week 3, but the above will be enough for now.\n\n\n\n\n\n\nAssessment\n\n\n\nThe course assessments will all assess reproducibility of results. You will submit your coursework using GitHub Classroom, as in labs. Therefore, make sure you get comfortable with it!\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWild, C. J., and M. Pfannkuch. 1999. “Statistical Thinking in Empirical Enquiry.” International Statistical Review 67 (3): 223–48. https://doi.org/10.1111/j.1751-5823.1999.tb00442.x.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  Coding in R",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#objectives",
    "href": "week02.html#objectives",
    "title": "2  Coding in R",
    "section": "",
    "text": "Understand the basic syntax and data types in R, including variables, vectors, and control flow statements.\nLearn how to write and use functions in R, including higher-order functions and closures.\nDevelop foundational debugging skills for R code within Quarto documents.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#an-introduction-to-r",
    "href": "week02.html#an-introduction-to-r",
    "title": "2  Coding in R",
    "section": "2.1 An Introduction to R",
    "text": "2.1 An Introduction to R\nR is a powerful programming language and environment designed specifically for statistical computing, data analysis, and graphics. R is one of the most popular tools for data science, used by statisticians, researchers and analysts across academia and industry. Its main competitor is python, which is also a very powerful and widely used programming language. In contrast to R, python was not written primarily for science, which has advantages and disadvantages.\n\n2.1.1 Why Choose R for Data Science?\n\nBuilt for Statistics: Unlike general-purpose programming languages, R was created with statistical analysis in mind.\nPackage Ecosystem: R has over 19.000 packages available on its package hosting platform, CRAN (Comprehensive R Archive Network). Compared with other languages, its packages are significantly more mature in some areas.\nData Visualisation: R excels at creating exploratory plots for your own understanding and publication-quality graphics for sharing results. The ggplot2 package, in particular, has revolutionised data visualisation with its “grammar of graphics” framework.\nOpen Source and Free: R is completely free to use, modify and distribute, which is important for reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#basic-r-syntax",
    "href": "week02.html#basic-r-syntax",
    "title": "2  Coding in R",
    "section": "2.2 Basic R Syntax",
    "text": "2.2 Basic R Syntax\nWe will now see some basic R syntax.\n\n2.2.1 Assignments and Types\nAs with most programming languages, at its most basic level R allows us to create variables.\n\nx &lt;- 5\ny &lt;- \"Hello\"\nz = TRUE\n\n\n\n\n\n\n\nAssignment in R\n\n\n\nThe idiomatic way to assign values to variables in R is using the “left pointy arrow” &lt;-, but you can also use the more standard = sign that you may be familiar with from other languages. My background is in python, so I will usually default to = rather than &lt;-. I don’t mind which you use in assessments.\n\n\nWhen you assign a variable, it gains a type:\n\ntypeof(x)\n\n[1] \"double\"\n\ntypeof(y)\n\n[1] \"character\"\n\ntypeof(z)\n\n[1] \"logical\"\n\n\nWe see that xhas type double (a decimal number, stored in the computer as double precision), y has type character (a text string) and z has type logical (a logical, i.e. true or false, value). We will see other types later in the course.\nUnlike lower level languages like C# or java, R is a dynamically typed, interpreted language. An immediate consequence of this is that I can reassign a str variable to another type without any consequence:\n\nnumber = 5\ntypeof(number)\n\n[1] \"double\"\n\nnumber = \"hello\"\ntypeof(number)\n\n[1] \"character\"\n\n\n\n\n2.2.2 Printing Things\nIn R, printing is how you display the value of variables or expressions. There are several ways to do this:\n\n2.2.2.1 Automatic Printing (Interactive Mode)\nWhen you enter an expression that is not assigned to a variable, R automatically prints the result:\n\n1 + 1\n\n[1] 2\n\n\n\n\n2.2.2.2 Explicit Printing with print()\nUse print(...) when you want to be sure something shows up, especially inside functions or scripts:\n\nx &lt;- 42\nprint(x)\n\n[1] 42\n\n\n\n\n2.2.2.3 Formatted Printing with cat()\nUse cat(...) for cleaner, formatted output, especially when combining text and values:\n\nname &lt;- \"Flipper\"\nlength &lt;- 210\n\ncat(\"Penguin\", name, \"has a flipper length of\", length, \"mm.\\n\")\n\nPenguin Flipper has a flipper length of 210 mm.\n\n\n\n\n\n\n\n\nTip\n\n\n\ncat() does not add a newline by default — use if you want one.\nIt also avoids [1] and quotes, making it nicer for messages.\n\n\n\n\n2.2.2.4 String Formatting with sprintf()\nFor more precise formatting:\n\nstr = sprintf(\"Flipper length: %.2f mm\", 209.537)\nstr\n\n[1] \"Flipper length: 209.54 mm\"\n\n\nThis works like printf(...) in other languages and returns a string.\n\n\n\n2.2.3 Vectors\nWe can also assemble vectors. The syntax for this is c(...), i.e.\n\nvector = c(1,2,3,4,5)\ntypeof(vector)\n\n[1] \"double\"\n\n\nNote that the type is reported as a double, just like the type of x above. This is because R treats scalars like length-1 vectors:\n\nlength(vector)\n\n[1] 5\n\nlength(x)\n\n[1] 1\n\n\nWe can retrieve elements of vectors by indexing with square brackets. Note that in R, vectors start from 1 rather than 0 as in other languages.\n\nvector[2]\n\n[1] 2\n\nvector[5] # the last element of the vector would be 4 in other languages, e.g. python.\n\n[1] 5\n\n\nWe can also slice vectors, i.e. retrieve a subset of the elements:\n\nvector[2:4]\n\n[1] 2 3 4\n\n\nLastly, it’s sometimes helpful to use logical indexing\n\nelements = c(TRUE, FALSE, TRUE, TRUE, FALSE)\nvector[elements]\n\n[1] 1 3 4\n\nvector[!elements] # the !elements syntax does elementwise negation.\n\n[1] 2 5\n\n\n\n\n2.2.4 Matrices\nMatrices are “just” vectors with two axes, so most of the above transfers:\n\nmat = matrix(1:6, nrow=2, ncol=3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmat[2,3] # get the (2,3) element\n\n[1] 6\n\nmat[,3] # third column\n\n[1] 5 6\n\nmat[2,] # second row\n\n[1] 2 4 6\n\nmat[1:2, 2:3] # slicing\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n\nmat[mat &lt; 4] # logical indexing (note this returns a vector of matching elements!)\n\n[1] 1 2 3\n\n\n\n\n2.2.5 Data Frames\nData frames are one of the most important data structures in R for storing and manipulating tabular data. Think of a data frame as a table, where each column can contain values of different types (numeric, character, logical, etc.), and each row represents an observation.\nYou can create a data frame using the data.frame() function:\n\ndf &lt;- data.frame(\n    name = c(\"Flipper\", \"Tux\", \"Pingu\"),\n    species = c(\"Adelie\", \"Gentoo\", \"Emperor\"),\n    flipper_length_mm = c(210, 217, 230)\n)\ndf\n\n     name species flipper_length_mm\n1 Flipper  Adelie               210\n2     Tux  Gentoo               217\n3   Pingu Emperor               230\n\n\nIn the data.frame function we specify multiple columns of data through &lt;col name&gt; = &lt;vector of col values&gt;. Of course, the vectors need to be the same length or we will get an error. When we print the data frame we can see the column names along the top, the row numbers on the left and the values in the middle. We can access columns from the data frame like so:\n\ndf$name\n\n[1] \"Flipper\" \"Tux\"     \"Pingu\"  \n\ndf$flipper_length_mm\n\n[1] 210 217 230\n\n\nRows can be accessed as:\n\ndf[2,]\n\n  name species flipper_length_mm\n2  Tux  Gentoo               217\n\n\nSpecific entries can be accessed as\n\ndf[2,\"species\"]\n\n[1] \"Gentoo\"\n\ndf[2,2]\n\n[1] \"Gentoo\"\n\n\n\n\n\n\n\n\nTibbles\n\n\n\nData frames have been supplanted by a more modern implementation called a tibble, provided through the tidyverse package of the same name. On a practical level the functionality provided is basically the same. We will not use data.frame much outside of this week, in favour of tibble, which we will see in Week 4.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#control-flow",
    "href": "week02.html#control-flow",
    "title": "2  Coding in R",
    "section": "2.3 Control Flow",
    "text": "2.3 Control Flow\n“Control flow” statements allow us to control the order in which instructions are executed in a program.\n\n2.3.1 Conditionals\nThe most commonly used conditional statement is an if statement.\n\nx = 1\nif(x == 1) print(\"hello\") # simple statements can be on one line\n\n[1] \"hello\"\n\nif(x &lt; 1) {\n    print(\"I don't know how you got here\") # more complex statements can be enclosed in braces\n}\n\n# we can also use else if, and else, for more fine grained control\nif(x &lt; 0) {\n    print(\"x was negative.\")\n} else if(x == 0) { # note that the else *must appear on the same line* as the closing brace!\n    print(\"x was 0\")\n} else {\n    print(\"x was positive\")\n}\n\n[1] \"x was positive\"\n\n# Finally, we can include more complex statements\nif(x &gt; 0 && x &lt; 2) print(\"hello\")\n\n[1] \"hello\"\n\n\nThere are also more complex conditionals, like switch statements, but we won’t cover these here.\n\n\n2.3.2 Loops\nLoops allow us to execute a block of code repeatedly. The most commonly used is a for loop:\n\n# this runs the block inside the braces for each value of i between 1 and 5\nfor(i in 1:5) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n# this shows more complex logic inside the loop.\n# break forces the loop to end\nfor(i in 1:10) {\n    if(i &gt; 2) print(i)\n    if(i &gt;= 5) break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\nWe can also use this structure to loop through elements of vectors. This replicates the previous loop but using boolean indexing.\n\nvector = 1:10\nfor(i in vector[(vector &gt; 2) & (vector &lt; 6)]) {\n    print(i)\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above uses more advanced boolean indexing; we will see this in more detail in the lab.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#functions",
    "href": "week02.html#functions",
    "title": "2  Coding in R",
    "section": "2.4 Functions",
    "text": "2.4 Functions\nWriting one long script is very bad practice. It’s much better to break R code up across multiple functions and, as we will see next week, across multiple script files. We can define callable functions in R as follows\n\nadd = function(x, y) {\n    x + y\n}\nadd(1, 10)\n\n[1] 11\n\n\nNotice that the above assigns a function to the variable add. This is because functions are objects in R. This enables a powerful paradigm called functional programming, which is a very efficient way to build complex programs by writing small building block functions that you compose to yield complexity. The tidyverse, which we will see in Week 4, embraces functional programming paradigms.\nIn other programming languages it is necessary to specify what you want to return from a function. This is not required in R; the default is that the last statement in the function will be the return value. The return keyword still exists, but is used slightly differently to other languages. The above code is equivalent to\n\nadd = function(x, y) {\n    return(x + y)\n}\nadd(1, 10)\n\n[1] 11\n\n\nOf course, there are times when you might need to use return to exit a function early. For example:\n\nsilly_add = function(x,y) {\n    if (x &lt; y) {\n        return(x + y)\n    }\n    x = -x\n    y - x\n}\nadd(1,10)\n\n[1] 11\n\nadd(10,1)\n\n[1] 11\n\n\n(Of course it is possible to think of more applied examples of this…)\n\n2.4.1 Higher Order Functions\nHigher order functions are functions that either take or return functions. Functions that take functions are really useful for array operations. Here we pass the “add” function into the “Reduce” function to sum the elements of the numbers from 1 to 10.\n\nReduce(add, 1:10)\n\n[1] 55\n\n\nWe don’t even need to assign the function to a variable - we can pass it as an anonymous function.\n\nReduce(function(x, y) x * y, 1:10)\n\n[1] 3628800\n\n\nAnother useful example are the apply family of functions, which take a vector and apply a function to each element of the vector.\n\nsapply(1:10, function(x) x*10)\n\n [1]  10  20  30  40  50  60  70  80  90 100\n\n\nFunctions that return functions can also be useful:\n\nmake_multiplier = function(multiplier) {\n    function(x) {\n        multiplier * x\n    }\n}\ntimes_two = make_multiplier(2)\ntimes_ten = make_multiplier(10)\n\ntimes_two(2)\n\n[1] 4\n\ntimes_ten(2)\n\n[1] 20\n\n\n\n\n2.4.2 Pipes\nPipes are a way to make code more readable and expressive by providing a succinct syntax for composition. Writing this mathematically, suppose we wanted to compute \\(f(g(x))\\). If we specify these functions in R, we might have\n\nf = function(y) {\n    y * 10\n}\n\ng = function(x) {\n    x + 5\n}\n\nx = 10\nf(g(x))\n\n[1] 150\n\n\nWe can express this equivalently using the forward pipe syntax |&gt;.\n\ng(x) |&gt; f()\n\n[1] 150\n\n\nor even\n\nx |&gt; g() |&gt; f()\n\n[1] 150\n\n\nThe parentheses () are required. R is implicitly inserting the argument “behind” the pipe into the first argument of the function. This means that we can use functions that have more arguments, for example:\n\nx |&gt; add(5) |&gt; times_two()\n\n[1] 30\n\n\nWe will use pipes much more extensively in Weeks 4-5!\n\n\n\n\n\n\nOther Pipes\n\n\n\nThere are other kinds of pipe available in R. Another one which is commonly used is the %&gt;% pipe, which precedes |&gt; and was introduced in the R package magrittr. In most cases the behaviour of these two pipes is the same, however |&gt; is part of the base R language while %&gt;% is not (though, it is included as part of tidyverse).\nAnother useful pipe is the “tee pipe” %T&gt;%, which can be used to “split” a pipe to call functions that don’t return anything (which would otherwise end the pipe), then continue to use the pipe expression afterwards.\n\n\n\n\n2.4.3 Closures (Advanced)\nLet’s examine the type of the add function we defined earlier.\n\ntypeof(add)\n\n[1] \"closure\"\n\n\nThis says add is a “closure”. “Closure” actually means something more complicated than just “function”. A closure has full access to the environment it was defined in. For example:\n\nz = 10\nclosure = function(x) {\n    x + z\n}\nclosure(1)\n\n[1] 11\n\n\nThe closure can “see” the value of z from the outside scope. This can be dangerous, because it means that functions can have unpredictable behaviour unless we are careful about controlling what variables they access.\n\nz = 50\nclosure(1)\n\n[1] 51\n\nz = 20\nclosure(1)\n\n[1] 21\n\n\nIn other words, the value of closure(1) can depend on things other than just the values input to the function. Fortunately functions do not (typically) modify the outer scope; this is referred to as “lexical scoping”\n\nbad_closure = function() {\n    z = rnorm(1) \n}\nbad_closure()\nclosure(1)\n\n[1] 21\n\nbad_closure()\nclosure(1)\n\n[1] 21\n\n\nIn the above we try to modify the value of z inside very_bad_closure, but we can see that the modification is limited to the “inner” scope of that function and does not propagate to the outer scope. However - we can still modify the outer scope if we really want to using the superassignment operator, &lt;&lt;-:\n\nvery_bad_closure = function() {\n    z &lt;&lt;- rnorm(1) \n}\nvery_bad_closure()\nclosure(1)\n\n[1] 0.1913682\n\nvery_bad_closure()\nclosure(1)\n\n[1] -0.8924406\n\n\nThis allows us to do some powerful - but dangerous - things!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week02.html#basic-debugging",
    "href": "week02.html#basic-debugging",
    "title": "2  Coding in R",
    "section": "2.5 Basic Debugging",
    "text": "2.5 Basic Debugging\nBugs happen, and a central part of being a data scientist is knowing how to fix them when they do. Some studies have shown that around 50% of development time is spent debugging, and the average software project has 15-50 bugs per 1000 lines of code. When working in quarto as we will mostly in this course, here are a few approaches to debugging that can help:\n\nUse print or cat to inspect values.\nIsolate the problem in a new chunk. If something isn’t working, break the chunk up into multiple sections to check the logic, or isolate portions of a complex function to test them.\nCheck your type assumptions using str or typeof to ensure the data is what you expect it to be.\nSet error: true in the chunk options to see an error message without halting the document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in `R`</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  Reproducible Research and Project Management",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#objectives",
    "href": "week03.html#objectives",
    "title": "3  Reproducible Research and Project Management",
    "section": "",
    "text": "Know the principles of reproducibility and literate programming, and how they can be realised in R using Quarto.\nKnow how to collaborate and track your code using git.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#introduction",
    "href": "week03.html#introduction",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nReproducibility is at the heart of credible scientific computing. Yet, it’s increasingly common to find projects where no one, not even the original author, can recreate past results. Code is scattered, data is missing, and assumptions live only in someone’s head. This undermines the foundations of science. In response to these issues, two powerful ideas have emerged: literate programming and version control.\nLiterate programming encourages us to weave narrative and code together — to treat analysis as a story told with data. Quarto, the modern successor to R Markdown, provides a seamless way to do this that supports R and many other languages.\nMeanwhile, Git gives us a robust system for tracking changes, collaborating with others, and safeguarding our work. With Git, we can see the evolution of a project, roll back mistakes, and work in parallel as a team.\nAs said in (Buckheit and Donoho 1995) (regarding research papers about computational results):\n\n… these documents are not the research [rather] these documents are the “advertising.” The research is the “full software environment, code, and data that produced the results”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#reproducibility-and-literate-programming-in-r-with-quarto",
    "href": "week03.html#reproducibility-and-literate-programming-in-r-with-quarto",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.2 Reproducibility and Literate Programming in R with Quarto",
    "text": "3.2 Reproducibility and Literate Programming in R with Quarto\nWe have already seen some Quarto documents, but up until now we have treated it as just a way to run R code. We will now give a brief, more formal introduction to Quarto. More information is available in the Quarto guide.\n\n3.2.1 Reproducibility and Literate Programming\n\n\n3.2.2 Anatomy of a Quarto Document\nA Quarto document is a plain text document with the extension .qmd. It can start with a YAML Header followed by narrative text interspersed with code chunks.\n\n3.2.2.1 Knitting\nFundamental to understanding Quarto is knowing what you can use it for. Quarto is intended to allow a user to write documents in the literate programming paradigm, in a way that is agnostic to the final output file format of the document. For example, these lecture notes are all written in Quarto, and it allows them to be compiled into HTML or PDF as you can find on the course Blackboard. There are a lot of other formats that Quarto can be compiled to, though! The process of taking the raw .qmd file and turning it into HTML or PDF is called “knitting”. In RStudio, knitting is accomplished by clicking the Render button.\n\n\n3.2.2.2 YAML Header\nYAML is an acronym for “Yet Another Markup Language” or “YAML Ain’t Markup Language”. It is intended to be a way of serialising data that is easily readable both by humans and machines. Giving a full specification of YAML is beyond the scope of this course, but those interested can find more information here.\nA Quarto document does not need to include a YAML header, but it can be helpful to include metadata as part of the document. For example, a minimal YAML header might look like:\n---\ntitle: An Example Quarto Document\n---\nThe two --- lines are important; these tell Quarto that this is the YAML header, and everything between the --- is to be treated as YAML. In this case, the contents simply tells Quarto that the title of the document is “An Example Quarto Document”. When the document is knitted, the title can be used in various ways. For example, it can be placed into the &lt;title&gt; tags in a HTML document, or it can be used to generate a PDF table of contents.\nThe YAML header is also used to inform the knitting process. For example, this header tells Quarto that we would like to knit to HTML:\n---\ntitle: An Example Quarto Document\nformat: html\n---\nThe next one specifies that either HTML or PDF are options. For HTML, it says the theme and injects some custom styles. For PDF it specifies the documentclass and injects a custom preamble.\n---\ntitle: An Example Quarto Document\nformat: \n    html:\n        theme: cosmo\n        css: styles.css\n    pdf:\n        documentclass: scrreport\n        include-in-header: preamble.tex\n---\nThere are a great many other keys that can be included to customise the document. For this course, the following simple header will usually suffice (or, even, the default one generated by RStudio):\n---\ntitle: &lt;Your Title Here&gt;\nformat: html\n---\n\n\n3.2.2.3 Narrative Text\nAfter the YAML header, the rest of the text is markdown interspersed with code chunks. Markdown is a simple language that describes how text should be formatted when knitted / rendered. For example:\n## An Amazing Title\n\nThis *text* is **even more** _amazing_ than the title. It can even include math! $\\pi = 4$.\nKnits to:\n\n\n\n\n\n\nOutput\n\n\n\n1.1 An Amazing Title\nThis text is even more amazing than the title. It can even include math! \\(\\pi = 4\\).\n\n\nA more complete description of Markdown can be found here.\n\n\n3.2.2.4 Code Chunks\nCode chunks in Quarto are indicated by using the ``` syntax, i.e.:\n\n```{r}\nprint(\"Hello world!\")\n```\n\nIf we embed this in a Quarto document, we get the following output when knitted:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nThe ``` are processed by the knitter and a nicely formatted version of the code block is substituted. But, crucially, the code is actually run and the output is placed after it! As we have already seen, this can be used to embed figures in the knitted output. For example, by adding this:\n\n```{r}\nlibrary(tidyverse)\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n```\n\nWe get this:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\npenguins &lt;- read_csv(\"data/penguins.csv\")\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rowid, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNotice, however, that this included a lot of “ugly”, unnecessary output. We can fix this by using code block options which we add at the top of code blocks.\n\n```{r}\n#| message: false\n#| warning: false\n#| echo: false\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  theme_minimal()\n```\n\nIn the above:\n\nmessage: false removes messages printed when loading the data or tidyverse,\nwarning: false removes warning messages\necho: false stops the code from being rendered, so we just get the figure.\n\nSo, if we run this, we just get the figure:\n\n\n\n\n\n\n\n\n\nImportantly, all the code to produce the figure is still in the .qmd, so reproducibility of the document is preserved.\n\n\n\n\n\n\nOther Execution Options\n\n\n\nFurther execution options are listed here. A particularly useful (but advanced) one is output: asis, which allows your code to generate raw markdown that Quarto will then postprocess. This allows you to do advanced things like autogenerate custom tables to display in the document.\n\n\nOne final note is that we can even run code inline. For example, suppose I wanted to include the text:\n\nThe Palmer Penguins dataset contains 344 records.\n\nIt’s not very convenient or robust to count the rows in the Palmer Penguins dataset and write it in the document as a string! Instead we can use an “inline” code chunk, e.g.\n\nThe Palmer Penguins dataset contains `r nrow(penguins)` records.\n\nThe text:\n\n`r nrow(penguins)`\n\ntells the knitting process to run nrow(penguins) and substitute the result into the document, so that this is not necessary. This is much better for reproducibility than including the number in the source, since now the document remains correct even if the raw data changes.\n\n\n\n3.2.3 Advantages of Quarto\nA major advantage of Quarto is that it is easy to integrate with version control because the files are “just” plain text files. We will examine this in more detail in the next section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#an-amazing-title",
    "href": "week03.html#an-amazing-title",
    "title": "3  Reproducible Research and Project Management",
    "section": "1.1 An Amazing Title",
    "text": "1.1 An Amazing Title\nThis text is even more amazing than the title. It can even include math! \\(\\pi = 4\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week03.html#introduction-to-git",
    "href": "week03.html#introduction-to-git",
    "title": "3  Reproducible Research and Project Management",
    "section": "3.3 Introduction to Git",
    "text": "3.3 Introduction to Git\nGit is a version control system which you can use to manage changes to your codebase. It can be used to record what changed, when and why across all the files you are tracking. At any point, you can “rewind” to a previous state, compare versions, or explore how your code evolved. You can even branch off to try something new without breaking the main version.\nGit is interacted with through git verbs. When we use git from the command line, the verbs are commands added after git that specify what we want git to do. However, understanding what the verbs mean also helps when interacting with GUI tools, such as the git extensions in VS Code and RStudio.\n\n3.3.1 Initialising Git Repositories\nThere are two major ways to create a git repository.\n\nUse the git init verb. This initialises an empty local git repository in a folder.\nUse the git clone verb. This allows you to clone a repository from GitHub to your local machine.\n\nUsing git init does not create a remote repository on GitHub automatically. You still need to go into GitHub and create the remote, then add it. When you create an empty remote on GitHub, the repository contains instructions that describe how to add it as a remote in a local repository, or how to clone it to your local machine.\n\n\n\n\n\n\nTip\n\n\n\nIn this course, because we will be using GitHub classroom, you will rarely have to create repositories for yourself. But it’s still useful to know how to do this from scratch.\n\n\n\n\n3.3.2 Commits\nThe basic atomic element of a git repository is a “commit”. Each commit is like a snapshot of the repository with a (user-supplied) message describing what changed between that snapshot and the previous one. The verbs which control commits are:\n\ngit add: stage local changes to the staging area.\ngit status: check what is currently staged in the staging area.\ngit commit: create a commit containing all of the changes in the staging area.\n\n\n\n3.3.3 Branches\nA branch is like a parallel version of your project in which you can work on new features, experiments or fixes without affecting the main code. The default branch is usually called main, but you can create a new branch and switch between them at any time.\nBranches are also very useful for collaboration. If you are working in parallel with other team members, best practice is to each work on your own branch of the code, then when you are done, merge your changes together. This ensures that all your changes are grouped together, and there is always a version of your code stored in your branch that works as expected.\nFinally, they are useful for clarity. If you are developing a major new feature in a codebase, grouping all of the commits for that feature together in a branch with the feature name makes the logical connection between them clearer.\nThe verbs which control branching are:\ngit branch # see all branches\n\ngit branch my-amazing-feature # create a new branch called &lt;branch-name&gt;\n# note that you are still on the original branch!\n\ngit checkout my-amazing-feature # switch to the branch &lt;branch-name&gt;\n# this command will not work if you have changes that are not committed!\nWhen you are done making changes on the branch my-amazing-feature, you will want to merge the changes back onto main. This can be done as follows:\ngit checkout main # switch back to the main branch\ngit merge my-amazing-feature # merge in changes\nGit will do its best to merge the changes onto main. However, if main already contains commits that are not present in my-amazing-feature, it may not be obvious how to combine the commits from my-amazing-feature and those on main. This creates a conflict, which we will see in a moment.\n\n\n3.3.4 Collaboration\nInside each git repository there is a folder named .git which stores the repository. The git commands you type at the command line examine and modify the contents of this .git folder according to the command you issue. This means that the entire repository is stored on your local machine; there is no need for git to talk to GitHub apart from to perform specific operations related to collaboration.\n\n\n\n\n\n\nWarning\n\n\n\nYou should never modify the contents of the .git folder manually unless you really know what you are doing. Modifying it can catastrophically break your local git repository. Only interact with it through the git commands, or GUIs.\n\n\nTo enable collaboration we need a “central” git repository which all members of a team can see and talk to. This is the role of GitHub (or other Git platforms, like GitLab), which provides these central repositories. GitHub essentially stores a separate copy of your .git folder, which you can push to.\ngit checkout my-amazing-feature\n# you can get the URL for origin from GitHub\ngit remote add origin git@github.com:jcockanye/my_amazing_app\n\ngit push # this command might fail if you have never pushed my-amazing-feature\n# if so you can run this command to tell GitHub to set the \"upstream\" branch on origin\ngit push -u origin my-amazing-feature\nPushing only works if the remote branch HEAD is contained in your local commit tree. If there is a mismatch between commits, then the difference first needs to be pulled down to your local machine.\ngit pull # pull down changes from the remote\nIf there are changes on origin that you don’t have locally, git will try to merge them, as when you use git merge. This has the same caveat, that it can lead to conflicts.\n\n\n3.3.5 Merge Conflicts\nMerge conflicts usually occur when git can’t automatically combine changes from two branches. The most common reason for this is if two people edited the same line of a file, or if you changed the same file in different branches. To fix the conflict you need to intervene manually to tell git how the changes should be combined. Merge conflicts are perfectly normal in collaborative work, and resolving them carefully ensures you don’t lose anyone’s changes.\nWhen a conflict occurs, Git will mark conflicting lines in the file like so:\n# Load data\ndata &lt;- read.csv(\"data.csv\")\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Calculate mean of column A\nmean_A &lt;- mean(data$A)\nprint(mean_A)\n=======\n# Calculate sum of column A\nsum_A &lt;- sum(data$A)\nprint(sum_A)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/sum-column\nThe &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD part up to ======= shows the code from your current branch. The text between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/sum-column show the end of the incoming changes. In this case, we need to examine what the two blocks do and work out how to resolve the conflict. In this case, a sensible way to do this would be as follows:\n# Load data\ndata &lt;- read.csv(\"data.csv\")\n\n# Calculate mean and sum of column A\nmean_A &lt;- mean(data$A)\nsum_A &lt;- sum(data$A)\n\nprint(mean_A)\nprint(sum_A)\nOnce all the conflicts have been resolved you can then create a new merge commit containing the merge results, and push up to GitHub.\n\n\n\n\n\n\nAvoiding Conflicts\n\n\n\n\nKeep changes small and focused to avoid conflicts. (Don’t create monolithic, 1000 line commits!)\nCommunicate with collaborators before making major changes, to avoid large conflicting rewrites.\nWork on separate branches, to simplify merges and reduce their frequency.\n\n\n\n\n\n3.3.6 Ignoring Files\nWe don’t always want to commit all files to version control. A common example is output files, e.g. images or PDFs. These are often called build artifacts - they are things your code is written to produce. Committing build artifacts is bad practice:\n\nThey can be very large, bloating the repository.\nThey can be recreated from source files\nThey often cause unnecessary merge conflicts.\n\nOf course, there are exceptions to this, for example if you want to store small, critical outputs for collaborators who don’t build locally.\nHowever - git doesn’t know what files are source files and what are build artifacts. We can use a .gitignore file, stored in the root directory of your Git repository, to tell Git what to exclude from commits, for example:\n*.html\n*.pdf\n*.png\n*.jpg\n*.RData\nAs an aside, generating .gitignore files is a fantastic use of GenAI. Github contains billions of them, so GenAI knows how to make them very well!\n\n\n\n\n\n\nTip\n\n\n\nFor this course the best practice is to only commit input and source files, not outputs. Your code should be able to regenerate all results from scratch.\n\n\n\n\n3.3.7 Using Git in RStudio\nAs with many IDEs, RStudio has built in Git integration which can make the git workflow easier than interacting purely through the terminal. However, knowing how to do things in the terminal is still helpful - not all features are exposed through IDEs.\nWhen you open a project that contains a Git repository in RStudio you will see a Git pane, typically on the right-hand-side in the top panel. We will explore how to interact with this briefly in the labs.\n\n\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “WaveLab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer New York. https://doi.org/10.1007/978-1-4612-2544-7_5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Research and Project Management</span>"
    ]
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#objectives",
    "href": "week04.html#objectives",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "",
    "text": "Know the “tidy data” philosophy.\nKnow how to realise this in R using dplyr, tidyr and the tidyverse.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#motivation",
    "href": "week04.html#motivation",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.1 Motivation",
    "text": "4.1 Motivation\nAs a data scientist, data is central. The problem is that data can be highly heterogeneous; the best way to organise it is different when it is stored and used for analysis. Moreover, the best way to organise it depends on what kind of analysis you intend to do.\n\n\n\n\n\n\nExample: Aircraft Data\n\n\n\nAn aircraft fleet manager has many aircraft. Each aircraft can have 2 or 4 engines. Extensive in-flight data about the aircraft is collected, including information about the engines (e.g. temperatures, pressures, thrust produced, fuel consumption rates) and the aircraft themselves (e.g. total number of passengers, source and destination airports, GPS coordinates, heights). The best way to store all of this information might be in a series of tables in a relational database (see Week 10).\nWhen analysing the data, it is helpful to have all the explanatory variables in a single matrix, and the response variables in a vector, one row per observation. Depending on what we are modelling, the shape of this data may change. For example: 1. If we are interested in flight time as a function of aircraft weight, we would have one row per journey. 2. If we are interested in engine performance as a function of aircraft height, we would have one row per engine monitoring record. The two datasets would have (significantly) different sizes, and moreover, would be completely different to the structure of the database in which the data is stored.\n\n\nThe goal of tidying data is to obtain a standardized data representation to allow the analysis itself to be as standardized as possible. Of course, the analysis will still be highly domain dependent, but if the functions we write and algorithms we use depend on a standardized data representation, it can significantly reduce the amount of application-specific coding that’s required whenever we encounter a new problem.\n\n\n\n\n\n\nImportant\n\n\n\nIn your assessments you will be allowed to re-use code you have written before the assessment! It is therefore helpful to understand the standardization process so that you can maximise the reusability of your code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#the-tidy-data-philosophy",
    "href": "week04.html#the-tidy-data-philosophy",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.2 The Tidy Data Philosophy",
    "text": "4.2 The Tidy Data Philosophy\nThe Tidy Data Philosophy is outlined in (Wickham, Çentnkaya-Rundel, and Grolemund 2023, sec. 5). In brief, this specifies that we should organise our data for analysis into a data frame such that:\n\nEach variable is a column.\nEach observation is a row.\nEach cell contains a single value.\n\nHere is an example borrowed from (Wickham, Çentnkaya-Rundel, and Grolemund 2023, sec. 5.2). Below we have some data organised in three different ways. Each shows the same values of four variables: country, year and population, and number of documented cases of Tuberculosis, but organises the data in different ways.\ntable1\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\ntable2\n#&gt; # A tibble: 12 × 4\n#&gt;   country      year type           count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999 cases            745\n#&gt; 2 Afghanistan  1999 population  19987071\n#&gt; 3 Afghanistan  2000 cases           2666\n#&gt; 4 Afghanistan  2000 population  20595360\n#&gt; 5 Brazil       1999 cases          37737\n#&gt; 6 Brazil       1999 population 172006362\n#&gt; # ℹ 6 more rows\n\ntable3\n#&gt; # A tibble: 6 × 3\n#&gt;   country      year rate             \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1 Afghanistan  1999 745/19987071     \n#&gt; 2 Afghanistan  2000 2666/20595360    \n#&gt; 3 Brazil       1999 37737/172006362  \n#&gt; 4 Brazil       2000 80488/174504898  \n#&gt; 5 China        1999 212258/1272915272\n#&gt; 6 China        2000 213766/1280428583\nOne of these, table1, will be much easier to work with because it is tidy.\nTidying data has several advantages:\n\nIt plays nicely with Data Frame representations\nWe can create standard algorithms that understand how to work with tidy data.\nWe can plug it into visualisation routines.\n\nMoreover, because of vectorised computation in R, it can be considerably faster to store your data is this way.\n\n\n\n\n\n\nNote\n\n\n\nAgain, a tidy format for analysis is not always the same as the storage format. Some datasets can be too large and complex to be stored as a square data frame like this, even though they can easily be stored and queried in more complex formats. Likewise, when you have “tidied” a dataset for your analysis you may not want always to save it to disk. This can cause problems if the underlying data changes and, depending on how you store the data, it could be considerably larger on disk than when stored in memory. On the other hand it can have speed advantages to cache the tidied data if it is static.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#the-tidyverse-ecosystem",
    "href": "week04.html#the-tidyverse-ecosystem",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.3 The tidyverse Ecosystem",
    "text": "4.3 The tidyverse Ecosystem\nThe tidyverse is a collection of R packages with a shared design philosophy. The core packages of the tidyverse are:\n\nggplot2 for visualisation, using the Grammar of Graphics (Week 6),\ndplyr which provides data manipulation tools,\ntidyr for transforming datasets to a tidy format,\nreadr for reading rectangular data in a variety of formats,\npurrr, a functional programming toolkit that reduces the need for loops,\ntibble for a more modern data frames,\nstringr, providing string manipulation tools,\nforcats for manipulating factor variables in R, and\nlubridate for date manipulation.\n\nHowever, the tidyverse can be installed and loaded as a single bundle of packages using\ninstall.packages(\"tidyverse\")\nand\nlibrary(tidyverse)\n\n4.3.1 An Opinionated Library\nThe tidyverse describes itself as opinionated, meaning that it is designed to do data analysis in a particular way, i.e. it forces users to conform to a particular style. This has some advantages; if you are happy with the style of the tidyverse then you can accomplish some very powerful things with minimal code. On the other hand, if you want to do things that are outside the focus of the tidyverse it can be much more complicated. This can be particularly challenging when it comes to data visualisation tools like ggplot2, where some visualisation types may not be supported.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#loading-data",
    "href": "week04.html#loading-data",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.4 Loading Data",
    "text": "4.4 Loading Data\nData loading is a fundamental skill in data science. Real world data comes in various formats and from different sources. R provides many built-in functions and packages to efficiently import data from various sources. This section covers the most common data loading techniques you’ll encounter in practice.\n\n4.4.1 Built-in Data Loading Functions\n\n4.4.1.1 CSV Files\nCSV stands for “Comma Separated Values”, meaning that the data is stored in plain text with commas separating the columns and new lines separating the rows. The Palmer Penguins dataset was stored in CSV; here are some example rows from the raw file:\n\"rowid\",\"species\",\"island\",\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\",\"sex\",\"year\"\n1,\"Adelie\",\"Torgersen\",39.1,18.7,181,3750,\"male\",2007\n2,\"Adelie\",\"Torgersen\",39.5,17.4,186,3800,\"female\",2007\n3,\"Adelie\",\"Torgersen\",40.3,18,195,3250,\"female\",2007\nIn this case the file contains a header row with column names and then multiple following rows with the data.\nCSV files are among the most common data formats. tidyverse provides several functions for loading them through the readr component, which is automatically loaded with library(tidyverse).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- read_csv(\"data/penguins.csv\")\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rowid, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ntidyverse load operations return data in tibble format, a modern reimagining of R’s core data.frame. Surprisingly, typeof(data) returns list; this is because the core data type of a tibble is actually a list. We can check whether an object is a tibble using several functions:\n\nis_tibble(data) \n\n[1] TRUE\n\nis.data.frame(data)\n\n[1] TRUE\n\nclass(data) \n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\n\n4.4.1.2 Other Delimited Files\nIn a csv file the comma is called a delimeter; it is used to delimit columns. It’s possible to store data in the same textual format, but with other delimeters. For example, this is what Palmer Penguins might look like if it was delimited with pipes (|) instead of commas:\n\"rowid\"|\"species\"|\"island\"|\"bill_length_mm\"|\"bill_depth_mm\"|\"flipper_length_mm\"|\"body_mass_g\"|\"sex\"|\"year\"\n1|\"Adelie\"|\"Torgersen\"|39.1|18.7|181|3750|\"male\"|2007\n2|\"Adelie\"|\"Torgersen\"|39.5|17.4|186|3800|\"female\"|2007\n3|\"Adelie\"|\"Torgersen\"|40.3|18|195|3250|\"female\"|2007\nOf course, we can’t use read_csv to load a file like this! Instead we would need to use the function read_delim, which takes a delimeter as an optional argument. If we had a version of Palmer Penguins with pipe delimeters stored in data/penguins_pipe.txt, we could load it with:\ndata = read_delim(\"data/penguins_pipe.txt\", delim=\"|\")\nKnowing which loading function to use for plain text input will typically involve looking at the file and experimenting until the data loads successfully. We will explore data loading issues in more detail in the lab.\n\n\n4.4.1.3 Other Loading Functions\nIn this course we will focus on delimited text files and, later, loading from relational databases. However, there are hundreds of other file types that you might encounter in the real world. Here are some examples:\n\nExcel Files: Using the read_excel function from tidyerse.\nparquet files, increasingly popular for big data. Uses the arrow package.\nJSON or XML files - common web formats. These can be read with jsonlite or xml2.\nStata, SPSS or SAS files, using the haven package (which is a part of tidyverse).\n\n\n\n\n\n\n\nTip\n\n\n\nYou will only be assessed on your ability to read delimited files and load data from databases. We might see some of the other formats in labs, but they will not appear in assessments.\n\n\n\n\n\n4.4.2 Core tidyverse Data Operations\nTo create tidy data the main operations are\n\nData Import: (readr::read_csv() or, later, dbplyr)\nTransformations (dplyr)\nReshaping (tidyr)\n\nWe will look at these, as well as some other common tidyverse libraries and functions, in the labs this week and next.\nIt is standard to use pipes (|&gt; and %&gt;%) to chain together operations in a readable, step-by-step way. Moreover tidyverse is tightly integrated with plotting libraries like ggplot2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#dplyr",
    "href": "week04.html#dplyr",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.5 dplyr",
    "text": "4.5 dplyr\nThe purpose of dplyr is to transform and summarise data frames in a readable, consistent way. As with git it consists of core verbs that are intended to be human-readable, e.g.\n\nfilter() - subset rows\nselect() - subset columns\nmutate() - add or transform columns\narrange() - reorder rows\n\nAs an example, consider the gapminder dataset. This provides country-level data on life expectancy, GDP per capita and population, over time.\n\nlibrary(tidyverse)\nlibrary(gapminder)\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nAs we can see, the gapminder is an r tibble, with 1704 rows and 6 columns. Suppose we want to study the GDP of countries in Asia in 2007. Here’s an example using dplyr to accomplish this.\n\ngapminder |&gt; \n    filter(year == 2007, continent == \"Asia\") |&gt;\n    mutate(gdp_billions = gdpPercap * pop / 1e9) |&gt;\n    arrange(desc(lifeExp)) |&gt;\n    select(country, lifeExp, gdp_billions)\n\n# A tibble: 33 × 3\n   country          lifeExp gdp_billions\n   &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 Japan               82.6       4035. \n 2 Hong Kong, China    82.2        277. \n 3 Israel              80.7        164. \n 4 Singapore           80.0        215. \n 5 Korea, Rep.         78.6       1145. \n 6 Taiwan              78.4        666. \n 7 Kuwait              77.6        119. \n 8 Oman                75.6         71.5\n 9 Bahrain             75.6         21.1\n10 Vietnam             74.2        208. \n# ℹ 23 more rows\n\n\nThe code above is human readable - each row describes an operation applied to the dataset, chained together with pipes |&gt;. In other words, we can look at the code and see that we - Took the gapminder dataset, - Applied a filter to the year and continent, - Added a computed column (gdp_billions), - Ordered by lifeExp, - Selected the relevant columns for output.\nIf we choose any pipe |&gt; and delete the pipe and everything after it, we still get a tibble output which just applies the subset of operations before the pipe. We can thus interpret the pipes as “chaining together” the operations we want to apply.\nNote that this can have an impact on cost. For example, consider this example:\n\ngapminder |&gt; \n    mutate(gdp_billions = gdpPercap * pop / 1e9) |&gt;\n    arrange(desc(lifeExp)) |&gt;\n    filter(year == 2007, continent == \"Asia\") |&gt;\n    select(country, lifeExp, gdp_billions)\n\n# A tibble: 33 × 3\n   country          lifeExp gdp_billions\n   &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 Japan               82.6       4035. \n 2 Hong Kong, China    82.2        277. \n 3 Israel              80.7        164. \n 4 Singapore           80.0        215. \n 5 Korea, Rep.         78.6       1145. \n 6 Taiwan              78.4        666. \n 7 Kuwait              77.6        119. \n 8 Oman                75.6         71.5\n 9 Bahrain             75.6         21.1\n10 Vietnam             74.2        208. \n# ℹ 23 more rows\n\n\nThe output is the same, but because we apply the mutate operation before the filter, we will compute gdp_billions for all 1704 rows before discarding all but 23 rows. This is wasteful, and can have a significant performance implication on larger datasets!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week04.html#tidyr",
    "href": "week04.html#tidyr",
    "title": "4  Data Wrangling I - Loading, Cleaning and Tidying",
    "section": "4.6 tidyr",
    "text": "4.6 tidyr\nThe gapminder dataset is a tall dataset; it contains multiple rows for each country and continent. tidyr contains pivot operations to allow us to convert tall datasets into wide datasets, for example:\n\ngapminder_wide = gapminder |&gt; \n    select(country, year, lifeExp) |&gt;\n    pivot_wider(names_from=year, values_from=lifeExp)\ngapminder_wide\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nNow we end up with a 142 \\(\\times\\) 13 tibble whose entries are lifeExp. We can use the “wide” dataset to easily calculate the change in life expecancy:\n\ngapminder_wide |&gt;\n    mutate(change = `2007` - `1952`) |&gt;\n    select(country, change)\n\n# A tibble: 142 × 2\n   country     change\n   &lt;fct&gt;        &lt;dbl&gt;\n 1 Afghanistan   15.0\n 2 Albania       21.2\n 3 Algeria       29.2\n 4 Angola        12.7\n 5 Argentina     12.8\n 6 Australia     12.1\n 7 Austria       13.0\n 8 Bahrain       24.7\n 9 Bangladesh    26.6\n10 Belgium       11.4\n# ℹ 132 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe backquotes, above, (around 2007 and 1952) are needed to tell R that these are not to be treated as numbers but as literal column names. They would also be needed if for column names containing spaces or symbols like + or $.\n\n\nThe “inverse” of pivot_wider is pivot_longer:\n\ngapminder_wide |&gt;\n    pivot_longer(\n        cols=colnames(gapminder_wide)[2:ncol(gapminder_wide)], \n        names_to=\"year\", \n        values_to=\"lifeExp\"\n    )\n\n# A tibble: 1,704 × 3\n   country     year  lifeExp\n   &lt;fct&gt;       &lt;chr&gt;   &lt;dbl&gt;\n 1 Afghanistan 1952     28.8\n 2 Afghanistan 1957     30.3\n 3 Afghanistan 1962     32.0\n 4 Afghanistan 1967     34.0\n 5 Afghanistan 1972     36.1\n 6 Afghanistan 1977     38.4\n 7 Afghanistan 1982     39.9\n 8 Afghanistan 1987     40.8\n 9 Afghanistan 1992     41.7\n10 Afghanistan 1997     41.8\n# ℹ 1,694 more rows\n\n\n\n\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Wrangling I - Loading, Cleaning and Tidying</span>"
    ]
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#objectives",
    "href": "week05.html#objectives",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "",
    "text": "Describe “messier” data types structures that might be encountered in the real world.\nBe able to join tables together using join functions.\nBe able to deal with date and string types.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#joins-connecting-multiple-tables",
    "href": "week05.html#joins-connecting-multiple-tables",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.1 Joins: Connecting Multiple Tables",
    "text": "5.1 Joins: Connecting Multiple Tables\nA join combines rows from two or mor tables based on a related column between them. There are various types of joins depending on what kinds of data you want to join; we’ll go through each of them. To illustrate we will use the (synthetic) student data from student_data.csv and student_grades.csv.\n\n#|\nlibrary(tidyverse)\nstudent_data = read_csv(\"data/student_data.csv\")\nstudent_grades = read_csv(\"data/student_grades.csv\")\nhead(student_data)\n\n# A tibble: 6 × 4\n     ID `Given Name` `Family Name` `Date of Birth`\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt;\n1     1 Adaora       Nkomo                   37205\n2     2 Hiroshi      Tanaka                  37845\n3     3 Aaliyah      Williams                37505\n4     4 Dimitri      Volkov                  37717\n5     5 Amara        Baptiste                37387\n6     6 Rajesh       Krishnamurthy           38218\n\nhead(student_grades)\n\n# A tibble: 6 × 3\n  `Student ID` Course   Grade\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1            1 MATH6195    68\n2            2 MATH6195    67\n3            3 MATH6195    68\n4            4 MATH6195    74\n5            5 MATH6195    52\n6            6 MATH6195    85\n\n\nWe have two tables, and they are linked by a common ID column (called Student ID in student_grades). Names and dates of birth are available in student_data but the student grades are in student_grades. Each student takes multiple courses, so the relationship between the student and grade tables is one-to-many.\n\n\n\n\n\n\nWhy Store Data Like This?\n\n\n\nStoring in multiple tables is good practice for multiple reasons:\n\nData size: If we stored it in a single flat table, the student name would be repeated for every course they take. This can make the data files significantly larger!\nConsistency and Integrity: If a student changes their name, it only needs to be updated in one place.\nFlexibilty: Adding new attributes is much easier. For example, if we wanted to start recording student reference requests, we could add a new student_references table, with a one-to-many relationship with student_data but no link at all to student_grades.\nClarity: It’s easier to think about the data structure if it is logically separated like this (e.g. “each student takes multiple courses”).\n\n\n\n\n5.1.1 Inner Join\nAn inner join takes two tables and joins them together into a single table on based on the matching column, returning only rows that exist in both tables.\n\nresult = inner_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nhead(result)\n\n# A tibble: 6 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course   Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 Adaora       Nkomo                   37205 MATH6195    68\n2     1 Adaora       Nkomo                   37205 MATH2010    59\n3     1 Adaora       Nkomo                   37205 MATH2011    58\n4     2 Hiroshi      Tanaka                  37845 MATH6195    67\n5     2 Hiroshi      Tanaka                  37845 MATH2010    72\n6     2 Hiroshi      Tanaka                  37845 MATH2011    72\n\n\nNotice how we have the union of columns from both tables, apart from the ID column which we joined on. Let’s look at the rows.\n\nnrow(result)\n\n[1] 53\n\nnrow(student_grades)\n\n[1] 59\n\n\nSee result has fewer columns than student grades. This must mean there are some student IDs in the student_grades table that are not in the student_data table. Let’s check:\n\nsetdiff(student_grades$`Student ID`, student_data$ID)\n\n[1] 21 22\n\n\nThere are two missing students from student_data. Because this is an inner join these were excluded from the final table. Let’s now check if there are any students in student_data that are not present in the result.\n\nsetdiff(student_data$ID, result$ID)\n\n[1] 7\n\n\nIt seems the student with ID 7 is missing:\n\nresult |&gt;\n    filter(ID == 7)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: ID &lt;dbl&gt;, Given Name &lt;chr&gt;, Family Name &lt;chr&gt;,\n#   Date of Birth &lt;dbl&gt;, Course &lt;chr&gt;, Grade &lt;dbl&gt;\n\n\nTo check why this might be, we can look for that student in the student_grades table:\n\nstudent_grades |&gt;\n    filter(`Student ID` == 7)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Student ID &lt;dbl&gt;, Course &lt;chr&gt;, Grade &lt;dbl&gt;\n\n\nSo the student is missing because they have no grade records, so the inner join excludes them from the final table. Hopefully they have applied for special considerations!\n\n\n5.1.2 Left / Right Join\nLeft and right join keep all the records in the left table or the right table, respectively, in the join statement. Let’s see what the result looks like with a left join instead:\n\nresult = left_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nresult |&gt; filter(ID == 7)\n\n# A tibble: 1 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     7 Lin          Yifan                   38146 &lt;NA&gt;      NA\n\n\nNow the student who has no student_grades records is still present in the final table. However, because there are no grade records, the table is filled with &lt;NA&gt; under Course and Grade. Right join is essentially the same, but keeps records in the right table.\n\n\n5.1.3 Outer Join\nOuter join keeps records in both tables. For some reason R calls this a full_join, which is not standard terminology.\n\nresult = full_join(student_data, student_grades, by=c(\"ID\" = \"Student ID\"))\nresult |&gt; filter(ID == 7)\n\n# A tibble: 1 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     7 Lin          Yifan                   38146 &lt;NA&gt;      NA\n\n\nWe see that the result contains the student with missing grades, but it will also contain the two students who were missing in the student_data table:\n\nresult |&gt; filter(ID %in% c(21,22))\n\n# A tibble: 6 × 6\n     ID `Given Name` `Family Name` `Date of Birth` Course   Grade\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH6195    70\n2    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH6195    89\n3    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2010    71\n4    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2010    56\n5    21 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2011    62\n6    22 &lt;NA&gt;         &lt;NA&gt;                       NA MATH2011    84\n\n\nNow the Given Name, Family Name and Date of Birth fields are all &lt;NA&gt;.\n\n\n5.1.4 Other Types of Join\nThere are some other types of join which are much less common to encounter. Here are some examples:\n\nsemi_join(x, y, by=\"id\"): keeps rows in x that have a match in y but does not add columns from y.\nanti_join(x, y, by=\"id\"): keeps rows in x that do not have a match in y, but again does not add columns from y.\ncross_join(x,y): gives the cartesian product of x and y, that is, the tibbles are not matched on any common column, but rather all possible combinations of rows in x and y appear.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#strings",
    "href": "week05.html#strings",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.2 Strings",
    "text": "5.2 Strings\nWe often need to deal with purely textual data in data science. While it would be nice for all the data we receive to be highly structured, we frequently get “free text” input from users, where the input may need to be cleaned or otherwise processed before it can be used. In tidyverse this is the role of the stringr library, which provides multiple functions (usually prefixed with str_) that are designed to work nicely with the tibble format. We will look at some string manipulations in the lab, including:\n\nstr_length: for getting the length of a string.\nstr_sub: for taking a substring (the subset of the string between two indices)\nstr_to_lower, str_to_upper, str_to_title: for standardising cases.\nstr_trim, str_squish: for removing leading/trailing whitespace or too many internal spaces.\n\nAll of these can be applied to either a single string or an array of strings, i.e.:\n\nstr_length(\"Hello World\")\n\n[1] 11\n\nstr_length(c(\"Alice\", \"Bob\", \"Dave\"))\n\n[1] 5 3 4\n\n\n\n\n\n\n\n\nRegular Expressions\n\n\n\nThere is a more advanced text processing tool called a regular expression this is essentially a “mini language” that allows a user to describe patterns they want to look for in text. This could be used, for example, to detect phone numbers or email addresses that have been entered in “free text” boxes in surveys. We won’t cover regular expressions in this course, but they are worth knowing about. A good resource for learning is regex101.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week05.html#dates",
    "href": "week05.html#dates",
    "title": "5  Data Wrangling II - Joins, Strings and Dates",
    "section": "5.3 Dates",
    "text": "5.3 Dates\nWhen we imported the student data above, there was a column called “Date of Birth” - but it did not appear to contain dates:\n\nstudent_data$`Date of Birth`\n\n [1] 37205 37845 37505 37717 37387 38218 38146 37596 38167 37590 37472 37803\n[13] 37283 37852 37900 37865 38162 37870 37282 37481\n\ntypeof(student_data$`Date of Birth`[1])\n\n[1] \"double\"\n\n\nThe numbers above are Excel format dates. They are (roughly) the number of days that have elapsed since December 31, 1989. In theory we could process these dates using this knowledge, but it is much more convenient to convert the number to a proper date type in R. This (and more) functionality is provided by lubridate, a package in tidyverse.\n\n5.3.1 Loading and Converting Dates\nLoading dates is surprisingly complex because there is no standard format. For example, here are a variety of perfectly valid ways to represent the date “11th August 2025”:\n\ndates = c(\n    '2025-08-11',\n    '11/08/2025',\n    '08/11/2025',\n    '11 August, 2025',\n    '11-Aug-2025'\n)\n\nThe list above is non-exhaustive, so some domain knowledge will always be required to load dates. R may try to infer a date format, but it might be wrong. It is always safer to specify the date format, or at least, to carefully check that dates have been loaded correctly before doing any data analysis.\nlubridate provides several helper functions to convert from some standard formats, that are fairly robust to different sub conventions like delimeters or padding.\n\ntoday1 = ymd(\"2025-August-11\")\ntoday2 = dmy(\"11/08/25\")\ntoday3 = mdy(\"8.11.2025\")\nc(today1, today2, today3)\n\n[1] \"2025-08-11\" \"2025-08-11\" \"2025-08-11\"\n\n\nR has correctly inferred the date in all three cases. We can also use the as.Date function to customise parsing further, by providing a format string to describe how the date string is expected to be formatted. For more information on formatting see the as.Date documentation.\nNote that while the dates are printed as strings, they do in fact have a date type:\n\nclass(today1)\n\n[1] \"Date\"\n\n\nFor the Excel date example, if we read Excel files with readxl there won’t be a problem. But, sometimes Excel dates make their way into other file formats, and we can use the janitor package to load them.\n\nlibrary(janitor)\nstudent_data$`Date of Birth` = excel_numeric_to_date(student_data$`Date of Birth`)\nstudent_data$`Date of Birth`\n\n [1] \"2001-11-10\" \"2003-08-12\" \"2002-09-06\" \"2003-04-06\" \"2002-05-11\"\n [6] \"2004-08-19\" \"2004-06-08\" \"2002-12-06\" \"2004-06-29\" \"2002-11-30\"\n[11] \"2002-08-04\" \"2003-07-01\" \"2002-01-27\" \"2003-08-19\" \"2003-10-06\"\n[16] \"2003-09-01\" \"2004-06-24\" \"2003-09-06\" \"2002-01-26\" \"2002-08-13\"\n\n\n\n\n5.3.2 Date Processing\nOnce dates are in the proper format, there exist various functions to process them. For example, we can extract the year, month or day:\n\nc(year(today()), month(today()), day(today()))\n\n[1] 2025   10    6\n\n\nFor example, if we want to compute a student’s age we can use lubridate’s interval function followed by time_length\n\nstudent_data$Age = interval(student_data$`Date of Birth`, today()) |&gt;\n    time_length(\"years\")\nstudent_data$Age\n\n [1] 23.90411 22.15068 23.08219 22.50137 23.40548 21.13151 21.32877 22.83288\n [9] 21.27123 22.84932 23.17260 22.26575 23.69041 22.13151 22.00000 22.09589\n[17] 21.28493 22.08219 23.69315 23.14795\n\nstudent_data$Age |&gt; floor()\n\n [1] 23 22 23 22 23 21 21 22 21 22 23 22 23 22 22 22 21 22 23 23\n\n\nOr, we could use as.period to get a precise calendar difference:\n\ninterval(student_data$`Date of Birth`, today()) |&gt;\n    as.period(unit=\"years\")\n\n [1] \"23y 10m 26d 0H 0M 0S\" \"22y 1m 24d 0H 0M 0S\"  \"23y 1m 0d 0H 0M 0S\"  \n [4] \"22y 6m 0d 0H 0M 0S\"   \"23y 4m 25d 0H 0M 0S\"  \"21y 1m 17d 0H 0M 0S\" \n [7] \"21y 3m 28d 0H 0M 0S\"  \"22y 10m 0d 0H 0M 0S\"  \"21y 3m 7d 0H 0M 0S\"  \n[10] \"22y 10m 6d 0H 0M 0S\"  \"23y 2m 2d 0H 0M 0S\"   \"22y 3m 5d 0H 0M 0S\"  \n[13] \"23y 8m 9d 0H 0M 0S\"   \"22y 1m 17d 0H 0M 0S\"  \"22y 0m 0d 0H 0M 0S\"  \n[16] \"22y 1m 5d 0H 0M 0S\"   \"21y 3m 12d 0H 0M 0S\"  \"22y 1m 0d 0H 0M 0S\"  \n[19] \"23y 8m 10d 0H 0M 0S\"  \"23y 1m 23d 0H 0M 0S\" \n\n\nWe could also just subtract the dates, but this gives a difference in terms of days which would be difficult to convert to years (due to leap years).\n\ntoday() - student_data$`Date of Birth`\n\nTime differences in days\n [1] 8731 8091 8431 8219 8549 7718 7790 8340 7769 8346 8464 8133 8653 8084 8036\n[16] 8071 7774 8066 8654 8455\n\n\n\n\n5.3.3 Times and Time Zones\nTimes can be handled similarly to dates. There are routines for loading times that are similar to the date-specific routines described above:\n\ntime1 = ymd_hms(\"2025-08-11 11:33:01\")\ntime1\n\n[1] \"2025-08-11 11:33:01 UTC\"\n\ntime2 = now()\ntime2\n\n[1] \"2025-10-06 10:46:10 BST\"\n\n\nWe can then extract hours, minutes and seconds:\n\nc(hour(time1), minute(time1), second(time1))\n\n[1] 11 33  1\n\n\nOne extra consideration with times is the time zone. For example, we can see that the time zone of time1 and time2 differ, above:\n\nc(tz(time1), tz(time2))\n\n[1] \"UTC\" \"\"   \n\n\nNote that tz(time2) returns an empty string, which is standard in R for the current time zone. If we want to be explicit we can force now to be in a particular time zone, e.g.\n\ntime3 = now(\"Asia/Tokyo\")\ntz(time3)\n\n[1] \"Asia/Tokyo\"\n\n\nWe can also change the time zone on times using the with_tz function:\n\nwith_tz(time1, \"Asia/Tokyo\")\n\n[1] \"2025-08-11 20:33:01 JST\"\n\ntime1\n\n[1] \"2025-08-11 11:33:01 UTC\"\n\n\nNotice that this adjusts the time from the previous time zone (UTC) to match the new time zone (JST). If we want to add a time zone without changing the time, we use the force_tz function:\n\nforce_tz(time1, \"Asia/Tokyo\")\n\n[1] \"2025-08-11 11:33:01 JST\"\n\n\nNow the time is the same as time1, but the time zone has been modified.\n\n\n5.3.4 Formatting Dates and Times\nBy default invoking today1 in R uses the specific format 2025-08-11. We may wish to use another format, for which we can invoke the format function. This takes a date and a format string (as per as.Date, above), and outputs a string in that format. For example:\n\nformat(today1, \"%Y-%m-%d\")\n\n[1] \"2025-08-11\"\n\nformat(today1, \"%d %b %y\")\n\n[1] \"11 Aug 25\"\n\nformat(time1, \"%d %b %y %H:%M\")\n\n[1] \"11 Aug 25 11:33\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling II - Joins, Strings and Dates</span>"
    ]
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "6  Data Visualisation Principles",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#objectives",
    "href": "week06.html#objectives",
    "title": "6  Data Visualisation Principles",
    "section": "",
    "text": "Understand the grammar of graphics framework.\nBe able to critique visualisations for clarity and effectiveness.\nKnow how visual perception affects the interpretation of plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#the-grammar-of-graphics-framework",
    "href": "week06.html#the-grammar-of-graphics-framework",
    "title": "6  Data Visualisation Principles",
    "section": "6.1 The Grammar of Graphics Framework",
    "text": "6.1 The Grammar of Graphics Framework\nThe Grammar of Graphics (Wilkinson 2005) is implemented in R through the library ggplot2, a core library in tidyverse. It provides a systematic approach to constructing data visualisations. Rather than thinking about chart types (bar chart, scatter plot, etc.), the grammar breaks down visualisations into fundamental components that can be combined in flexible ways.\n\n6.1.1 Core Components of the Grammar\nThe grammar consists of several key layers:\nData: The dataset being visualised. In ggplot2 this is assumed to be in tidy format. Aesthetics: referred to as aes in ggplot2. These are the visual properties that represent data variables, including:\n\nPosition (\\((x,y)\\) coordinates)\nColour (hue, saturation)\nShape (point shapes, line types)\nSize (point size, line width)\nTransparency (alpha values)\n\nGeometries: the geometric objects that represent data points. How these are referred to in ggplot2 is included in parentheses below.\n\nPoints (geom_point())\nLines (geom_line())\nBars (geom_bar(), geom_col())\nAreas (geom_area())\nText (geom_text())\n\nStatistics (stat): (optional) statistical transformations applied to data before plotting, e.g.:\n\nCount (frequency tables)\nSmoothing (trend lines)\nBinning (histograms)\n\nScales: How aesthetics translate data values to visual properties:\n\nContinuous scales (for numeric data)\nDiscrete scales (for categorical data)\nColour scales (viridis colour maps, brewer colour palettes)\nPosition scales (log or square-root transformations)\n\nCoordinate Systems: How data coordinates map to the plane of the plot:\n\nCartesian coordinates (most common)\nPolar coordinates\nMap projections\n\nFaceting: Break data into subsets for producing multiple smaller charts:\n\nfacet_wrap(): arranges panels in rows and columns\nfacet_grid(): arranging panels in a grid based on variables.\n\n\n\n6.1.2 Constructing Plots by Composing Components\nThe grammar allows components to be composed with one another by “adding” them to a plot to build visualisations incrementally. Here are multiple examples using the Palmer penguins dataset; we will go into building these in much more detail in the lab.\n\nlibrary(tidyverse)\npenguins &lt;- read_csv(\"data/penguins.csv\")\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  facet_wrap(~island) +\n  labs(\n    title = \"Penguin measurements by species, faceted by island\",\n    x = \"Flipper length (mm)\",\n    y = \"Body Mass (g)\"\n  ) + \n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(colour = species), alpha = 0.6, size = 2) + # scatter points\n  geom_smooth(aes(colour = species), method = \"lm\", se = TRUE) + # trend-line per species\n  geom_smooth(method = \"lm\", colour = \"black\", linetype = \"dashed\", se = FALSE) + # overall trend line\n  scale_colour_viridis_d(name = \"Species\") + # Species colour scale\n  labs(\n    title = \"Multiple Geometries: Points + Trend Lines\",\n    subtitle = \"Species-specific trends (colored) vs overall trend (dashed)\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 2 rows containing non-finite outside the scale range (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(aes(y = after_stat(density), fill = species), \n                 alpha = 0.7, bins = 30, position = \"identity\") + # Histogram of densities (rather than counts)\n  geom_density(aes(colour = species), size = 1.2) + # Add density lines\n  scale_fill_viridis_d(name = \"Species\", alpha = 0.7) + \n  scale_colour_viridis_d(name = \"Species\") + # colour lines and fill blocks with Viridis D\n  labs(\n    title = \"Statistical Transformations: Histogram + Density\",\n    x = \"Body Mass (g)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(colour = body_mass_g, size = flipper_length_mm), alpha = 0.7) +\n  scale_colour_viridis_c(name = \"Body Mass (g)\", trans = \"log10\") +\n  scale_size_continuous(name = \"Flipper Length (mm)\", range = c(1, 8)) +\n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\") +\n  labs(\n    title = \"Scale Transformations: Log Scales\",\n    subtitle = \"Both axes and colour scale transformed\",\n    x = \"Bill Length (mm) - Log Scale\",\n    y = \"Bill Depth (mm) - Log Scale\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(ggforce)\npenguins %&gt;%\n  ggplot(aes(x = body_mass_g, y = species)) +\n  ggforce::geom_sina(aes(colour = island), size = 1.5, alpha = 0.6) +\n  geom_boxplot(alpha = 0.3, outlier.shape = NA, colour = \"black\") +\n  scale_colour_manual(name = \"Island\", values = c(\"#E31A1C\", \"#FF7F00\", \"#1F78B4\")) +\n  labs(\n    title = \"Advanced Composition: Sina Plot + Boxplot\",\n    subtitle = \"Combining ggforce geometries with standard ggplot2\",\n    x = \"Body Mass (g)\",\n    y = \"Species\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_sina()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "week06.html#visual-perception-and-plot-interpretation",
    "href": "week06.html#visual-perception-and-plot-interpretation",
    "title": "6  Data Visualisation Principles",
    "section": "6.2 Visual Perception and Plot Interpretation",
    "text": "6.2 Visual Perception and Plot Interpretation\nYou may already have noticed that not all of the above plots are straightforward to interpret. ggplot gives us a lot of freedom to design our figures, but it does not tell us what are “good” and “bad” designs. For example, this plot gives the impression that welfare recipients in the US are spiralling out of control - but the \\(y\\)-axis starts from 94,000,000:\n\n\n\nPlot of Federal Welfare Recipients in the USA, from here\n\n\nThe next example was presented by General H.R. McMaster to US officials in 2009, intended to explain the complexity of US military strategy in Afghanistan. It led the New York Times to publish an article called We Have Met the Enemy, and He Is Powerpoint.\n\n\n\nWe Have Met the Enemy, and He Is Powerpoint, from here.\n\n\nCareful thought is needed to ensure that plots effectively convey the information that we want them to convey.\n\n6.2.1 Perceptual Accuracy of Visual Elements\n(Cleveland and McGill 1984) developed a theory of graphical perception, the process by which people decode quantitative information from visual representations. They argue that the effectiveness of a graph depends on how accurately viewers can visually judge the data or information that the graph encodes. They rank elementary perceptual tasks, like judging position along a common scale, length, area, angle, colour etc. by how precisely humans can perform them, based on controlled experiments.\nThey found that:\n\nPosition along a common scale is judged most accurately,\nLength, angle and slope are less accurate, with angle and slope worse than length,\nArea, volume or colour saturation are even less reliable.\n\nThese provide guidance for how we should encode data into graphs. For example, bar charts (measuring length), aligned on a common scale (height) might be preferred to pie charts (comparing angle and area).\nSince (Cleveland and McGill 1984), visualisation theory has continued to develop. The full theory is well beyond the scope of this course, but the main points are:\n\nNot all visual encodings are equal. As (Cleveland and McGill 1984) shows, position can be read most accurately, then length/angle, then area/colour.\nDesign with perception in mind. Prioritise encodings that make the main comparison easier, using the most informative visual encoding.\nModern tools help. ggplot2 is based on the (Wilkinson 2005), which builds on these ideas. The mapping of aes to geoms describe these perceptual choices.\n\n\n\n6.2.2 Colour Blindness Considerations\nAnother important concern is accessibility. Around 8% of men and 0.5% of women are affected by colour blindness, which can make some plots impossible to perceive for some users. It is also worthwhile to be mindful that your plots may be printed out in black-and-white or viewed on greyscale screens, so relying solely on colour to make points is better avoided.\nThe most common type of colour blindness is red-green deficiencies, so using red-green contrast should be avoided. Fortunately, ggplot makes it straightforward to adhere to this by providing perceptually uniform, colorblind-safe palettes such as viridis or the ColorBrewer safe sets. Where possible it is still best to make sure there is visual redundancy, rather than using colour alone, using label, shape or line type.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(size=3) +\n  scale_colour_manual(values=c(\"Adelie\" = \"red\", \"Chinstrap\" = \"green\", \"Gentoo\" = \"blue\")) +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIt would be difficult for someone with red-green colour blindness to distinguish Adelie and Chinstrap penguins.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species, shape=species)) +\n  geom_point(size=3) +\n  scale_colour_brewer(type=\"qual\") + \n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing scale_colour_brewer with type=\"qual\" tells R that this is qualitative data (as opposed to sequential, where the factor ordering is meaningful); an appropriate colour palette is then selected automatically. The addition of shape=species also ensures that a viewer who finds colour perception challenging will still be able to interpret the plot.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualisation Principles</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Buckheit, Jonathan B., and David L. Donoho. 1995. “WaveLab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer New York. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nWickham, Hadley, Mine Çentnkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. Sebastopol, CA: O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWild, C. J., and M. Pfannkuch. 1999. “Statistical Thinking in\nEmpirical Enquiry.” International Statistical Review 67\n(3): 223–48. https://doi.org/10.1111/j.1751-5823.1999.tb00442.x.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics.\nSpringer-Verlag. https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "Bibliography"
    ]
  }
]